<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/../../../../Client/archiver.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/archiver.py" />
              <option name="originalContent" value="import os&#10;import shutil&#10;&#10;&#10;class Archiver:&#10;    def __init__(self, archive_dir='archive'):&#10;// ...existing code...&#10;&#10;" />
              <option name="updatedContent" value="import os&#13;&#10;import shutil&#13;&#10;&#13;&#10;&#13;&#10;class Archiver:&#13;&#10;    def __init__(self, archive_dir='archive'):&#13;&#10;        self.archive_dir = archive_dir&#13;&#10;        if not os.path.exists(self.archive_dir):&#13;&#10;            os.makedirs(self.archive_dir)&#13;&#10;&#13;&#10;    def archive_directory(self, source_dir):&#13;&#10;        &quot;&quot;&quot;Archives a directory by moving it into the archive directory.&quot;&quot;&quot;&#13;&#10;        try:&#13;&#10;            # Generate a unique name for the archive directory&#13;&#10;            timestamp = datetime.datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#13;&#10;            archive_name = f&quot;{os.path.basename(source_dir)}_{timestamp}&quot;&#13;&#10;            destination_dir = os.path.join(self.archive_dir, archive_name)&#13;&#10;&#13;&#10;            # Move the directory&#13;&#10;            shutil.move(source_dir, destination_dir)&#13;&#10;            print(f&quot;Directory '{source_dir}' archived to '{destination_dir}'&quot;)&#13;&#10;            return destination_dir&#13;&#10;        except Exception as e:&#13;&#10;            print(f&quot;Error archiving directory '{source_dir}': {e}&quot;)&#13;&#10;            return None&#13;&#10;&#13;&#10;    def restore_directory(self, archive_path, destination_dir):&#13;&#10;        &quot;&quot;&quot;Restores a directory from the archive.&quot;&quot;&quot;&#13;&#10;        try:&#13;&#10;            # Move the directory back&#13;&#10;            shutil.move(archive_path, destination_dir)&#13;&#10;            print(f&quot;Directory '{archive_path}' restored to '{destination_dir}'&quot;)&#13;&#10;            return destination_dir&#13;&#10;        except Exception as e:&#13;&#10;            print(f&quot;Error restoring directory '{archive_path}': {e}&quot;)&#13;&#10;            return None" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/contributor_wall_app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/contributor_wall_app.py" />
              <option name="originalContent" value="// ...existing code...&#10;class ContributorWallApp:&#10;    def __init__(self, root):&#10;// ...existing code...&#10;        self.title_label = tk.Label(root, text=&quot;Nexapod Contributor Wall&quot;,&#10;                                    font=(&quot;Helvetica&quot;, 24, &quot;bold&quot;),&#10;                                    bg=&quot;#2E2E2E&quot;, fg=&quot;#FFFFFF&quot;)&#10;        self.title_label.pack(pady=20)&#10;&#10;        self.canvas = tk.Canvas(root, bg=&quot;#2E2E2E&quot;, highlightthickness=0)&#10;        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)&#10;&#10;        self.scrollbar = tk.Scrollbar(root, orient=tk.VERTICAL,&#10;                                      command=self.canvas.yview)&#10;        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)&#10;        self.canvas.configure(yscrollcommand=self.scrollbar.set)&#10;&#10;        self.contributors_frame = tk.Frame(self.canvas, bg=&quot;#2E2E2E&quot;)&#10;        self.canvas.create_window((0, 0), window=self.contributors_frame,&#10;                                  anchor=tk.NW)&#10;&#10;        self.contributors_data = [&#10;            {&quot;name&quot;: &quot;Kunya&quot;, &quot;image_path&quot;: &quot;assets/kunya.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/kunya66&quot;},&#10;            {&quot;name&quot;: &quot;ChatGPT&quot;, &quot;image_path&quot;: &quot;assets/chatgpt.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/chatgpt&quot;},&#10;            {&quot;name&quot;: &quot;Gemini&quot;, &quot;image_path&quot;: &quot;assets/gemini.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/gemini&quot;},&#10;            {&quot;name&quot;: &quot;Claude&quot;, &quot;image_path&quot;: &quot;assets/claude.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/claude&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 1&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor1&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 2&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor2&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 3&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor3&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 4&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor4&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 5&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor5&quot;},&#10;            {&quot;name&quot;: &quot;Other Contributor 6&quot;,&#10;             &quot;image_path&quot;: &quot;assets/default.png&quot;,&#10;             &quot;github_url&quot;: &quot;https://github.com/contributor6&quot;},&#10;        ]&#10;&#10;        self.create_contributor_widgets()&#10;        self.contributors_frame.bind(&quot;&lt;Configure&gt;&quot;, self.on_frame_configure)&#10;        self.canvas.bind_all(&quot;&lt;MouseWheel&gt;&quot;, self.on_mouse_wheel)&#10;&#10;    def on_frame_configure(self, event):&#10;        self.canvas.configure(scrollregion=self.canvas.bbox(&quot;all&quot;))&#10;&#10;    def on_mouse_wheel(self, event):&#10;        self.canvas.yview_scroll(int(-1 * (event.delta / 120)), &quot;units&quot;)&#10;&#10;    def create_contributor_widgets(self):&#10;        for i, contributor in enumerate(self.contributors_data):&#10;            row, col = divmod(i, 4)&#10;            contributor_frame = tk.Frame(self.contributors_frame,&#10;                                         bg=&quot;#3C3C3C&quot;, relief=tk.RAISED,&#10;                                         borderwidth=2)&#10;            contributor_frame.grid(row=row, column=col, padx=20, pady=20,&#10;                                   sticky=&quot;nsew&quot;)&#10;&#10;            try:&#10;                img = Image.open(contributor[&quot;image_path&quot;])&#10;                img = img.resize((100, 100), Image.Resampling.LANCZOS)&#10;                photo = ImageTk.PhotoImage(img)&#10;                image_label = tk.Label(contributor_frame, image=photo,&#10;                                       bg=&quot;#3C3C3C&quot;)&#10;                image_label.image = photo&#10;                image_label.pack(pady=10)&#10;            except FileNotFoundError:&#10;                # Handle case where image is not found&#10;                placeholder_label = tk.Label(contributor_frame,&#10;                                             text=&quot;Image not found&quot;,&#10;                                             bg=&quot;#3C3C3C&quot;, fg=&quot;#FFFFFF&quot;)&#10;                placeholder_label.pack(pady=10)&#10;&#10;            name_label = tk.Label(contributor_frame, text=contributor[&quot;name&quot;],&#10;                                  font=(&quot;Helvetica&quot;, 12, &quot;bold&quot;),&#10;                                  bg=&quot;#3C3C3C&quot;, fg=&quot;#FFFFFF&quot;)&#10;            name_label.pack()&#10;&#10;            github_link = tk.Label(contributor_frame, text=&quot;GitHub Profile&quot;,&#10;                                   font=(&quot;Helvetica&quot;, 10, &quot;underline&quot;),&#10;                                   fg=&quot;#1E90FF&quot;, bg=&quot;#3C3C3C&quot;, cursor=&quot;hand2&quot;)&#10;            github_link.pack(pady=5)&#10;            github_link.bind(&quot;&lt;Button-1&gt;&quot;, lambda e,&#10;                             url=contributor[&quot;github_url&quot;]: self.open_link(url))&#10;&#10;    def open_link(self, url):&#10;        webbrowser.open_new(url)&#10;&#10;&#10;def main():&#10;    root = tk.Tk()&#10;    app = ContributorWallApp(root)&#10;    root.mainloop()&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;&#10;" />
              <option name="updatedContent" value="import tkinter as tk&#13;&#10;import webbrowser&#13;&#10;from PIL import Image, ImageTk&#13;&#10;&#13;&#10;&#13;&#10;class ContributorWallApp:&#13;&#10;    def __init__(self, root):&#13;&#10;        self.root = root&#13;&#10;        self.root.title(&quot;NexaPod Contributor Wall&quot;)&#13;&#10;        self.root.geometry(&quot;1200x800&quot;)&#13;&#10;        self.root.configure(bg=&quot;#2E2E2E&quot;)&#13;&#10;&#13;&#10;        self.title_label = tk.Label(&#13;&#10;            root,&#13;&#10;            text=&quot;Nexapod Contributor Wall&quot;,&#13;&#10;            font=(&quot;Helvetica&quot;, 24, &quot;bold&quot;),&#13;&#10;            bg=&quot;#2E2E2E&quot;,&#13;&#10;            fg=&quot;#FFFFFF&quot;,&#13;&#10;        )&#13;&#10;        self.title_label.pack(pady=20)&#13;&#10;&#13;&#10;        self.canvas = tk.Canvas(root, bg=&quot;#2E2E2E&quot;, highlightthickness=0)&#13;&#10;        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)&#13;&#10;&#13;&#10;        self.scrollbar = tk.Scrollbar(&#13;&#10;            root, orient=tk.VERTICAL, command=self.canvas.yview&#13;&#10;        )&#13;&#10;        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)&#13;&#10;        self.canvas.configure(yscrollcommand=self.scrollbar.set)&#13;&#10;&#13;&#10;        self.contributors_frame = tk.Frame(self.canvas, bg=&quot;#2E2E2E&quot;)&#13;&#10;        self.canvas.create_window(&#13;&#10;            (0, 0), window=self.contributors_frame, anchor=tk.NW&#13;&#10;        )&#13;&#10;&#13;&#10;        self.contributors_data = [&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Kunya&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/kunya.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/kunya66&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;ChatGPT&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/chatgpt.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/chatgpt&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Gemini&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/gemini.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/gemini&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Claude&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/claude.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/claude&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 1&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor1&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 2&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor2&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 3&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor3&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 4&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor4&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 5&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor5&quot;,&#13;&#10;            },&#13;&#10;            {&#13;&#10;                &quot;name&quot;: &quot;Other Contributor 6&quot;,&#13;&#10;                &quot;image_path&quot;: &quot;assets/default.png&quot;,&#13;&#10;                &quot;github_url&quot;: &quot;https://github.com/contributor6&quot;,&#13;&#10;            },&#13;&#10;        ]&#13;&#10;&#13;&#10;        self.create_contributor_widgets()&#13;&#10;        self.contributors_frame.bind(&quot;&lt;Configure&gt;&quot;, self.on_frame_configure)&#13;&#10;        self.canvas.bind_all(&quot;&lt;MouseWheel&gt;&quot;, self.on_mouse_wheel)&#13;&#10;&#13;&#10;    def on_frame_configure(self, event):&#13;&#10;        self.canvas.configure(scrollregion=self.canvas.bbox(&quot;all&quot;))&#13;&#10;&#13;&#10;    def on_mouse_wheel(self, event):&#13;&#10;        self.canvas.yview_scroll(int(-1 * (event.delta / 120)), &quot;units&quot;)&#13;&#10;&#13;&#10;    def create_contributor_widgets(self):&#13;&#10;        for i, contributor in enumerate(self.contributors_data):&#13;&#10;            row, col = divmod(i, 4)&#13;&#10;            contributor_frame = tk.Frame(&#13;&#10;                self.contributors_frame,&#13;&#10;                bg=&quot;#3C3C3C&quot;,&#13;&#10;                relief=tk.RAISED,&#13;&#10;                borderwidth=2,&#13;&#10;            )&#13;&#10;            contributor_frame.grid(&#13;&#10;                row=row, column=col, padx=20, pady=20, sticky=&quot;nsew&quot;&#13;&#10;            )&#13;&#10;&#13;&#10;            try:&#13;&#10;                img = Image.open(contributor[&quot;image_path&quot;])&#13;&#10;                img = img.resize((100, 100), Image.Resampling.LANCZOS)&#13;&#10;                photo = ImageTk.PhotoImage(img)&#13;&#10;                image_label = tk.Label(&#13;&#10;                    contributor_frame, image=photo, bg=&quot;#3C3C3C&quot;&#13;&#10;                )&#13;&#10;                image_label.image = photo&#13;&#10;                image_label.pack(pady=10)&#13;&#10;            except FileNotFoundError:&#13;&#10;                # Handle case where image is not found&#13;&#10;                placeholder_label = tk.Label(&#13;&#10;                    contributor_frame,&#13;&#10;                    text=&quot;Image not found&quot;,&#13;&#10;                    bg=&quot;#3C3C3C&quot;,&#13;&#10;                    fg=&quot;#FFFFFF&quot;,&#13;&#10;                )&#13;&#10;                placeholder_label.pack(pady=10)&#13;&#10;&#13;&#10;            name_label = tk.Label(&#13;&#10;                contributor_frame,&#13;&#10;                text=contributor[&quot;name&quot;],&#13;&#10;                font=(&quot;Helvetica&quot;, 12, &quot;bold&quot;),&#13;&#10;                bg=&quot;#3C3C3C&quot;,&#13;&#10;                fg=&quot;#FFFFFF&quot;,&#13;&#10;            )&#13;&#10;            name_label.pack()&#13;&#10;&#13;&#10;            github_link = tk.Label(&#13;&#10;                contributor_frame,&#13;&#10;                text=&quot;GitHub Profile&quot;,&#13;&#10;                font=(&quot;Helvetica&quot;, 10, &quot;underline&quot;),&#13;&#10;                fg=&quot;#1E90FF&quot;,&#13;&#10;                bg=&quot;#3C3C3C&quot;,&#13;&#10;                cursor=&quot;hand2&quot;,&#13;&#10;            )&#13;&#10;            github_link.pack(pady=5)&#13;&#10;            github_link.bind(&#13;&#10;                &quot;&lt;Button-1&gt;&quot;,&#13;&#10;                lambda e, url=contributor[&quot;github_url&quot;]: self.open_link(url),&#13;&#10;            )&#13;&#10;&#13;&#10;    def open_link(self, url):&#13;&#10;        webbrowser.open_new(url)&#13;&#10;&#13;&#10;&#13;&#10;def main():&#13;&#10;    root = tk.Tk()&#13;&#10;    app = ContributorWallApp(root)&#13;&#10;    root.mainloop()&#13;&#10;&#13;&#10;&#13;&#10;if __name__ == &quot;__main__&quot;:&#13;&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/dashboard.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/dashboard.py" />
              <option name="updatedContent" value="// ...existing code...&#10;from .contributor_wall_app import ContributorWallApp&#10;from .nexapod_client import NexaPodClient&#10;&#10;# Constants&#10;// ...existing code...&#10;# ...existing code...&#10;class ToolTip:&#10;    def __init__(self, widget, text):&#10;// ...existing code...&#10;        self.widget.bind(&quot;&lt;Leave&gt;&quot;, self.hidetip)&#10;&#10;    def showtip(self, event=None):&#10;// ...existing code...&#10;        x = y = 0&#10;        x, y, cx, cy = self.widget.bbox(&quot;insert&quot;)&#10;        x += self.widget.winfo_rootx() + 25&#10;        y += self.widget.winfo_rooty() + 20&#10;        self.tw.wm_geometry(f&quot;+{x}+{y}&quot;)&#10;        self.tw.wm_deiconify()&#10;&#10;    def hidetip(self, event=None):&#10;        if self.tw:&#10;            self.tw.wm_withdraw()&#10;&#10;&#10;class Dashboard:&#10;    def __init__(self, root):&#10;        self.root = root&#10;// ...existing code...&#10;        self.root.title(&quot;NexaPod Dashboard&quot;)&#10;        self.root.geometry(&quot;1200x800&quot;)&#10;        self.root.configure(bg=BG_COLOR)&#10;&#10;        self.client = NexaPodClient()&#10;        self.style = ttk.Style()&#10;        self.style.theme_use('clam')&#10;        self.configure_styles()&#10;&#10;        self.create_widgets()&#10;&#10;    def configure_styles(self):&#10;        self.style.configure(&quot;TFrame&quot;, background=BG_COLOR)&#10;        self.style.configure(&quot;TLabel&quot;, background=BG_COLOR,&#10;                             foreground=TEXT_COLOR,&#10;                             font=(FONT_FAMILY, 12))&#10;        self.style.configure(&quot;Header.TLabel&quot;, font=(FONT_FAMILY, 24, &quot;bold&quot;))&#10;        self.style.configure(&quot;TButton&quot;, background=BUTTON_COLOR,&#10;                             foreground=TEXT_COLOR,&#10;                             font=(FONT_FAMILY, 12, &quot;bold&quot;),&#10;                             borderwidth=1)&#10;        self.style.map(&quot;TButton&quot;,&#10;                       background=[('active', BUTTON_HOVER_COLOR)])&#10;        self.style.configure(&quot;TEntry&quot;,&#10;                             fieldbackground=ENTRY_BG_COLOR,&#10;                             foreground=TEXT_COLOR,&#10;                             insertcolor=TEXT_COLOR)&#10;        self.style.configure(&quot;Treeview&quot;,&#10;                             background=ENTRY_BG_COLOR,&#10;                             foreground=TEXT_COLOR,&#10;                             fieldbackground=ENTRY_BG_COLOR,&#10;                             font=(FONT_FAMILY, 10))&#10;        self.style.map(&quot;Treeview&quot;,&#10;                       background=[('selected', BUTTON_COLOR)])&#10;        self.style.configure(&quot;Treeview.Heading&quot;,&#10;                             background=BUTTON_COLOR,&#10;                             foreground=TEXT_COLOR,&#10;                             font=(FONT_FAMILY, 12, &quot;bold&quot;))&#10;&#10;    def create_widgets(self):&#10;        # Header&#10;        header_frame = ttk.Frame(self.root)&#10;        header_frame.pack(pady=20, padx=20, fill='x')&#10;        header_label = ttk.Label(header_frame, text=&quot;NexaPod Dashboard&quot;,&#10;                                 style=&quot;Header.TLabel&quot;)&#10;        header_label.pack()&#10;&#10;        # Main content frame&#10;        main_frame = ttk.Frame(self.root)&#10;        main_frame.pack(expand=True, fill='both', padx=20, pady=10)&#10;&#10;        # Left panel for controls&#10;        left_panel = ttk.Frame(main_frame, width=400)&#10;        left_panel.pack(side='left', fill='y', padx=(0, 10))&#10;        left_panel.pack_propagate(False)&#10;&#10;        # Right panel for logs and info&#10;        right_panel = ttk.Frame(main_frame)&#10;        right_panel.pack(side='right', expand=True, fill='both')&#10;&#10;        # Control sections in left panel&#10;        self.create_profile_section(left_panel)&#10;        self.create_task_section(left_panel)&#10;        self.create_network_section(left_panel)&#10;        self.create_actions_section(left_panel)&#10;&#10;        # Info sections in right panel&#10;        self.create_log_section(right_panel)&#10;        self.create_stats_section(right_panel)&#10;&#10;    def create_profile_section(self, parent):&#10;        profile_frame = ttk.LabelFrame(parent, text=&quot;User Profile&quot;,&#10;                                       padding=(10, 5))&#10;        profile_frame.pack(pady=10, fill='x')&#10;&#10;        ttk.Label(profile_frame, text=&quot;Username:&quot;).grid(row=0, column=0,&#10;                                                        sticky='w', pady=2)&#10;        self.username_entry = ttk.Entry(profile_frame, width=30)&#10;        self.username_entry.grid(row=0, column=1, sticky='ew', pady=2)&#10;&#10;        ttk.Label(profile_frame, text=&quot;Private Key:&quot;).grid(row=1, column=0,&#10;                                                           sticky='w', pady=2)&#10;        self.private_key_entry = ttk.Entry(profile_frame, width=30, show=&quot;*&quot;)&#10;        self.private_key_entry.grid(row=1, column=1, sticky='ew', pady=2)&#10;&#10;        profile_buttons_frame = ttk.Frame(profile_frame)&#10;        profile_buttons_frame.grid(row=2, column=0, columnspan=2, pady=5)&#10;&#10;        self.load_profile_button = ttk.Button(profile_buttons_frame,&#10;                                              text=&quot;Load Profile&quot;,&#10;                                              command=self.load_profile)&#10;        self.load_profile_button.pack(side='left', padx=5)&#10;        self.create_profile_button = ttk.Button(profile_buttons_frame,&#10;                                                text=&quot;Create Profile&quot;,&#10;                                                command=self.create_profile)&#10;        self.create_profile_button.pack(side='left', padx=5)&#10;&#10;    def create_task_section(self, parent):&#10;        task_frame = ttk.LabelFrame(parent, text=&quot;Task Management&quot;,&#10;                                    padding=(10, 5))&#10;        task_frame.pack(pady=10, fill='x')&#10;&#10;        ttk.Label(task_frame, text=&quot;Task Type:&quot;).grid(row=0, column=0,&#10;                                                      sticky='w', pady=2)&#10;        self.task_type_combo = ttk.Combobox(&#10;            task_frame,&#10;            values=[&quot;protein_folding&quot;, &quot;molecular_docking&quot;, &quot;image_analysis&quot;])&#10;        self.task_type_combo.grid(row=0, column=1, sticky='ew', pady=2)&#10;        self.task_type_combo.set(&quot;protein_folding&quot;)&#10;&#10;        ttk.Label(task_frame, text=&quot;Task Data (JSON):&quot;).grid(row=1, column=0,&#10;                                                             sticky='w',&#10;                                                             pady=2)&#10;        self.task_data_text = tk.Text(task_frame, height=5, width=40,&#10;                                      bg=ENTRY_BG_COLOR, fg=TEXT_COLOR,&#10;                                      insertbackground=TEXT_COLOR)&#10;        self.task_data_text.grid(row=2, column=0, columnspan=2,&#10;                                 sticky='ew', pady=2)&#10;&#10;        self.submit_task_button = ttk.Button(task_frame, text=&quot;Submit Task&quot;,&#10;                                             command=self.submit_task)&#10;        self.submit_task_button.grid(row=3, column=0, columnspan=2, pady=10)&#10;&#10;    def create_network_section(self, parent):&#10;        network_frame = ttk.LabelFrame(parent, text=&quot;Network Status&quot;,&#10;                                       padding=(10, 5))&#10;        network_frame.pack(pady=10, fill='x')&#10;&#10;        self.network_status_label = ttk.Label(network_frame,&#10;                                              text=&quot;Status: Disconnected&quot;,&#10;                                              foreground=&quot;red&quot;)&#10;        self.network_status_label.pack(pady=5)&#10;&#10;        self.connect_button = ttk.Button(network_frame, text=&quot;Connect&quot;,&#10;                                         command=self.connect_to_network)&#10;        self.connect_button.pack(pady=5)&#10;&#10;    def create_actions_section(self, parent):&#10;        actions_frame = ttk.LabelFrame(parent, text=&quot;Actions&quot;,&#10;                                       padding=(10, 5))&#10;        actions_frame.pack(pady=10, fill='x')&#10;&#10;        self.view_tasks_button = ttk.Button(actions_frame,&#10;                                            text=&quot;View My Tasks&quot;,&#10;                                            command=self.view_my_tasks)&#10;        self.view_tasks_button.pack(fill='x', pady=5)&#10;&#10;        self.contributor_wall_button = ttk.Button(&#10;            actions_frame,&#10;            text=&quot;Show Contributor Wall&quot;,&#10;            command=self.show_contributor_wall)&#10;        self.contributor_wall_button.pack(fill='x', pady=5)&#10;&#10;    def create_log_section(self, parent):&#10;        log_frame = ttk.LabelFrame(parent, text=&quot;Logs&quot;, padding=(10, 5))&#10;        log_frame.pack(expand=True, fill='both', pady=(0, 10))&#10;&#10;        self.log_text = tk.Text(log_frame, height=15,&#10;                                bg=ENTRY_BG_COLOR, fg=TEXT_COLOR,&#10;                                state='disabled', wrap='word')&#10;        self.log_text.pack(expand=True, fill='both', padx=5, pady=5)&#10;&#10;        log_scrollbar = ttk.Scrollbar(self.log_text,&#10;                                      command=self.log_text.yview)&#10;        log_scrollbar.pack(side='right', fill='y')&#10;        self.log_text['yscrollcommand'] = log_scrollbar.set&#10;&#10;    def create_stats_section(self, parent):&#10;        stats_frame = ttk.LabelFrame(parent, text=&quot;Statistics&quot;,&#10;                                     padding=(10, 5))&#10;        stats_frame.pack(fill='x')&#10;&#10;        self.stats_tree = ttk.Treeview(stats_frame, columns=(&quot;Value&quot;),&#10;                                       show=&quot;headings&quot;, height=5)&#10;        self.stats_tree.heading(&quot;Value&quot;, text=&quot;Value&quot;)&#10;        self.stats_tree.column(&quot;Value&quot;, width=150)&#10;        self.stats_tree.pack(expand=True, fill='both', padx=5, pady=5)&#10;&#10;        self.stats_tree.insert(&quot;&quot;, &quot;end&quot;, text=&quot;Reputation&quot;,&#10;                               values=(&quot;N/A&quot;,))&#10;        self.stats_tree.insert(&quot;&quot;, &quot;end&quot;, text=&quot;Tasks Completed&quot;,&#10;                               values=(&quot;N/A&quot;,))&#10;        self.stats_tree.insert(&quot;&quot;, &quot;end&quot;, text=&quot;NexaTokens Earned&quot;,&#10;                               values=(&quot;N/A&quot;,))&#10;        self.stats_tree.insert(&quot;&quot;, &quot;end&quot;, text=&quot;Tier&quot;,&#10;                               values=(&quot;N/A&quot;,))&#10;        self.stats_tree.insert(&quot;&quot;, &quot;end&quot;, text=&quot;Network Peers&quot;,&#10;                               values=(&quot;N/A&quot;,))&#10;&#10;    def log(self, message):&#10;        self.log_text.config(state='normal')&#10;        self.log_text.insert('end', f&quot;{message}\n&quot;)&#10;        self.log_text.config(state='disabled')&#10;        self.log_text.see('end')&#10;&#10;    def load_profile(self):&#10;        username = self.username_entry.get()&#10;        private_key = self.private_key_entry.get()&#10;        if not username or not private_key:&#10;            messagebox.showerror(&quot;Error&quot;,&#10;                                 &quot;Username and Private Key are required.&quot;)&#10;            return&#10;        try:&#10;            self.client.load_profile(username, private_key)&#10;            self.log(f&quot;Profile for '{username}' loaded successfully.&quot;)&#10;            self.update_stats()&#10;        except Exception as e:&#10;            messagebox.showerror(&quot;Profile Load Error&quot;, str(e))&#10;            self.log(f&quot;Failed to load profile for '{username}': {e}&quot;)&#10;&#10;    def create_profile(self):&#10;        username = self.username_entry.get()&#10;        if not username:&#10;            messagebox.showerror(&quot;Error&quot;, &quot;Username is required.&quot;)&#10;            return&#10;        try:&#10;            private_key = self.client.create_profile(username)&#10;            self.private_key_entry.delete(0, 'end')&#10;            self.private_key_entry.insert(0, private_key)&#10;            self.log(f&quot;Profile for '{username}' created.&quot;)&#10;            self.log(&quot;Your private key has been generated. &quot;&#10;                     &quot;Store it securely!&quot;)&#10;            self.update_stats()&#10;        except Exception as e:&#10;            messagebox.showerror(&quot;Profile Creation Error&quot;, str(e))&#10;            self.log(f&quot;Failed to create profile for '{username}': {e}&quot;)&#10;&#10;    def submit_task(self):&#10;        if not self.client.profile:&#10;            messagebox.showerror(&quot;Error&quot;, &quot;Please load or create a profile &quot;&#10;                                          &quot;before submitting a task.&quot;)&#10;            return&#10;&#10;        task_type = self.task_type_combo.get()&#10;        task_data_str = self.task_data_text.get(&quot;1.0&quot;, 'end-1c')&#10;&#10;        if not task_type or not task_data_str:&#10;            messagebox.showerror(&quot;Error&quot;, &quot;Task Type and Task Data are &quot;&#10;                                          &quot;required.&quot;)&#10;            return&#10;&#10;        try:&#10;            task_data = json.loads(task_data_str)&#10;            task_id = self.client.submit_task(task_type, task_data)&#10;            self.log(f&quot;Task '{task_id}' submitted successfully.&quot;)&#10;            messagebox.showinfo(&quot;Success&quot;,&#10;                                f&quot;Task submitted with ID: {task_id}&quot;)&#10;        except json.JSONDecodeError:&#10;            messagebox.showerror(&quot;Error&quot;, &quot;Invalid JSON in Task Data.&quot;)&#10;            self.log(&quot;Error: Invalid JSON format in task data.&quot;)&#10;        except Exception as e:&#10;            messagebox.showerror(&quot;Task Submission Error&quot;, str(e))&#10;            self.log(f&quot;Failed to submit task: {e}&quot;)&#10;&#10;    def connect_to_network(self):&#10;        self.log(&quot;Attempting to connect to the NexaPod network...&quot;)&#10;        # This is a placeholder for actual network connection logic&#10;        # In a real app, this would involve P2P discovery, etc.&#10;        self.network_status_label.config(text=&quot;Status: Connected&quot;,&#10;                                         foreground=&quot;green&quot;)&#10;        self.log(&quot;Successfully connected to the network.&quot;)&#10;        # Simulate updating peer count&#10;        self.stats_tree.item(self.stats_tree.get_children()[4],&#10;                             values=(f&quot;{random.randint(5, 50)}&quot;,))&#10;&#10;    def update_stats(self):&#10;        if self.client.profile:&#10;            reputation = self.client.get_reputation()&#10;            tasks_completed = len(self.client.get_completed_tasks())&#10;            # Placeholder for token calculation&#10;            tokens = tasks_completed * 10&#10;            tier = self.client.get_tier()&#10;&#10;            self.stats_tree.item(self.stats_tree.get_children()[0],&#10;                                 values=(f&quot;{reputation}&quot;,))&#10;            self.stats_tree.item(self.stats_tree.get_children()[1],&#10;                                 values=(f&quot;{tasks_completed}&quot;,))&#10;            self.stats_tree.item(self.stats_tree.get_children()[2],&#10;                                 values=(f&quot;{tokens} NT&quot;,))&#10;            self.stats_tree.item(self.stats_tree.get_children()[3],&#10;                                 values=(f&quot;{tier}&quot;,))&#10;            self.log(&quot;User stats updated.&quot;)&#10;        else:&#10;            self.log(&quot;Cannot update stats, no profile loaded.&quot;)&#10;&#10;    def view_my_tasks(self):&#10;        if not self.client.profile:&#10;            messagebox.showerror(&quot;Error&quot;, &quot;Please load a profile first.&quot;)&#10;            return&#10;&#10;        tasks = self.client.get_all_tasks()&#10;        if not tasks:&#10;            messagebox.showinfo(&quot;My Tasks&quot;, &quot;You have no tasks.&quot;)&#10;            return&#10;&#10;        task_window = tk.Toplevel(self.root)&#10;        task_window.title(&quot;My Tasks&quot;)&#10;        task_window.geometry(&quot;600x400&quot;)&#10;        task_window.configure(bg=BG_COLOR)&#10;&#10;        task_tree = ttk.Treeview(task_window,&#10;                                 columns=(&quot;ID&quot;, &quot;Type&quot;, &quot;Status&quot;),&#10;                                 show=&quot;headings&quot;)&#10;        task_tree.heading(&quot;ID&quot;, text=&quot;Task ID&quot;)&#10;        task_tree.heading(&quot;Type&quot;, text=&quot;Type&quot;)&#10;        task_tree.heading(&quot;Status&quot;, text=&quot;Status&quot;)&#10;        task_tree.pack(expand=True, fill='both', padx=10, pady=10)&#10;&#10;        for task_id, task_info in tasks.items():&#10;            task_tree.insert(&quot;&quot;, &quot;end&quot;, values=(&#10;                task_id, task_info['type'], task_info['status']))&#10;&#10;    def show_contributor_wall(self):&#10;        contributor_window = tk.Toplevel(self.root)&#10;        contributor_window.title(&quot;Contributor Wall&quot;)&#10;        contributor_window.geometry(&quot;800x600&quot;)&#10;        ContributorWallApp(contributor_window)&#10;&#10;&#10;def main():&#10;    root = tk.Tk()&#10;    app = Dashboard(root)&#10;    root.mainloop()&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/descriptor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/descriptor.py" />
              <option name="updatedContent" value="import json&#10;&#10;&#10;class Descriptor:&#10;    def __init__(self, file_path):&#10;        self.file_path = file_path&#10;// ...existing code...&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/executor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/executor.py" />
              <option name="updatedContent" value="import subprocess&#10;import json&#10;&#10;&#10;class Executor:&#10;    def execute_task(self, task_data):&#10;        &quot;&quot;&quot;&#10;        Executes a task based on its type.&#10;        This is a simplified executor. A real implementation would be more&#10;        robust, sandboxed, and support different environments.&#10;        &quot;&quot;&quot;&#10;        task_type = task_data.get('type')&#10;        if task_type == 'execute_script':&#10;            script_path = task_data.get('script_path')&#10;            try:&#10;                result = subprocess.run(['python', script_path],&#10;                                        capture_output=True, text=True,&#10;                                        check=True)&#10;                return {&quot;stdout&quot;: result.stdout, &quot;stderr&quot;: result.stderr}&#10;            except subprocess.CalledProcessError as e:&#10;                return {&quot;error&quot;: str(e), &quot;stdout&quot;: e.stdout,&#10;                        &quot;stderr&quot;: e.stderr}&#10;            except FileNotFoundError:&#10;                return {&quot;error&quot;: f&quot;Script not found at {script_path}&quot;}&#10;        elif task_type == 'data_processing':&#10;            # Example of another task type&#10;            data = task_data.get('data')&#10;            # Process the data in some way&#10;            processed_data = {&quot;processed_length&quot;: len(data)}&#10;            return {&quot;result&quot;: processed_data}&#10;        else:&#10;            return {&quot;error&quot;: f&quot;Unsupported task type: {task_type}&quot;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/input_fetch.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/input_fetch.py" />
              <option name="updatedContent" value="import requests&#10;&#10;&#10;class InputFetcher:&#10;    def fetch(self, url):&#10;        try:&#10;// ...existing code...&#10;            response.raise_for_status()&#10;            return response.text&#10;        except requests.exceptions.RequestException as e:&#10;            print(f&quot;Error fetching input from {url}: {e}&quot;)&#10;            return None&#10;&#10;&#10;class FileInputFetcher:&#10;    def fetch(self, file_path):&#10;        try:&#10;// ...existing code...&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/ledger.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/ledger.py" />
              <option name="updatedContent" value="// ...existing code...&#10;class Ledger:&#10;    def __init__(self, db_path='ledger.db'):&#10;// ...existing code...&#10;        self._create_tables()&#10;&#10;    def _create_tables(self):&#10;        &quot;&quot;&quot;Create the necessary tables if they don't exist.&quot;&quot;&quot;&#10;// ...existing code...&#10;            c.execute('''&#10;                CREATE TABLE IF NOT EXISTS transactions (&#10;                    id TEXT PRIMARY KEY,&#10;                    timestamp REAL,&#10;                    type TEXT,&#10;                    data TEXT,&#10;                    signature TEXT&#10;                )&#10;            ''')&#10;            self.conn.commit()&#10;&#10;    def add_transaction(self, transaction):&#10;// ...existing code...&#10;        with self.lock:&#10;            c = self.conn.cursor()&#10;            c.execute(&quot;INSERT INTO transactions VALUES (?, ?, ?, ?, ?)&quot;,&#10;                      (tx_id, tx_timestamp, tx_type, tx_data, tx_signature))&#10;            self.conn.commit()&#10;        return tx_id&#10;&#10;&#10;class Transaction:&#10;    def __init__(self, tx_type, data, key_manager):&#10;        self.id = None&#10;// ...existing code...&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/logger.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/logger.py" />
              <option name="updatedContent" value="import logging&#10;from cryptography.hazmat.primitives import hashes&#10;from cryptography.hazmat.primitives.asymmetric import padding&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO,&#10;// ...existing code...&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/../../../../Client/nexapod_client.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/../../../../Client/nexapod_client.py" />
              <option name="updatedContent" value="// ...existing code...&#10;class NexaPodClient:&#10;    def __init__(self, server_url=&quot;http://127.0.0.1:5000&quot;):&#10;        self.server_url = server_url&#10;// ...existing code...&#10;        self.reputation_manager = ReputationManager()&#10;        self.tier_manager = TierManager()&#10;&#10;    def create_profile(self, username):&#10;        if self.profiles.get_profile(username):&#10;            raise ValueError(f&quot;Profile for '{username}' already exists.&quot;)&#10;        private_key, public_key = self.key_manager.generate_keys()&#10;        self.profiles.create_profile(username, public_key)&#10;        # In a real app, the private key should be handled more securely&#10;        # (e.g., encrypted storage)&#10;        print(f&quot;Generated private key for {username}: {private_key}&quot;)&#10;        self.load_profile(username, private_key)&#10;        return private_key&#10;&#10;    def load_profile(self, username, private_key):&#10;        profile_data = self.profiles.get_profile(username)&#10;        if not profile_data:&#10;            raise ValueError(f&quot;No profile found for '{username}'.&quot;)&#10;        # In a real app, you'd verify the private key matches the public key&#10;        self.profile = {&quot;username&quot;: username, &quot;private_key&quot;: private_key,&#10;                        &quot;public_key&quot;: profile_data['public_key']}&#10;        print(f&quot;Profile for {username} loaded.&quot;)&#10;&#10;    def submit_task(self, task_type, task_data):&#10;        if not self.profile:&#10;            raise ConnectionError(&quot;No profile loaded. Please create or load &quot;&#10;                                  &quot;a profile first.&quot;)&#10;&#10;        task = {&#10;            &quot;id&quot;: str(uuid.uuid4()),&#10;            &quot;type&quot;: task_type,&#10;            &quot;data&quot;: task_data,&#10;            &quot;submitter&quot;: self.profile['username'],&#10;            &quot;timestamp&quot;: time.time(),&#10;            &quot;status&quot;: &quot;pending&quot;&#10;        }&#10;        # Sign the task to prove identity&#10;        signature = self.key_manager.sign(self.profile['private_key'],&#10;                                          json.dumps(task, sort_keys=True))&#10;        &#10;        payload = {&quot;task&quot;: task, &quot;signature&quot;: signature}&#10;&#10;        try:&#10;            response = requests.post(f&quot;{self.server_url}/submit_task&quot;,&#10;                                     json=payload)&#10;            response.raise_for_status()&#10;            self.ledger.add_transaction(&#10;                Transaction(&quot;submit_task&quot;, payload, self.key_manager))&#10;            return task['id']&#10;        except requests.exceptions.RequestException as e:&#10;            print(f&quot;Error submitting task to server: {e}&quot;)&#10;            raise&#10;&#10;    def get_reputation(self):&#10;        if not self.profile:&#10;            return 0&#10;        return self.reputation_manager.get_reputation(&#10;            self.profile['username'])&#10;&#10;    def get_completed_tasks(self):&#10;        # This would fetch from a local DB or the network&#10;        return []&#10;&#10;    def get_all_tasks(self):&#10;        # Placeholder to get tasks from the ledger/local state&#10;        return {}&#10;&#10;    def get_tier(self):&#10;        if not self.profile:&#10;            return &quot;N/A&quot;&#10;        reputation = self.get_reputation()&#10;        return self.tier_manager.get_tier(reputation)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/.github/workflows/ci.yml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.github/workflows/ci.yml" />
              <option name="originalContent" value="name: CI/CD Pipeline&#10;&#10;on:&#10;  push:&#10;    branches: [ main ]&#10;  pull_request:&#10;    branches: [ main ]&#10;&#10;jobs:&#10;  test:&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - uses: actions/checkout@v3&#10;      - name: Set up Python&#10;        uses: actions/setup-python@v4&#10;        with:&#10;          python-version: '3.11'&#10;      - name: Install dependencies&#10;        run: pip install -r requirements.txt&#10;      - name: Run tests (placeholder)&#10;        run: |&#10;          # Add your test commands here, e.g., pytest&#10;          echo &quot;No tests configured. Passing by default.&quot;&#10;&#10;  build_and_push:&#10;    needs: test&#10;    runs-on: ubuntu-latest&#10;    # Only run this job on pushes to the main branch, not on pull requests&#10;    if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'&#10;    steps:&#10;      - uses: actions/checkout@v3&#10;&#10;      - name: Log in to GitHub Container Registry&#10;        uses: docker/login-action@v2&#10;        with:&#10;          registry: ghcr.io&#10;          username: ${{ github.actor }}&#10;          password: ${{ secrets.GITHUB_TOKEN }}&#10;&#10;      - name: Build and push images&#10;        run: |&#10;          OWNER_LC=$(echo &quot;${{ github.repository_owner }}&quot; | tr '[:upper:]' '[:lower:]')&#10;          # The '.' at the end of this command sets the build context to the project root.&#10;          docker build -t ghcr.io/$OWNER_LC/nexapod-server:latest -f Infrastructure/Dockerfile.server .&#10;          docker build -t ghcr.io/$OWNER_LC/nexapod-client:latest -f Infrastructure/Dockerfile.client .&#10;          docker push ghcr.io/$OWNER_LC/nexapod-server:latest&#10;          docker push ghcr.io/$OWNER_LC/nexapod-client:latest&#10;&#10;  deploy:&#10;    needs: build_and_push&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - name: Deploy to DigitalOcean&#10;        uses: appleboy/ssh-action@master&#10;        with:&#10;          host: ${{ secrets.DO_HOST }}&#10;          username: ${{ secrets.DO_USERNAME }}&#10;          key: ${{ secrets.DO_SSH_KEY }}&#10;          script: |&#10;            OWNER_LC=$(echo &quot;${{ github.repository_owner }}&quot; | tr '[:upper:]' '[:lower:]')&#10;            # Log in to GHCR using a long-lived Deploy Token&#10;            echo ${{ secrets.DEPLOY_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin&#10;            &#10;            # Stop and remove the old container if it exists&#10;            docker stop nexapod-server || true&#10;            docker rm nexapod-server || true&#10;            &#10;            # Pull the latest server image&#10;            docker pull ghcr.io/$OWNER_LC/nexapod-server:latest&#10;            &#10;            # Run the new container&#10;            docker run -d \&#10;              --name nexapod-server \&#10;              --restart always \&#10;              -p 8000:8000 \&#10;              ghcr.io/$OWNER_LC/nexapod-server:latest&#10;" />
              <option name="updatedContent" value="name: CI/CD Pipeline&#10;&#10;on:&#10;  push:&#10;    branches: [ main ]&#10;  pull_request:&#10;    branches: [ main ]&#10;&#10;jobs:&#10;  test:&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - uses: actions/checkout@v3&#10;      - name: Set up Python&#10;        uses: actions/setup-python@v4&#10;        with:&#10;          python-version: '3.11'&#10;      - name: Install dependencies&#10;        # The root requirements file is used for comprehensive testing&#10;        run: pip install -r requirements.txt&#10;      - name: Run tests (placeholder)&#10;        run: |&#10;          # Add your test commands here, e.g., pytest&#10;          echo &quot;No tests configured. Passing by default.&quot;&#10;&#10;  build_and_push:&#10;    needs: test&#10;    runs-on: ubuntu-latest&#10;    # Only run this job on pushes to the main branch, not on pull requests&#10;    if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'&#10;    steps:&#10;      - uses: actions/checkout@v3&#10;&#10;      - name: Log in to GitHub Container Registry&#10;        uses: docker/login-action@v2&#10;        with:&#10;          registry: ghcr.io&#10;          username: ${{ github.actor }}&#10;          password: ${{ secrets.GITHUB_TOKEN }}&#10;&#10;      - name: Build and push images&#10;        run: |&#10;          OWNER_LC=$(echo &quot;${{ github.repository_owner }}&quot; | tr '[:upper:]' '[:lower:]')&#10;          # The '.' at the end of this command sets the build context to the project root.&#10;          docker build -t ghcr.io/$OWNER_LC/nexapod-server:latest -f Infrastructure/Dockerfile.server .&#10;          docker build -t ghcr.io/$OWNER_LC/nexapod-client:latest -f Infrastructure/Dockerfile.client .&#10;          docker push ghcr.io/$OWNER_LC/nexapod-server:latest&#10;          docker push ghcr.io/$OWNER_LC/nexapod-client:latest&#10;&#10;  deploy:&#10;    needs: build_and_push&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - name: Deploy to DigitalOcean&#10;        uses: appleboy/ssh-action@master&#10;        with:&#10;          host: ${{ secrets.DO_HOST }}&#10;          username: ${{ secrets.DO_USERNAME }}&#10;          key: ${{ secrets.DO_SSH_KEY }}&#10;          script: |&#10;            OWNER_LC=$(echo &quot;${{ github.repository_owner }}&quot; | tr '[:upper:]' '[:lower:]')&#10;            # Log in to GHCR using a long-lived Deploy Token&#10;            echo ${{ secrets.DEPLOY_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin&#10;            &#10;            # Stop and remove the old container if it exists&#10;            docker stop nexapod-server || true&#10;            docker rm nexapod-server || true&#10;            &#10;            # Pull the latest server image&#10;            docker pull ghcr.io/$OWNER_LC/nexapod-server:latest&#10;            &#10;            # Run the new container&#10;            docker run -d \&#10;              --name nexapod-server \&#10;              --restart always \&#10;              -p 8000:8000 \&#10;              ghcr.io/$OWNER_LC/nexapod-server:latest" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Client/comms.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Client/comms.py" />
              <option name="originalContent" value="import time&#10;import requests&#10;&#10;&#10;class CoordinatorClient:&#10;    &quot;&quot;&quot;Client for interacting with the coordinator API.&quot;&quot;&quot;&#10;    def __init__(self, config: dict):&#10;        self.url = config['coordinator_url']&#10;        self.node_id = config.get('node_id')&#10;        self.poll_interval = config.get('poll_interval', 10)&#10;&#10;    def register_node(self, profile: dict):&#10;        &quot;&quot;&quot;Register this node with the coordinator.&quot;&quot;&quot;&#10;        resp = requests.post(f&quot;{self.url}/register&quot;, json=profile)&#10;        if resp.ok:&#10;            self.node_id = resp.json().get('node_id')&#10;        else:&#10;            raise Exception(f&quot;Registration failed: {resp.text}&quot;)&#10;&#10;    def poll_job(self) -&gt; dict:&#10;        &quot;&quot;&quot;Poll coordinator for a new job.&quot;&quot;&quot;&#10;        resp = requests.get(&#10;            f&quot;{self.url}/job&quot;,&#10;            params={'node_id': self.node_id}&#10;        )&#10;        if resp.ok and resp.json():&#10;            return resp.json()&#10;        time.sleep(self.poll_interval)&#10;        return None&#10;&#10;    def submit_result(self, result: dict) -&gt; dict:&#10;        &quot;&quot;&quot;Submit execution result back to coordinator.&quot;&quot;&quot;&#10;        resp = requests.post(f&quot;{self.url}/result&quot;, json=result)&#10;        if resp.ok:&#10;            return resp.json()&#10;        raise Exception(f&quot;Result submission failed: {resp.text}&quot;)&#10;&#10;    def submit_job(self, param: dict) -&gt; dict:&#10;        &quot;&quot;&quot;Submit a new job to the coordinator.&quot;&quot;&quot;&#10;        resp = requests.post(f&quot;{self.url}/jobs&quot;, json=param)&#10;        if not resp.ok:&#10;            raise Exception(f&quot;Job submission failed: {resp.text}&quot;)&#10;        return resp.json()&#10;&#10;    def get_status(self) -&gt; dict:&#10;        &quot;&quot;&quot;Check node status with coordinator.&quot;&quot;&quot;&#10;        resp = requests.get(&#10;            f&quot;{self.url}/status&quot;,&#10;            params={'node_id': self.node_id}&#10;        )&#10;        if resp.ok:&#10;            return resp.json()&#10;        raise Exception(f&quot;Status check failed: {resp.text}&quot;)&#10;&#10;&#10;def get_node_profile() -&gt; dict:&#10;    &quot;&quot;&quot;Simulate a static node profile for testing.&quot;&quot;&quot;&#10;    return {&#10;        'cpu': 'Intel Xeon',&#10;        'ram_gb': 32,&#10;        'cores': 16,&#10;        'node_id': 'testnode'&#10;    }&#10;&#10;&#10;def execute_job(job: dict) -&gt; dict:&#10;    &quot;&quot;&quot;Simulate job execution for testing.&quot;&quot;&quot;&#10;    print(f&quot;Executing job {job['job_id']} with image {job['docker_image']}&quot;)&#10;    time.sleep(2)&#10;    return {&#10;        'status': 'completed',&#10;        'output': f&quot;Output of job {job['job_id']}&quot;&#10;    }&#10;&#10;&#10;def log_result(result: dict):&#10;    &quot;&quot;&quot;Simulate logging of a job result.&quot;&quot;&quot;&#10;    log_file = f&quot;nexapod_{result['job_id']}_log.json&quot;&#10;    with open(log_file, 'w') as f:&#10;        f.write(str(result))&#10;    print(f&quot;Result logged to {log_file}&quot;)&#10;" />
              <option name="updatedContent" value="import time&#13;&#10;import requests&#13;&#10;&#13;&#10;&#13;&#10;class CoordinatorClient:&#13;&#10;    &quot;&quot;&quot;Client for interacting with the coordinator API.&quot;&quot;&quot;&#13;&#10;    def __init__(self, config: dict):&#13;&#10;        self.url = config['coordinator_url']&#13;&#10;        self.node_id = config.get('node_id')&#13;&#10;        self.poll_interval = config.get('poll_interval', 10)&#13;&#10;&#13;&#10;    def register_node(self, profile: dict):&#13;&#10;        &quot;&quot;&quot;Register this node with the coordinator.&quot;&quot;&quot;&#13;&#10;        resp = requests.post(f&quot;{self.url}/register&quot;, json=profile)&#13;&#10;        if resp.ok:&#13;&#10;            self.node_id = resp.json().get('node_id')&#13;&#10;        else:&#13;&#10;            raise Exception(f&quot;Registration failed: {resp.text}&quot;)&#13;&#10;&#13;&#10;    def poll_job(self) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Poll coordinator for a new job.&quot;&quot;&quot;&#13;&#10;        resp = requests.get(&#13;&#10;            f&quot;{self.url}/job&quot;,&#13;&#10;            params={'node_id': self.node_id}&#13;&#10;        )&#13;&#10;        if resp.ok and resp.json():&#13;&#10;            return resp.json()&#13;&#10;        time.sleep(self.poll_interval)&#13;&#10;        return None&#13;&#10;&#13;&#10;    def submit_result(self, result: dict) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Submit execution result back to coordinator.&quot;&quot;&quot;&#13;&#10;        resp = requests.post(f&quot;{self.url}/result&quot;, json=result)&#13;&#10;        if resp.ok:&#13;&#10;            return resp.json()&#13;&#10;        raise Exception(f&quot;Result submission failed: {resp.text}&quot;)&#13;&#10;&#13;&#10;    def submit_job(self, param: dict) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Submit a new job to the coordinator.&quot;&quot;&quot;&#13;&#10;        resp = requests.post(f&quot;{self.url}/jobs&quot;, json=param)&#13;&#10;        if not resp.ok:&#13;&#10;            raise Exception(f&quot;Job submission failed: {resp.text}&quot;)&#13;&#10;        return resp.json()&#13;&#10;&#13;&#10;    def get_status(self) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Check node status with coordinator.&quot;&quot;&quot;&#13;&#10;        resp = requests.get(&#13;&#10;            f&quot;{self.url}/status&quot;,&#13;&#10;            params={'node_id': self.node_id}&#13;&#10;        )&#13;&#10;        if resp.ok:&#13;&#10;            return resp.json()&#13;&#10;        raise Exception(f&quot;Status check failed: {resp.text}&quot;)&#13;&#10;&#13;&#10;    def get_nodes(self) -&gt; list:&#13;&#10;        &quot;&quot;&quot;Fetch list of all registered nodes.&quot;&quot;&quot;&#13;&#10;        resp = requests.get(f&quot;{self.url}/nodes&quot;)&#13;&#10;        if resp.ok:&#13;&#10;            return resp.json()&#13;&#10;        raise Exception(f&quot;Failed to fetch nodes: {resp.text}&quot;)&#13;&#10;&#13;&#10;    def get_jobs_list(self) -&gt; list:&#13;&#10;        &quot;&quot;&quot;Fetch list of all jobs in the system.&quot;&quot;&quot;&#13;&#10;        resp = requests.get(f&quot;{self.url}/jobs&quot;)&#13;&#10;        if resp.ok:&#13;&#10;            return resp.json()&#13;&#10;        raise Exception(f&quot;Failed to fetch jobs: {resp.text}&quot;)&#13;&#10;&#13;&#10;&#13;&#10;def get_node_profile() -&gt; dict:&#13;&#10;    &quot;&quot;&quot;Simulate a static node profile for testing.&quot;&quot;&quot;&#13;&#10;    return {&#13;&#10;        'cpu': 'Intel Xeon',&#13;&#10;        'ram_gb': 32,&#13;&#10;        'cores': 16,&#13;&#10;        'node_id': 'testnode'&#13;&#10;    }&#13;&#10;&#13;&#10;&#13;&#10;def execute_job(job: dict) -&gt; dict:&#13;&#10;    &quot;&quot;&quot;Simulate job execution for testing.&quot;&quot;&quot;&#13;&#10;    print(f&quot;Executing job {job['job_id']} with image {job['docker_image']}&quot;)&#13;&#10;    time.sleep(2)&#13;&#10;    return {&#13;&#10;        'status': 'completed',&#13;&#10;        'output': f&quot;Output of job {job['job_id']}&quot;&#13;&#10;    }&#13;&#10;&#13;&#10;&#13;&#10;def log_result(result: dict):&#13;&#10;    &quot;&quot;&quot;Simulate logging of a job result.&quot;&quot;&quot;&#13;&#10;    log_file = f&quot;nexapod_{result['job_id']}_log.json&quot;&#13;&#10;    with open(log_file, 'w') as f:&#13;&#10;        f.write(str(result))&#13;&#10;    print(f&quot;Result logged to {log_file}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Client/dashboard.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Client/dashboard.py" />
              <option name="originalContent" value="import json&#10;import yaml&#10;from datetime import datetime&#10;from typing import List, Dict, Tuple&#10;&#10;import networkx as nx&#10;import numpy as np&#10;import plotly.graph_objects as go&#10;import streamlit as st&#10;&#10;from comms import CoordinatorClient&#10;&#10;st.set_page_config(&#10;    page_title=&quot;NEXAPod Dashboard&quot;,&#10;    page_icon=&quot;&quot;,&#10;    layout=&quot;wide&quot;,&#10;    initial_sidebar_state=&quot;expanded&quot;&#10;)&#10;&#10;st.markdown(&quot;&quot;&quot;&#10;&lt;style&gt;&#10;    .main-header {&#10;        font-size: 2.5rem;&#10;        font-weight: bold;&#10;        color: #2c3e50;&#10;        text-align: center;&#10;        margin-bottom: 1rem;&#10;    }&#10;    .subtitle {&#10;        text-align: center;&#10;        color: #7f8c8d;&#10;        font-size: 1.1rem;&#10;        margin-bottom: 2rem;&#10;    }&#10;    .metric-container {&#10;        background: #f8f9fa;&#10;        border: 1px solid #e9ecef;&#10;        border-radius: 8px;&#10;        padding: 1rem;&#10;        margin: 0.5rem 0;&#10;    }&#10;    .status-active { color: #27ae60; font-weight: bold; }&#10;    .status-busy { color: #f39c12; font-weight: bold; }&#10;    .status-offline { color: #e74c3c; font-weight: bold; }&#10;    .status-maintenance { color: #8e44ad; font-weight: bold; }&#10;    .sidebar-section {&#10;        background-color: #f8f9fa;&#10;        padding: 1rem;&#10;        border-radius: 6px;&#10;        margin: 1rem 0;&#10;        border-left: 3px solid #3498db;&#10;    }&#10;    .data-table {&#10;        font-size: 0.9rem;&#10;    }&#10;&lt;/style&gt;&#10;&quot;&quot;&quot;, unsafe_allow_html=True)&#10;&#10;&#10;class SpinningGlobeNetwork:&#10;    &quot;&quot;&quot;&#10;    3D Spinning Globe Network Visualization&#10;&#10;    Creates an interactive 3D visualization of a complete graph&#10;    network with nodes distributed on a sphere using a Fibonacci&#10;    spiral. Nodes use larger markers and show single-line JSON hover&#10;    information. Live animation rotates the globe and pulses nodes.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, nodes_data: List[Dict]):&#10;        self.nodes_data = nodes_data&#10;        self.num_nodes = len(nodes_data)&#10;&#10;    def fibonacci_sphere_distribution(&#10;            self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:&#10;        &quot;&quot;&quot;Generate optimal node positions using Fibonacci spiral.&quot;&quot;&quot;&#10;        n = self.num_nodes&#10;        indices = np.arange(n) + 0.5&#10;        phi = np.arccos(1 - 2 * (indices / n))&#10;        theta = np.pi * (1 + np.sqrt(5)) * indices&#10;        x = np.sin(phi) * np.cos(theta)&#10;        y = np.sin(phi) * np.sin(theta)&#10;        z = np.cos(phi)&#10;        return x, y, z&#10;&#10;    @staticmethod&#10;    def create_sphere_surface(resolution: int = 50) -&gt; go.Surface:&#10;        &quot;&quot;&quot;Create the background sphere surface.&quot;&quot;&quot;&#10;        u = np.linspace(0, np.pi, resolution)&#10;        v = np.linspace(0, 2 * np.pi, resolution)&#10;        U, V = np.meshgrid(u, v)&#10;        X_sphere = np.sin(U) * np.cos(V)&#10;        Y_sphere = np.sin(U) * np.sin(V)&#10;        Z_sphere = np.cos(U)&#10;        return go.Surface(&#10;            x=X_sphere, y=Y_sphere, z=Z_sphere,&#10;            colorscale='Viridis',&#10;            opacity=0.5,&#10;            showscale=False,&#10;            hoverinfo='skip'&#10;        )&#10;&#10;    def create_network_edges(&#10;            self, node_positions: Tuple[np.ndarray, ...]) -&gt; List[go.Scatter3d]:&#10;        &quot;&quot;&quot;Create edge traces for complete graph connectivity.&quot;&quot;&quot;&#10;        x_coords, y_coords, z_coords = node_positions&#10;        G = nx.complete_graph(self.num_nodes)&#10;        edge_traces = []&#10;        for edge in G.edges():&#10;            i, j = edge&#10;            edge_trace = go.Scatter3d(&#10;                x=[x_coords[i], x_coords[j]],&#10;                y=[y_coords[i], y_coords[j]],&#10;                z=[z_coords[i], z_coords[j]],&#10;                mode='lines',&#10;                line=dict(color='rgba(100, 149, 237, 0.6)', width=2),&#10;                hoverinfo='skip',&#10;                showlegend=False&#10;            )&#10;            edge_traces.append(edge_trace)&#10;        return edge_traces&#10;&#10;    def prepare_hover_information(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;&#10;        Prepare JSON-formatted hover text for each node in a single-line format.&#10;        &quot;&quot;&quot;&#10;        hover_texts = []&#10;        for node in self.nodes_data:&#10;            hover_data = {&#10;                'id': node['id'],&#10;                'tier': node['tier'],&#10;                'status': node['status'],&#10;                'region': node['region'],&#10;                'cpu_usage': f&quot;{node['metrics']['cpu_usage']}%&quot;,&#10;                'memory_usage': f&quot;{node['metrics']['memory_usage']}%&quot;,&#10;                'jobs_completed': node['metrics']['jobs_completed'],&#10;                'reputation_score': node['metrics']['reputation_score'],&#10;                'credits_earned':&#10;                    f&quot;${node['metrics']['credits_earned']:.2f}&quot;,&#10;                'uptime_hours': node['profile']['uptime_hours']&#10;            }&#10;            hover_text = json.dumps(hover_data)&#10;            hover_texts.append(hover_text)&#10;        return hover_texts&#10;&#10;    def create_node_trace(self, node_positions: Tuple[np.ndarray, ...]) -&gt; go.Scatter3d:&#10;        &quot;&quot;&quot;Create the main node scatter trace with larger markers.&quot;&quot;&quot;&#10;        x_coords, y_coords, z_coords = node_positions&#10;        hover_texts = self.prepare_hover_information()&#10;        return go.Scatter3d(&#10;            x=x_coords, y=y_coords, z=z_coords,&#10;            mode='markers+text',&#10;            marker=dict(&#10;                symbol='circle',&#10;                size=[12] * self.num_nodes,&#10;                color='red',&#10;                line=dict(width=2, color='white'),&#10;                opacity=0.9&#10;            ),&#10;            text=[f&quot;Node {i}&quot; for i in range(self.num_nodes)],&#10;            textposition='middle center',&#10;            textfont=dict(size=10, color='white'),&#10;            hovertext=hover_texts,&#10;            hoverinfo='text',&#10;            showlegend=False&#10;        )&#10;&#10;    def generate_animation_frames(self, num_frames: int = 60) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;&#10;        Generate animation frames for camera rotation and node pulsing.&#10;        &quot;&quot;&quot;&#10;        frames = []&#10;        phases = [2 * np.pi * j / self.num_nodes for j in range(self.num_nodes)]&#10;        for i in range(num_frames):&#10;            angle = 2 * np.pi * i / num_frames&#10;            cam_eye = dict(x=2 * np.cos(angle), y=2 * np.sin(angle), z=0.5)&#10;            node_sizes = [&#10;                12 + 3 * ((np.sin(&#10;                    2 * np.pi * i / num_frames + phases[j]) + 1) / 2)&#10;                for j in range(self.num_nodes)&#10;            ]&#10;            frame = dict(&#10;                data=[dict(marker=dict(size=node_sizes))],&#10;                traces=[1],&#10;                layout=dict(scene=dict(camera=dict(eye=cam_eye)))&#10;            )&#10;            frames.append(frame)&#10;        return frames&#10;&#10;    def create_visualization(self) -&gt; go.Figure:&#10;        &quot;&quot;&quot;Create the complete 3D spinning globe network visualization.&quot;&quot;&quot;&#10;        node_positions = self.fibonacci_sphere_distribution()&#10;        sphere_trace = self.create_sphere_surface()&#10;        node_trace = self.create_node_trace(node_positions)&#10;        edge_traces = self.create_network_edges(node_positions)&#10;        data = [sphere_trace, node_trace] + edge_traces&#10;&#10;        layout = dict(&#10;            scene=dict(&#10;                xaxis=dict(visible=False),&#10;                yaxis=dict(visible=False),&#10;                zaxis=dict(visible=False),&#10;                aspectmode='cube',&#10;                bgcolor='rgba(0,0,0,0.05)'&#10;            ),&#10;            title=dict(&#10;                text=(f&quot;NEXAPod Network Globe - {self.num_nodes} &quot;&#10;                      &quot;Compute Nodes&quot;),&#10;                font=dict(size=16, color='#2c3e50'),&#10;                x=0.5&#10;            ),&#10;            width=800,&#10;            height=600,&#10;            margin=dict(t=60, b=10, l=10, r=10),&#10;            updatemenus=[{&#10;                &quot;buttons&quot;: [{&#10;                    &quot;args&quot;: [&#10;                        None,&#10;                        {&#10;                            &quot;frame&quot;: {&quot;duration&quot;: 50, &quot;redraw&quot;: True},&#10;                            &quot;fromcurrent&quot;: True,&#10;                            &quot;transition&quot;: {&quot;duration&quot;: 0}&#10;                        }&#10;                    ],&#10;                    &quot;label&quot;: &quot;Play&quot;,&#10;                    &quot;method&quot;: &quot;animate&quot;&#10;                }, {&#10;                    &quot;args&quot;: [&#10;                        [None],&#10;                        {&#10;                            &quot;frame&quot;: {&quot;duration&quot;: 0, &quot;redraw&quot;: True},&#10;                            &quot;mode&quot;: &quot;immediate&quot;,&#10;                            &quot;transition&quot;: {&quot;duration&quot;: 0}&#10;                        }&#10;                    ],&#10;                    &quot;label&quot;: &quot;Pause&quot;,&#10;                    &quot;method&quot;: &quot;animate&quot;&#10;                }],&#10;                &quot;direction&quot;: &quot;left&quot;,&#10;                &quot;pad&quot;: {&quot;r&quot;: 10, &quot;t&quot;: 70},&#10;                &quot;showactive&quot;: False,&#10;                &quot;type&quot;: &quot;buttons&quot;,&#10;                &quot;x&quot;: 0.1,&#10;                &quot;xanchor&quot;: &quot;right&quot;,&#10;                &quot;y&quot;: 0.02,&#10;                &quot;yanchor&quot;: &quot;top&quot;&#10;            }],&#10;            annotations=[{&#10;                'text': (&#10;                    &quot;Fibonacci sphere distribution • Complete graph topology • &quot;&#10;                    &quot;JSON hover data&quot;&#10;                ),&#10;                'showarrow': False,&#10;                'xref': 'paper',&#10;                'yref': 'paper',&#10;                'x': 0.5,&#10;                'y': 0.02,&#10;                'xanchor': 'center',&#10;                'yanchor': 'bottom',&#10;                'font': {'size': 11, 'color': 'gray'}&#10;            }]&#10;        )&#10;&#10;        fig = go.Figure(data=data, layout=layout)&#10;        frames = self.generate_animation_frames()&#10;        fig.frames = frames&#10;        return fig&#10;&#10;&#10;class NEXAPodDashboard:&#10;    &quot;&quot;&quot;&#10;    Clean, streamlined NEXAPod Dashboard that displays key metrics,&#10;    data tables, and the 3D live network globe.&#10;    &quot;&quot;&quot;&#10;    def __init__(self):&#10;        self.initialize_session_state()&#10;&#10;    def initialize_session_state(self):&#10;        &quot;&quot;&quot;Fetch live nodes and jobs from coordinator.&quot;&quot;&quot;&#10;        if 'coord' not in st.session_state:&#10;            cfg = yaml.safe_load(open(&quot;config.yaml&quot;))&#10;            st.session_state.coord = CoordinatorClient({&#10;                &quot;coordinator_url&quot;: cfg[&quot;coordinator_url&quot;]&#10;            })&#10;        if 'nodes' not in st.session_state:&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;        if 'jobs' not in st.session_state:&#10;            st.session_state.jobs = st.session_state.coord.get_jobs_list()&#10;        if 'last_update' not in st.session_state:&#10;            st.session_state.last_update = datetime.now()&#10;&#10;    @staticmethod&#10;    def generate_demo_nodes(count: int) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;Generate realistic demo node data.&quot;&quot;&quot;&#10;        tiers = ['CPU', 'CONSUMER_GPU', 'HPC']&#10;        statuses = ['Active', 'Busy', 'Maintenance', 'Offline']&#10;        regions = ['US-East', 'US-West', 'EU-Central', 'APAC', 'Americas']&#10;        nodes = []&#10;&#10;        for i in range(count):&#10;            tier = np.random.choice(tiers, p=[0.6, 0.3, 0.1])&#10;            status = np.random.choice(statuses, p=[0.7, 0.2, 0.07, 0.03])&#10;            node = {&#10;                'id': f&quot;node_{i:03d}&quot;,&#10;                'tier': tier,&#10;                'status': status,&#10;                'region': np.random.choice(regions),&#10;                'profile': {&#10;                    'cpu': f&quot;Intel Xeon E5-{2600 + i*10}&quot;,&#10;                    'cores': int(np.random.randint(8, 32)),&#10;                    'ram_gb': int(np.random.choice([16, 32, 64, 128])),&#10;                    'uptime_hours': int(np.random.randint(100, 8000))&#10;                },&#10;                'metrics': {&#10;                    'jobs_completed': int(np.random.randint(50, 800)),&#10;                    'cpu_usage': int(np.random.randint(20, 85)),&#10;                    'memory_usage': int(np.random.randint(25, 75)),&#10;                    'reputation_score': round(np.random.uniform(0.8, 1.0), 3),&#10;                    'credits_earned': round(np.random.uniform(500, 8000), 2)&#10;                }&#10;            }&#10;            nodes.append(node)&#10;        return nodes&#10;&#10;    @staticmethod&#10;    def generate_demo_jobs(count: int) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;Generate demo job data.&quot;&quot;&quot;&#10;        job_types = ['protein_folding', 'weather_simulation',&#10;                     'quantum_computation', 'materials_modeling',&#10;                     'ml_training', 'molecular_dynamics']&#10;        statuses = ['Queued', 'Running', 'Completed', 'Failed']&#10;        jobs = []&#10;&#10;        for i in range(count):&#10;            job_type = np.random.choice(job_types)&#10;            status = np.random.choice(statuses, p=[0.2, 0.3, 0.45, 0.05])&#10;            job = {&#10;                'id': f&quot;job_{i:04d}&quot;,&#10;                'type': job_type,&#10;                'status': status,&#10;                'submitter': f&quot;researcher_{chr(97 + i % 5)}&quot;,&#10;                'progress': (&#10;                    int(np.random.randint(0, 100))&#10;                    if status == 'Running'&#10;                    else (&#10;                        100&#10;                        if status == 'Completed'&#10;                        else 0&#10;                    )&#10;                ),&#10;                'credits_allocated': round(&#10;                    np.random.uniform(50, 500), 2&#10;                ),&#10;                'estimated_time': (&#10;                    f&quot;{np.random.randint(30, 240)} min&quot;&#10;                )&#10;            }&#10;            jobs.append(job)&#10;        return jobs&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Main NEXAPod Dashboard application.&quot;&quot;&quot;&#10;    dashboard = NEXAPodDashboard()&#10;&#10;    st.markdown('&lt;h1 class=&quot;main-header&quot;&gt;NEXAPod Live Dashboard&lt;/h1&gt;',&#10;                unsafe_allow_html=True)&#10;    st.markdown('&lt;p class=&quot;subtitle&quot;&gt;Distributed Compute Fabric for '&#10;                'Scientific Problems&lt;/p&gt;', unsafe_allow_html=True)&#10;&#10;    # Sidebar controls&#10;    with st.sidebar:&#10;        st.header(&quot;Dashboard Controls&quot;)&#10;        st.subheader(&quot;Network Configuration&quot;)&#10;        node_count = st.slider(&quot;Number of Nodes&quot;, min_value=5, max_value=20,&#10;                              value=10)&#10;&#10;        if st.button(&quot;Regenerate Network&quot;):&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;            st.rerun()&#10;&#10;        st.subheader(&quot;Filters&quot;)&#10;        tier_filter = st.multiselect(&#10;            &quot;Node Tiers&quot;,&#10;            options=['CPU', 'CONSUMER_GPU', 'HPC'],&#10;            default=['CPU', 'CONSUMER_GPU', 'HPC']&#10;        )&#10;        status_filter = st.multiselect(&#10;            &quot;Node Status&quot;,&#10;            options=['Active', 'Busy', 'Maintenance', 'Offline'],&#10;            default=['Active', 'Busy']&#10;        )&#10;&#10;        st.subheader(&quot;System Status&quot;)&#10;        st.write(&quot;**Coordinator:** Online&quot;)&#10;        st.write(&quot;**Database:** Connected&quot;)&#10;        last_update_time = st.session_state.last_update.strftime('%H:%M:%S')&#10;        st.write(f&quot;**Last Update:** {last_update_time}&quot;)&#10;&#10;        if st.button(&quot;Refresh Dashboard&quot;):&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;            st.session_state.jobs = st.session_state.coord.get_jobs_list()&#10;            st.session_state.last_update = datetime.now()&#10;            st.rerun()&#10;&#10;    # Filter nodes&#10;    filtered_nodes = [&#10;        node for node in st.session_state.nodes&#10;        if node['tier'] in tier_filter and node['status'] in status_filter&#10;    ]&#10;&#10;    # Metrics row&#10;    col1, col2, col3, col4 = st.columns(4)&#10;    with col1:&#10;        active_nodes = len([n for n in filtered_nodes if n['status'] == 'Active'])&#10;        delta_value = f&quot;+{np.random.randint(0, 2)}&quot;&#10;        st.metric(&quot;Active Nodes&quot;, active_nodes, delta=delta_value)&#10;&#10;    with col2:&#10;        running_jobs = len([j for j in st.session_state.jobs&#10;                           if j['status'] == 'Running'])&#10;        delta_value = f&quot;+{np.random.randint(-1, 3)}&quot;&#10;        st.metric(&quot;Running Jobs&quot;, running_jobs, delta=delta_value)&#10;&#10;    with col3:&#10;        total_credits = sum(node['metrics']['credits_earned']&#10;                           for node in filtered_nodes)&#10;        delta_value = f&quot;+${np.random.randint(100,500)}&quot;&#10;        st.metric(&quot;Total Credits&quot;, f&quot;${total_credits:,.0f}&quot;, delta=delta_value)&#10;&#10;    with col4:&#10;        utilization = np.random.randint(70, 95)&#10;        delta_value = f&quot;{np.random.randint(-3, 5)}%&quot;&#10;        st.metric(&quot;Network Utilization&quot;, f&quot;{utilization}%&quot;, delta=delta_value)&#10;&#10;    # Network Globe&#10;    st.header(&quot;Real-Time Network Globe&quot;)&#10;    st.markdown(&quot;&lt;p style='text-align:center; color:#e74c3c;'&gt;Disclaimer: &quot;&#10;                &quot;This is demo data only&lt;/p&gt;&quot;, unsafe_allow_html=True)&#10;&#10;    if filtered_nodes:&#10;        if len(filtered_nodes) != len(st.session_state.nodes):&#10;            display_nodes = filtered_nodes&#10;        else:&#10;            display_nodes = st.session_state.nodes[:node_count]&#10;        globe_network = SpinningGlobeNetwork(display_nodes)&#10;        fig = globe_network.create_visualization()&#10;&#10;        config = {&#10;            'displayModeBar': True,&#10;            'displaylogo': False,&#10;            'modeBarButtonsToRemove': [&#10;                'pan3d', 'orbitRotation', 'tableRotation',&#10;                'resetCameraDefault3d', 'resetCameraLastSave3d'&#10;            ]&#10;        }&#10;        st.plotly_chart(fig, use_container_width=True, config=config)&#10;    else:&#10;        st.warning(&quot;No nodes match current filter criteria.&quot;)&#10;&#10;    # Data tables&#10;    col1, col2 = st.columns(2)&#10;&#10;    with col1:&#10;        st.subheader(&quot;Node Registry&quot;)&#10;        if filtered_nodes:&#10;            node_table_data = []&#10;            for node in filtered_nodes[:8]:&#10;                node_table_data.append({&#10;                    'ID': node['id'],&#10;                    'Tier': node['tier'],&#10;                    'Status': node['status'],&#10;                    'Region': node['region'],&#10;                    'Jobs': node['metrics']['jobs_completed'],&#10;                    'Credits': f&quot;${node['metrics']['credits_earned']:.0f}&quot;,&#10;                    'Reputation': f&quot;{node['metrics']['reputation_score']:.3f}&quot;&#10;                })&#10;            st.dataframe(node_table_data, use_container_width=True)&#10;&#10;            st.write(&quot;**Node Distribution:**&quot;)&#10;            tier_counts = {}&#10;            for node in filtered_nodes:&#10;                tier = node['tier']&#10;                tier_counts[tier] = tier_counts.get(tier, 0) + 1&#10;            for tier, count in tier_counts.items():&#10;                st.write(f&quot;• {tier}: {count} nodes&quot;)&#10;&#10;    with col2:&#10;        st.subheader(&quot;Job Queue Status&quot;)&#10;        if st.session_state.jobs:&#10;            job_table_data = []&#10;            for job in st.session_state.jobs:&#10;                progress_text = (&#10;                    f&quot;{job['progress']}%&quot;&#10;                    if job['status'] == 'Running'&#10;                    else '—'&#10;                )&#10;                job_table_data.append({&#10;                    'ID': job['id'],&#10;                    'Type': job['type'].replace('_', ' ').title(),&#10;                    'Status': job['status'],&#10;                    'Progress': progress_text,&#10;                    'Credits': f&quot;${job['credits_allocated']:.0f}&quot;,&#10;                    'Time Est.': job.get('estimated_time', '—'),&#10;                    'Submitter': job['submitter']&#10;                })&#10;            st.dataframe(job_table_data, use_container_width=True)&#10;&#10;            st.write(&quot;**Job Distribution:**&quot;)&#10;            status_counts = {}&#10;            for job in st.session_state.jobs:&#10;                status = job['status']&#10;                status_counts[status] = status_counts.get(status, 0) + 1&#10;            for status, count in status_counts.items():&#10;                st.write(f&quot;• {status}: {count} jobs&quot;)&#10;&#10;    # Performance metrics&#10;    st.header(&quot;System Performance&quot;)&#10;    col1, col2, col3 = st.columns(3)&#10;&#10;    with col1:&#10;        st.subheader(&quot;Compute Metrics&quot;)&#10;        tflops_value = np.random.uniform(500, 2000)&#10;        st.write(f&quot;**Total FLOPS:** {tflops_value:.1f} TFLOPS&quot;)&#10;        avg_time = np.random.randint(45, 180)&#10;        st.write(f&quot;**Average Job Time:** {avg_time} minutes&quot;)&#10;        success_rate = np.random.uniform(96, 99.5)&#10;        st.write(f&quot;**Success Rate:** {success_rate:.1f}%&quot;)&#10;        queue_wait = np.random.randint(2, 12)&#10;        st.write(f&quot;**Queue Wait:** {queue_wait} minutes&quot;)&#10;&#10;    with col2:&#10;        st.subheader(&quot;Resource Usage&quot;)&#10;        cpu_util = np.random.randint(60, 85)&#10;        st.write(f&quot;**CPU Utilization:** {cpu_util}%&quot;)&#10;        mem_usage = np.random.randint(55, 80)&#10;        st.write(f&quot;**Memory Usage:** {mem_usage}%&quot;)&#10;        network_io = np.random.uniform(15, 45)&#10;        st.write(f&quot;**Network I/O:** {network_io:.1f} Gbps&quot;)&#10;        storage_io = np.random.uniform(5, 25)&#10;        st.write(f&quot;**Storage I/O:** {storage_io:.1f} GB/s&quot;)&#10;&#10;    with col3:&#10;        st.subheader(&quot;Economic Data&quot;)&#10;        credits_per_hour = np.random.uniform(75, 250)&#10;        st.write(f&quot;**Credits/Hour:** ${credits_per_hour:.0f}&quot;)&#10;        daily_volume = np.random.uniform(5000, 15000)&#10;        st.write(f&quot;**Daily Volume:** ${daily_volume:.0f}&quot;)&#10;        avg_rate = np.random.uniform(15, 35)&#10;        st.write(f&quot;**Avg Rate:** ${avg_rate:.2f}/job&quot;)&#10;        if filtered_nodes:&#10;            credits = [n['metrics']['credits_earned'] for n in filtered_nodes]&#10;            st.write(f&quot;**Top Earner:** ${max(credits):.0f}&quot;)&#10;&#10;    # Footer&#10;    st.markdown(&quot;---&quot;)&#10;    col1, col2, col3 = st.columns(3)&#10;&#10;    with col1:&#10;        st.write(&quot;**System Status:** All systems operational&quot;)&#10;    with col2:&#10;        st.write(&quot;**API Endpoint:** http://localhost:5000&quot;)&#10;    with col3:&#10;        if st.button(&quot;Export Report&quot;):&#10;            st.success(&quot;System report exported&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="import json&#10;import yaml&#10;from datetime import datetime&#10;from typing import List, Dict, Tuple&#10;&#10;import networkx as nx&#10;import numpy as np&#10;import plotly.graph_objects as go&#10;import streamlit as st&#10;&#10;from comms import CoordinatorClient&#10;&#10;st.set_page_config(&#10;    page_title=&quot;NEXAPod Dashboard&quot;,&#10;    page_icon=&quot;&quot;,&#10;    layout=&quot;wide&quot;,&#10;    initial_sidebar_state=&quot;expanded&quot;&#10;)&#10;&#10;st.markdown(&quot;&quot;&quot;&#10;&lt;style&gt;&#10;    .main-header {&#10;        font-size: 2.5rem;&#10;        font-weight: bold;&#10;        color: #2c3e50;&#10;        text-align: center;&#10;        margin-bottom: 1rem;&#10;    }&#10;    .subtitle {&#10;        text-align: center;&#10;        color: #7f8c8d;&#10;        font-size: 1.1rem;&#10;        margin-bottom: 2rem;&#10;    }&#10;    .metric-container {&#10;        background: #f8f9fa;&#10;        border: 1px solid #e9ecef;&#10;        border-radius: 8px;&#10;        padding: 1rem;&#10;        margin: 0.5rem 0;&#10;    }&#10;    .status-active { color: #27ae60; font-weight: bold; }&#10;    .status-busy { color: #f39c12; font-weight: bold; }&#10;    .status-offline { color: #e74c3c; font-weight: bold; }&#10;    .status-maintenance { color: #8e44ad; font-weight: bold; }&#10;    .sidebar-section {&#10;        background-color: #f8f9fa;&#10;        padding: 1rem;&#10;        border-radius: 6px;&#10;        margin: 1rem 0;&#10;        border-left: 3px solid #3498db;&#10;    }&#10;    .data-table {&#10;        font-size: 0.9rem;&#10;    }&#10;&lt;/style&gt;&#10;&quot;&quot;&quot;, unsafe_allow_html=True)&#10;&#10;&#10;class SpinningGlobeNetwork:&#10;    &quot;&quot;&quot;&#10;    3D Spinning Globe Network Visualization&#10;&#10;    Creates an interactive 3D visualization of a complete graph&#10;    network with nodes distributed on a sphere using a Fibonacci&#10;    spiral. Nodes use larger markers and show single-line JSON hover&#10;    information. Live animation rotates the globe and pulses nodes.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, nodes_data: List[Dict]):&#10;        self.nodes_data = nodes_data&#10;        self.num_nodes = len(nodes_data)&#10;&#10;    def fibonacci_sphere_distribution(&#10;            self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:&#10;        &quot;&quot;&quot;Generate optimal node positions using Fibonacci spiral.&quot;&quot;&quot;&#10;        n = self.num_nodes&#10;        indices = np.arange(n) + 0.5&#10;        phi = np.arccos(1 - 2 * (indices / n))&#10;        theta = np.pi * (1 + np.sqrt(5)) * indices&#10;        x = np.sin(phi) * np.cos(theta)&#10;        y = np.sin(phi) * np.sin(theta)&#10;        z = np.cos(phi)&#10;        return x, y, z&#10;&#10;    @staticmethod&#10;    def create_sphere_surface(resolution: int = 50) -&gt; go.Surface:&#10;        &quot;&quot;&quot;Create the background sphere surface.&quot;&quot;&quot;&#10;        u = np.linspace(0, np.pi, resolution)&#10;        v = np.linspace(0, 2 * np.pi, resolution)&#10;        U, V = np.meshgrid(u, v)&#10;        X_sphere = np.sin(U) * np.cos(V)&#10;        Y_sphere = np.sin(U) * np.sin(V)&#10;        Z_sphere = np.cos(U)&#10;        return go.Surface(&#10;            x=X_sphere, y=Y_sphere, z=Z_sphere,&#10;            colorscale='Viridis',&#10;            opacity=0.5,&#10;            showscale=False,&#10;            hoverinfo='skip'&#10;        )&#10;&#10;    def create_network_edges(&#10;            self, node_positions: Tuple[np.ndarray, ...]) -&gt; List[go.Scatter3d]:&#10;        &quot;&quot;&quot;Create edge traces for complete graph connectivity.&quot;&quot;&quot;&#10;        x_coords, y_coords, z_coords = node_positions&#10;        G = nx.complete_graph(self.num_nodes)&#10;        edge_traces = []&#10;        for edge in G.edges():&#10;            i, j = edge&#10;            edge_trace = go.Scatter3d(&#10;                x=[x_coords[i], x_coords[j]],&#10;                y=[y_coords[i], y_coords[j]],&#10;                z=[z_coords[i], z_coords[j]],&#10;                mode='lines',&#10;                line=dict(color='rgba(100, 149, 237, 0.6)', width=2),&#10;                hoverinfo='skip',&#10;                showlegend=False&#10;            )&#10;            edge_traces.append(edge_trace)&#10;        return edge_traces&#10;&#10;    def prepare_hover_information(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;&#10;        Prepare JSON-formatted hover text for each node in a single-line format.&#10;        &quot;&quot;&quot;&#10;        hover_texts = []&#10;        for node in self.nodes_data:&#10;            hover_data = {&#10;                'id': node['id'],&#10;                'tier': node['tier'],&#10;                'status': node['status'],&#10;                'region': node['region'],&#10;                'cpu_usage': f&quot;{node['metrics']['cpu_usage']}%&quot;,&#10;                'memory_usage': f&quot;{node['metrics']['memory_usage']}%&quot;,&#10;                'jobs_completed': node['metrics']['jobs_completed'],&#10;                'reputation_score': node['metrics']['reputation_score'],&#10;                'credits_earned':&#10;                    f&quot;${node['metrics']['credits_earned']:.2f}&quot;,&#10;                'uptime_hours': node['profile']['uptime_hours']&#10;            }&#10;            hover_text = json.dumps(hover_data)&#10;            hover_texts.append(hover_text)&#10;        return hover_texts&#10;&#10;    def create_node_trace(self, node_positions: Tuple[np.ndarray, ...]) -&gt; go.Scatter3d:&#10;        &quot;&quot;&quot;Create the main node scatter trace with larger markers.&quot;&quot;&quot;&#10;        x_coords, y_coords, z_coords = node_positions&#10;        hover_texts = self.prepare_hover_information()&#10;        return go.Scatter3d(&#10;            x=x_coords, y=y_coords, z=z_coords,&#10;            mode='markers+text',&#10;            marker=dict(&#10;                symbol='circle',&#10;                size=[12] * self.num_nodes,&#10;                color='red',&#10;                line=dict(width=2, color='white'),&#10;                opacity=0.9&#10;            ),&#10;            text=[f&quot;Node {i}&quot; for i in range(self.num_nodes)],&#10;            textposition='middle center',&#10;            textfont=dict(size=10, color='white'),&#10;            hovertext=hover_texts,&#10;            hoverinfo='text',&#10;            showlegend=False&#10;        )&#10;&#10;    def generate_animation_frames(self, num_frames: int = 60) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;&#10;        Generate animation frames for camera rotation and node pulsing.&#10;        &quot;&quot;&quot;&#10;        frames = []&#10;        phases = [2 * np.pi * j / self.num_nodes for j in range(self.num_nodes)]&#10;        for i in range(num_frames):&#10;            angle = 2 * np.pi * i / num_frames&#10;            cam_eye = dict(x=2 * np.cos(angle), y=2 * np.sin(angle), z=0.5)&#10;            node_sizes = [&#10;                12 + 3 * ((np.sin(&#10;                    2 * np.pi * i / num_frames + phases[j]) + 1) / 2)&#10;                for j in range(self.num_nodes)&#10;            ]&#10;            frame = dict(&#10;                data=[dict(marker=dict(size=node_sizes))],&#10;                traces=[1],&#10;                layout=dict(scene=dict(camera=dict(eye=cam_eye)))&#10;            )&#10;            frames.append(frame)&#10;        return frames&#10;&#10;    def create_visualization(self) -&gt; go.Figure:&#10;        &quot;&quot;&quot;Create the complete 3D spinning globe network visualization.&quot;&quot;&quot;&#10;        node_positions = self.fibonacci_sphere_distribution()&#10;        sphere_trace = self.create_sphere_surface()&#10;        node_trace = self.create_node_trace(node_positions)&#10;        edge_traces = self.create_network_edges(node_positions)&#10;        data = [sphere_trace, node_trace] + edge_traces&#10;&#10;        layout = dict(&#10;            scene=dict(&#10;                xaxis=dict(visible=False),&#10;                yaxis=dict(visible=False),&#10;                zaxis=dict(visible=False),&#10;                aspectmode='cube',&#10;                bgcolor='rgba(0,0,0,0.05)'&#10;            ),&#10;            title=dict(&#10;                text=(f&quot;NEXAPod Network Globe - {self.num_nodes} &quot;&#10;                      &quot;Compute Nodes&quot;),&#10;                font=dict(size=16, color='#2c3e50'),&#10;                x=0.5&#10;            ),&#10;            width=800,&#10;            height=600,&#10;            margin=dict(t=60, b=10, l=10, r=10),&#10;            updatemenus=[{&#10;                &quot;buttons&quot;: [{&#10;                    &quot;args&quot;: [&#10;                        None,&#10;                        {&#10;                            &quot;frame&quot;: {&quot;duration&quot;: 50, &quot;redraw&quot;: True},&#10;                            &quot;fromcurrent&quot;: True,&#10;                            &quot;transition&quot;: {&quot;duration&quot;: 0}&#10;                        }&#10;                    ],&#10;                    &quot;label&quot;: &quot;Play&quot;,&#10;                    &quot;method&quot;: &quot;animate&quot;&#10;                }, {&#10;                    &quot;args&quot;: [&#10;                        [None],&#10;                        {&#10;                            &quot;frame&quot;: {&quot;duration&quot;: 0, &quot;redraw&quot;: True},&#10;                            &quot;mode&quot;: &quot;immediate&quot;,&#10;                            &quot;transition&quot;: {&quot;duration&quot;: 0}&#10;                        }&#10;                    ],&#10;                    &quot;label&quot;: &quot;Pause&quot;,&#10;                    &quot;method&quot;: &quot;animate&quot;&#10;                }],&#10;                &quot;direction&quot;: &quot;left&quot;,&#10;                &quot;pad&quot;: {&quot;r&quot;: 10, &quot;t&quot;: 70},&#10;                &quot;showactive&quot;: False,&#10;                &quot;type&quot;: &quot;buttons&quot;,&#10;                &quot;x&quot;: 0.1,&#10;                &quot;xanchor&quot;: &quot;right&quot;,&#10;                &quot;y&quot;: 0.02,&#10;                &quot;yanchor&quot;: &quot;top&quot;&#10;            }],&#10;            annotations=[{&#10;                'text': (&#10;                    &quot;Fibonacci sphere distribution • Complete graph topology • &quot;&#10;                    &quot;JSON hover data&quot;&#10;                ),&#10;                'showarrow': False,&#10;                'xref': 'paper',&#10;                'yref': 'paper',&#10;                'x': 0.5,&#10;                'y': 0.02,&#10;                'xanchor': 'center',&#10;                'yanchor': 'bottom',&#10;                'font': {'size': 11, 'color': 'gray'}&#10;            }]&#10;        )&#10;&#10;        fig = go.Figure(data=data, layout=layout)&#10;        frames = self.generate_animation_frames()&#10;        fig.frames = frames&#10;        return fig&#10;&#10;&#10;class NEXAPodDashboard:&#10;    &quot;&quot;&quot;&#10;    Clean, streamlined NEXAPod Dashboard that displays key metrics,&#10;    data tables, and the 3D live network globe.&#10;    &quot;&quot;&quot;&#10;    def __init__(self):&#10;        self.initialize_session_state()&#10;&#10;    def initialize_session_state(self):&#10;        &quot;&quot;&quot;Fetch live nodes and jobs from coordinator.&quot;&quot;&quot;&#10;        if 'coord' not in st.session_state:&#10;            cfg = yaml.safe_load(open(&quot;config.yaml&quot;))&#10;            st.session_state.coord = CoordinatorClient({&#10;                &quot;coordinator_url&quot;: cfg[&quot;coordinator_url&quot;]&#10;            })&#10;        if 'nodes' not in st.session_state:&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;        if 'jobs' not in st.session_state:&#10;            st.session_state.jobs = st.session_state.coord.get_jobs_list()&#10;        if 'last_update' not in st.session_state:&#10;            st.session_state.last_update = datetime.now()&#10;&#10;def main():&#10;    &quot;&quot;&quot;Main NEXAPod Dashboard application.&quot;&quot;&quot;&#10;    dashboard = NEXAPodDashboard()&#10;&#10;    st.markdown('&lt;h1 class=&quot;main-header&quot;&gt;NEXAPod Live Dashboard&lt;/h1&gt;',&#10;                unsafe_allow_html=True)&#10;    st.markdown('&lt;p class=&quot;subtitle&quot;&gt;Distributed Compute Fabric for '&#10;                'Scientific Problems&lt;/p&gt;', unsafe_allow_html=True)&#10;&#10;    # Sidebar controls&#10;    with st.sidebar:&#10;        st.header(&quot;Dashboard Controls&quot;)&#10;        st.subheader(&quot;Network Configuration&quot;)&#10;        node_count = st.slider(&quot;Number of Nodes&quot;, min_value=5, max_value=20,&#10;                              value=10)&#10;&#10;        if st.button(&quot;Regenerate Network&quot;):&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;            st.rerun()&#10;&#10;        st.subheader(&quot;Filters&quot;)&#10;        tier_filter = st.multiselect(&#10;            &quot;Node Tiers&quot;,&#10;            options=['CPU', 'CONSUMER_GPU', 'HPC'],&#10;            default=['CPU', 'CONSUMER_GPU', 'HPC']&#10;        )&#10;        status_filter = st.multiselect(&#10;            &quot;Node Status&quot;,&#10;            options=['Active', 'Busy', 'Maintenance', 'Offline'],&#10;            default=['Active', 'Busy']&#10;        )&#10;&#10;        st.subheader(&quot;System Status&quot;)&#10;        st.write(&quot;**Coordinator:** Online&quot;)&#10;        st.write(&quot;**Database:** Connected&quot;)&#10;        last_update_time = st.session_state.last_update.strftime('%H:%M:%S')&#10;        st.write(f&quot;**Last Update:** {last_update_time}&quot;)&#10;&#10;        if st.button(&quot;Refresh Dashboard&quot;):&#10;            st.session_state.nodes = st.session_state.coord.get_nodes()&#10;            st.session_state.jobs = st.session_state.coord.get_jobs_list()&#10;            st.session_state.last_update = datetime.now()&#10;            st.rerun()&#10;&#10;    # Filter nodes&#10;    filtered_nodes = [&#10;        node for node in st.session_state.nodes&#10;        if node['tier'] in tier_filter and node['status'] in status_filter&#10;    ]&#10;&#10;    # Metrics row&#10;    col1, col2, col3, col4 = st.columns(4)&#10;    with col1:&#10;        active_nodes = len([n for n in filtered_nodes if n['status'] == 'Active'])&#10;        delta_value = f&quot;+{np.random.randint(0, 2)}&quot;&#10;        st.metric(&quot;Active Nodes&quot;, active_nodes, delta=delta_value)&#10;&#10;    with col2:&#10;        running_jobs = len([j for j in st.session_state.jobs&#10;                           if j['status'] == 'Running'])&#10;        delta_value = f&quot;+{np.random.randint(-1, 3)}&quot;&#10;        st.metric(&quot;Running Jobs&quot;, running_jobs, delta=delta_value)&#10;&#10;    with col3:&#10;        total_credits = sum(node['metrics']['credits_earned']&#10;                           for node in filtered_nodes)&#10;        delta_value = f&quot;+${np.random.randint(100,500)}&quot;&#10;        st.metric(&quot;Total Credits&quot;, f&quot;${total_credits:,.0f}&quot;, delta=delta_value)&#10;&#10;    with col4:&#10;        utilization = np.random.randint(70, 95)&#10;        delta_value = f&quot;{np.random.randint(-3, 5)}%&quot;&#10;        st.metric(&quot;Network Utilization&quot;, f&quot;{utilization}%&quot;, delta=delta_value)&#10;&#10;    # Network Globe&#10;    st.header(&quot;Real-Time Network Globe&quot;)&#10;&#10;    # always use real nodes fetched via comms&#10;    if filtered_nodes:&#10;        display_nodes = filtered_nodes[:node_count]&#10;        globe_network = SpinningGlobeNetwork(display_nodes)&#10;        fig = globe_network.create_visualization()&#10;&#10;        config = {&#10;            'displayModeBar': True,&#10;            'displaylogo': False,&#10;            'modeBarButtonsToRemove': [&#10;                'pan3d', 'orbitRotation', 'tableRotation',&#10;                'resetCameraDefault3d', 'resetCameraLastSave3d'&#10;            ]&#10;        }&#10;        st.plotly_chart(fig, use_container_width=True, config=config)&#10;    else:&#10;        st.warning(&quot;No nodes match current filter criteria.&quot;)&#10;&#10;    # Data tables&#10;    col1, col2 = st.columns(2)&#10;&#10;    with col1:&#10;        st.subheader(&quot;Node Registry&quot;)&#10;        if filtered_nodes:&#10;            node_table_data = []&#10;            for node in filtered_nodes[:8]:&#10;                node_table_data.append({&#10;                    'ID': node['id'],&#10;                    'Tier': node['tier'],&#10;                    'Status': node['status'],&#10;                    'Region': node['region'],&#10;                    'Jobs': node['metrics']['jobs_completed'],&#10;                    'Credits': f&quot;${node['metrics']['credits_earned']:.0f}&quot;,&#10;                    'Reputation': f&quot;{node['metrics']['reputation_score']:.3f}&quot;&#10;                })&#10;            st.dataframe(node_table_data, use_container_width=True)&#10;&#10;            st.write(&quot;**Node Distribution:**&quot;)&#10;            tier_counts = {}&#10;            for node in filtered_nodes:&#10;                tier = node['tier']&#10;                tier_counts[tier] = tier_counts.get(tier, 0) + 1&#10;            for tier, count in tier_counts.items():&#10;                st.write(f&quot;• {tier}: {count} nodes&quot;)&#10;&#10;    with col2:&#10;        st.subheader(&quot;Job Queue Status&quot;)&#10;        if st.session_state.jobs:&#10;            job_table_data = []&#10;            for job in st.session_state.jobs:&#10;                progress_text = (&#10;                    f&quot;{job['progress']}%&quot;&#10;                    if job['status'] == 'Running'&#10;                    else '—'&#10;                )&#10;                job_table_data.append({&#10;                    'ID': job['id'],&#10;                    'Type': job['type'].replace('_', ' ').title(),&#10;                    'Status': job['status'],&#10;                    'Progress': progress_text,&#10;                    'Credits': f&quot;${job['credits_allocated']:.0f}&quot;,&#10;                    'Time Est.': job.get('estimated_time', '—'),&#10;                    'Submitter': job['submitter']&#10;                })&#10;            st.dataframe(job_table_data, use_container_width=True)&#10;&#10;            st.write(&quot;**Job Distribution:**&quot;)&#10;            status_counts = {}&#10;            for job in st.session_state.jobs:&#10;                status = job['status']&#10;                status_counts[status] = status_counts.get(status, 0) + 1&#10;            for status, count in status_counts.items():&#10;                st.write(f&quot;• {status}: {count} jobs&quot;)&#10;&#10;    # Performance metrics&#10;    st.header(&quot;System Performance&quot;)&#10;    col1, col2, col3 = st.columns(3)&#10;&#10;    with col1:&#10;        st.subheader(&quot;Compute Metrics&quot;)&#10;        tflops_value = np.random.uniform(500, 2000)&#10;        st.write(f&quot;**Total FLOPS:** {tflops_value:.1f} TFLOPS&quot;)&#10;        avg_time = np.random.randint(45, 180)&#10;        st.write(f&quot;**Average Job Time:** {avg_time} minutes&quot;)&#10;        success_rate = np.random.uniform(96, 99.5)&#10;        st.write(f&quot;**Success Rate:** {success_rate:.1f}%&quot;)&#10;        queue_wait = np.random.randint(2, 12)&#10;        st.write(f&quot;**Queue Wait:** {queue_wait} minutes&quot;)&#10;&#10;    with col2:&#10;        st.subheader(&quot;Resource Usage&quot;)&#10;        cpu_util = np.random.randint(60, 85)&#10;        st.write(f&quot;**CPU Utilization:** {cpu_util}%&quot;)&#10;        mem_usage = np.random.randint(55, 80)&#10;        st.write(f&quot;**Memory Usage:** {mem_usage}%&quot;)&#10;        network_io = np.random.uniform(15, 45)&#10;        st.write(f&quot;**Network I/O:** {network_io:.1f} Gbps&quot;)&#10;        storage_io = np.random.uniform(5, 25)&#10;        st.write(f&quot;**Storage I/O:** {storage_io:.1f} GB/s&quot;)&#10;&#10;    with col3:&#10;        st.subheader(&quot;Economic Data&quot;)&#10;        credits_per_hour = np.random.uniform(75, 250)&#10;        st.write(f&quot;**Credits/Hour:** ${credits_per_hour:.0f}&quot;)&#10;        daily_volume = np.random.uniform(5000, 15000)&#10;        st.write(f&quot;**Daily Volume:** ${daily_volume:.0f}&quot;)&#10;        avg_rate = np.random.uniform(15, 35)&#10;        st.write(f&quot;**Avg Rate:** ${avg_rate:.2f}/job&quot;)&#10;        if filtered_nodes:&#10;            credits = [n['metrics']['credits_earned'] for n in filtered_nodes]&#10;            st.write(f&quot;**Top Earner:** ${max(credits):.0f}&quot;)&#10;&#10;    # Footer&#10;    st.markdown(&quot;---&quot;)&#10;    col1, col2, col3 = st.columns(3)&#10;&#10;    with col1:&#10;        st.write(&quot;**System Status:** All systems operational&quot;)&#10;    with col2:&#10;        st.write(&quot;**API Endpoint:** http://localhost:5000&quot;)&#10;    with col3:&#10;        if st.button(&quot;Export Report&quot;):&#10;            st.success(&quot;System report exported&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Client/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Client/requirements.txt" />
              <option name="updatedContent" value="torch==2.7.1&#10;safetensors==0.5.3&#10;requests&gt;=2.31.0&#10;psutil&gt;=5.9.0&#10;PyYAML==6.0.2&#10;cryptography&gt;=41.0.0&#10;numpy&gt;=1.24.0&#10;streamlit&gt;=1.28.0&#10;plotly&gt;=5.17.0&#10;pandas&gt;=2.1.0&#10;networkx&gt;=3.1&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Docs/API.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Docs/API.md" />
              <option name="originalContent" value="# NEXAPod API Reference&#10;&#10;## Overview&#10;&#10;The NEXAPod REST API provides endpoints for node registration, job submission, status monitoring, and credit management. All endpoints use JSON for request/response bodies and include comprehensive error handling.&#10;&#10;**Base URL**: `http://localhost:5000` (development)  &#10;**API Version**: `v1`  &#10;**Content-Type**: `application/json`  &#10;**Authentication**: Ed25519 signatures (future), API keys (MVP)&#10;&#10;## Core Endpoints&#10;&#10;### Node Management&#10;&#10;#### POST /register&#10;Register a new compute node with the system.&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;id&quot;: &quot;node_12345&quot;,&#10;  &quot;tier&quot;: &quot;CONSUMER_GPU&quot;,&#10;  &quot;profile&quot;: {&#10;    &quot;os&quot;: &quot;Linux&quot;,&#10;    &quot;os_version&quot;: &quot;Ubuntu 22.04&quot;,&#10;    &quot;processor&quot;: &quot;Intel Core i7-12700K&quot;,&#10;    &quot;cores&quot;: 12,&#10;    &quot;threads&quot;: 20,&#10;    &quot;ram_gb&quot;: 32,&#10;    &quot;gpu&quot;: [&#10;      {&#10;        &quot;model&quot;: &quot;NVIDIA RTX 3080&quot;,&#10;        &quot;memory_gb&quot;: 10&#10;      }&#10;    ],&#10;    &quot;storage_gb&quot;: 500,&#10;    &quot;network_mbps&quot;: 1000&#10;  },&#10;  &quot;public_key&quot;: &quot;ed25519_public_key_hex&quot;,&#10;  &quot;capabilities&quot;: [&quot;docker&quot;, &quot;gpu_compute&quot;, &quot;large_memory&quot;]&#10;}&#10;```&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;status&quot;: &quot;registered&quot;,&#10;  &quot;node_id&quot;: &quot;node_12345&quot;,&#10;  &quot;assigned_tier&quot;: &quot;CONSUMER_GPU&quot;,&#10;  &quot;reputation_score&quot;: 1.0,&#10;  &quot;registration_time&quot;: &quot;2024-01-01T12:00:00Z&quot;&#10;}&#10;```&#10;&#10;**Response** (400 Bad Request):&#10;```json&#10;{&#10;  &quot;error&quot;: &quot;INVALID_PROFILE&quot;,&#10;  &quot;message&quot;: &quot;Missing required field: processor&quot;,&#10;  &quot;details&quot;: {&#10;    &quot;missing_fields&quot;: [&quot;processor&quot;],&#10;    &quot;provided_fields&quot;: [&quot;os&quot;, &quot;cores&quot;, &quot;ram_gb&quot;]&#10;  }&#10;}&#10;```&#10;&#10;#### GET /nodes&#10;List all registered nodes (admin endpoint).&#10;&#10;**Query Parameters**:&#10;- `tier` (optional): Filter by node tier&#10;- `status` (optional): Filter by node status (available, busy, offline)&#10;- `limit` (optional): Maximum number of results (default: 100)&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;nodes&quot;: [&#10;    {&#10;      &quot;id&quot;: &quot;node_12345&quot;,&#10;      &quot;tier&quot;: &quot;CONSUMER_GPU&quot;, &#10;      &quot;status&quot;: &quot;available&quot;,&#10;      &quot;reputation&quot;: 0.95,&#10;      &quot;last_seen&quot;: &quot;2024-01-01T12:00:00Z&quot;,&#10;      &quot;jobs_completed&quot;: 142,&#10;      &quot;credits_earned&quot;: 1250.5&#10;    }&#10;  ],&#10;  &quot;total_count&quot;: 1,&#10;  &quot;available_count&quot;: 1&#10;}&#10;```&#10;&#10;#### GET /nodes/{node_id}&#10;Get detailed information about a specific node.&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;id&quot;: &quot;node_12345&quot;,&#10;  &quot;tier&quot;: &quot;CONSUMER_GPU&quot;,&#10;  &quot;profile&quot;: { /* full profile object */ },&#10;  &quot;status&quot;: &quot;available&quot;,&#10;  &quot;reputation&quot;: 0.95,&#10;  &quot;statistics&quot;: {&#10;    &quot;jobs_completed&quot;: 142,&#10;    &quot;jobs_failed&quot;: 3,&#10;    &quot;total_compute_hours&quot;: 876.5,&#10;    &quot;credits_earned&quot;: 1250.5&#10;  },&#10;  &quot;last_seen&quot;: &quot;2024-01-01T12:00:00Z&quot;,&#10;  &quot;registration_time&quot;: &quot;2023-12-01T08:30:00Z&quot;&#10;}&#10;```&#10;&#10;### Job Management&#10;&#10;#### POST /submit-job&#10;Submit a new computational job to the system.&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;id&quot;: &quot;job_001&quot;,&#10;  &quot;title&quot;: &quot;Protein Folding Simulation&quot;,&#10;  &quot;description&quot;: &quot;AlphaFold prediction for novel protein sequence&quot;,&#10;  &quot;docker_image&quot;: &quot;nexapod/alphafold:2.3.0&quot;,&#10;  &quot;compute_estimate&quot;: 1000000000,&#10;  &quot;requirements&quot;: {&#10;    &quot;min_tier&quot;: &quot;CONSUMER_GPU&quot;,&#10;    &quot;min_memory_gb&quot;: 8,&#10;    &quot;min_storage_gb&quot;: 5,&#10;    &quot;gpu_required&quot;: true,&#10;    &quot;timeout_seconds&quot;: 3600&#10;  },&#10;  &quot;input_files&quot;: [&quot;sequence.fasta&quot;, &quot;params.json&quot;],&#10;  &quot;input_uri&quot;: &quot;s3://nexapod-inputs/job_001/&quot;,&#10;  &quot;output_path&quot;: &quot;/results/&quot;,&#10;  &quot;validation&quot;: {&#10;    &quot;redundancy_factor&quot;: 2,&#10;    &quot;tolerance&quot;: 0.01&#10;  },&#10;  &quot;credits&quot;: {&#10;    &quot;rate&quot;: 1.0,&#10;    &quot;max_budget&quot;: 100.0&#10;  },&#10;  &quot;metadata&quot;: {&#10;    &quot;submitter&quot;: &quot;researcher_alice&quot;,&#10;    &quot;priority&quot;: &quot;normal&quot;,&#10;    &quot;tags&quot;: [&quot;protein_folding&quot;, &quot;alphafold&quot;]&#10;  }&#10;}&#10;```&#10;&#10;**Response** (202 Accepted):&#10;```json&#10;{&#10;  &quot;status&quot;: &quot;job submitted&quot;,&#10;  &quot;job_id&quot;: &quot;job_001&quot;, &#10;  &quot;queue_position&quot;: 3,&#10;  &quot;estimated_start_time&quot;: &quot;2024-01-01T12:05:00Z&quot;,&#10;  &quot;estimated_completion_time&quot;: &quot;2024-01-01T13:05:00Z&quot;&#10;}&#10;```&#10;&#10;#### GET /jobs/{job_id}&#10;Get detailed status of a specific job.&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;id&quot;: &quot;job_001&quot;,&#10;  &quot;status&quot;: &quot;executing&quot;,&#10;  &quot;progress&quot;: 0.65,&#10;  &quot;assigned_nodes&quot;: [&quot;node_12345&quot;, &quot;node_67890&quot;],&#10;  &quot;start_time&quot;: &quot;2024-01-01T12:05:30Z&quot;,&#10;  &quot;estimated_completion&quot;: &quot;2024-01-01T13:05:30Z&quot;,&#10;  &quot;resources_used&quot;: {&#10;    &quot;cpu_hours&quot;: 8.5,&#10;    &quot;gpu_hours&quot;: 2.1,&#10;    &quot;memory_gb_hours&quot;: 64.0&#10;  },&#10;  &quot;intermediate_results&quot;: {&#10;    &quot;checkpoints&quot;: 3,&#10;    &quot;last_update&quot;: &quot;2024-01-01T12:45:00Z&quot;&#10;  }&#10;}&#10;```&#10;&#10;#### GET /jobs&#10;List jobs with filtering and pagination.&#10;&#10;**Query Parameters**:&#10;- `status` (optional): Filter by job status  &#10;- `submitter` (optional): Filter by submitter&#10;- `limit` (optional): Maximum results (default: 50)&#10;- `offset` (optional): Pagination offset&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;jobs&quot;: [&#10;    {&#10;      &quot;id&quot;: &quot;job_001&quot;,&#10;      &quot;title&quot;: &quot;Protein Folding Simulation&quot;, &#10;      &quot;status&quot;: &quot;executing&quot;,&#10;      &quot;progress&quot;: 0.65,&#10;      &quot;submitter&quot;: &quot;researcher_alice&quot;,&#10;      &quot;created_at&quot;: &quot;2024-01-01T12:00:00Z&quot;,&#10;      &quot;credits_allocated&quot;: 85.5&#10;    }&#10;  ],&#10;  &quot;pagination&quot;: {&#10;    &quot;total&quot;: 156,&#10;    &quot;limit&quot;: 50,&#10;    &quot;offset&quot;: 0,&#10;    &quot;has_more&quot;: true&#10;  }&#10;}&#10;```&#10;&#10;### System Status&#10;&#10;#### GET /status&#10;Get overall system health and statistics.&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;system&quot;: {&#10;    &quot;status&quot;: &quot;healthy&quot;,&#10;    &quot;version&quot;: &quot;1.0.0&quot;,&#10;    &quot;uptime_seconds&quot;: 86400&#10;  },&#10;  &quot;nodes&quot;: {&#10;    &quot;total&quot;: 42,&#10;    &quot;available&quot;: 38,&#10;    &quot;busy&quot;: 4,&#10;    &quot;offline&quot;: 0&#10;  },&#10;  &quot;jobs&quot;: {&#10;    &quot;queued&quot;: 12,&#10;    &quot;executing&quot;: 7,&#10;    &quot;completed_today&quot;: 156,&#10;    &quot;failed_today&quot;: 2&#10;  },&#10;  &quot;compute&quot;: {&#10;    &quot;total_flops&quot;: &quot;2.5 TFLOPS&quot;,&#10;    &quot;active_flops&quot;: &quot;1.8 TFLOPS&quot;,&#10;    &quot;utilization_percent&quot;: 72&#10;  },&#10;  &quot;credits&quot;: {&#10;    &quot;total_issued&quot;: 15420.5,&#10;    &quot;total_pending&quot;: 342.8,&#10;    &quot;average_rate&quot;: 12.5&#10;  }&#10;}&#10;```&#10;&#10;#### GET /metrics&#10;Prometheus-compatible metrics endpoint.&#10;&#10;**Response** (200 OK):&#10;```&#10;# HELP nexapod_nodes_total Total number of registered nodes&#10;# TYPE nexapod_nodes_total gauge&#10;nexapod_nodes_total{tier=&quot;CPU&quot;} 15&#10;nexapod_nodes_total{tier=&quot;CONSUMER_GPU&quot;} 25&#10;nexapod_nodes_total{tier=&quot;HPC&quot;} 2&#10;&#10;# HELP nexapod_jobs_total Total number of jobs by status&#10;# TYPE nexapod_jobs_total counter&#10;nexapod_jobs_total{status=&quot;completed&quot;} 1247&#10;nexapod_jobs_total{status=&quot;failed&quot;} 23&#10;```&#10;&#10;### Credit System&#10;&#10;#### GET /credits/{node_id}&#10;Get credit balance and transaction history for a node.&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;node_id&quot;: &quot;node_12345&quot;,&#10;  &quot;current_balance&quot;: 1250.5,&#10;  &quot;total_earned&quot;: 1385.0,&#10;  &quot;total_spent&quot;: 134.5,&#10;  &quot;transactions&quot;: [&#10;    {&#10;      &quot;id&quot;: &quot;tx_789abc&quot;,&#10;      &quot;type&quot;: &quot;earned&quot;,&#10;      &quot;amount&quot;: 14.5,&#10;      &quot;job_id&quot;: &quot;job_001&quot;,&#10;      &quot;timestamp&quot;: &quot;2024-01-01T13:05:30Z&quot;,&#10;      &quot;description&quot;: &quot;Job completion bonus&quot;&#10;    }&#10;  ],&#10;  &quot;earning_rate_24h&quot;: 125.5,&#10;  &quot;ranking&quot;: 15&#10;}&#10;```&#10;&#10;#### POST /credits/transfer&#10;Transfer credits between nodes (future feature).&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;from_node&quot;: &quot;node_12345&quot;,&#10;  &quot;to_node&quot;: &quot;node_67890&quot;, &#10;  &quot;amount&quot;: 50.0,&#10;  &quot;memo&quot;: &quot;Payment for collaborative computation&quot;,&#10;  &quot;signature&quot;: &quot;ed25519_signature&quot;&#10;}&#10;```&#10;&#10;### Administrative Endpoints&#10;&#10;#### GET /admin/logs&#10;Retrieve system logs (admin only).&#10;&#10;**Query Parameters**:&#10;- `level` (optional): Log level filter (debug, info, warn, error)&#10;- `since` (optional): ISO timestamp for log start time&#10;- `component` (optional): Filter by system component&#10;&#10;#### POST /admin/maintenance&#10;Put system into maintenance mode.&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;mode&quot;: &quot;maintenance&quot;,&#10;  &quot;message&quot;: &quot;System upgrade in progress&quot;,&#10;  &quot;estimated_duration_minutes&quot;: 30&#10;}&#10;```&#10;&#10;## Error Responses&#10;&#10;### Standard Error Format&#10;&#10;All error responses follow this format:&#10;&#10;```json&#10;{&#10;  &quot;error&quot;: {&#10;    &quot;code&quot;: &quot;ERROR_CODE&quot;,&#10;    &quot;message&quot;: &quot;Human-readable error description&quot;,&#10;    &quot;details&quot;: {&#10;      &quot;field&quot;: &quot;additional_context&quot;,&#10;      &quot;timestamp&quot;: &quot;2024-01-01T12:00:00Z&quot;&#10;    },&#10;    &quot;request_id&quot;: &quot;req_123456&quot;&#10;  }&#10;}&#10;```&#10;&#10;### Common Error Codes&#10;&#10;| HTTP Status | Error Code            | Description                                 |&#10;|-------------|-----------------------|---------------------------------------------|&#10;| 400         | `INVALID_REQUEST`     | Malformed request body or parameters        |&#10;| 401         | `UNAUTHORIZED`        | Missing or invalid authentication           |&#10;| 403         | `FORBIDDEN`           | Insufficient permissions                    |&#10;| 404         | `NOT_FOUND`           | Resource does not exist                     |&#10;| 409         | `CONFLICT`            | Resource conflict (e.g., duplicate node ID) |&#10;| 422         | `VALIDATION_ERROR`    | Request validation failed                   |&#10;| 429         | `RATE_LIMITED`        | Too many requests                           |&#10;| 500         | `INTERNAL_ERROR`      | Server-side error                           |&#10;| 503         | `SERVICE_UNAVAILABLE` | System in maintenance mode                  |&#10;&#10;## Authentication &amp; Security&#10;&#10;### API Key Authentication (MVP)&#10;&#10;Include API key in request headers:&#10;```&#10;Authorization: Bearer your_api_key_here&#10;```&#10;&#10;### Ed25519 Signature Authentication (Future)&#10;&#10;1. Generate message signature:&#10;```python&#10;message = canonical_json(request_body)&#10;signature = node_private_key.sign(message.encode())&#10;```&#10;&#10;2. Include in headers:&#10;```&#10;X-Node-ID: node_12345&#10;X-Signature: hex_encoded_signature&#10;X-Timestamp: unix_timestamp&#10;```&#10;&#10;### Rate Limiting&#10;&#10;| Endpoint      | Rate Limit            |&#10;|---------------|-----------------------|&#10;| `/register`   | 10 per hour per IP    |&#10;| `/submit-job` | 100 per hour per user |&#10;| `/status`     | 1000 per hour per IP  |&#10;| `/jobs/*`     | 500 per hour per user |&#10;&#10;## SDK Examples&#10;&#10;### Python Client&#10;&#10;```python&#10;import requests&#10;from nexapod_client import NexaPodClient&#10;&#10;# Initialize client&#10;client = NexaPodClient(&#10;    base_url=&quot;http://localhost:5000&quot;,&#10;    api_key=&quot;your_api_key&quot;&#10;)&#10;&#10;# Register node&#10;profile = client.get_node_profile()&#10;response = client.register_node(profile)&#10;&#10;# Submit job&#10;job = {&#10;    &quot;id&quot;: &quot;my_job_001&quot;,&#10;    &quot;docker_image&quot;: &quot;tensorflow/tensorflow:latest&quot;,&#10;    &quot;requirements&quot;: {&quot;min_memory_gb&quot;: 8}&#10;}&#10;response = client.submit_job(job)&#10;&#10;# Check status&#10;status = client.get_job_status(&quot;my_job_001&quot;)&#10;```&#10;&#10;### cURL Examples&#10;&#10;```bash&#10;# Register node&#10;curl -X POST http://localhost:5000/register \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -H &quot;Authorization: Bearer your_api_key&quot; \&#10;  -d '{&quot;id&quot;: &quot;node_001&quot;, &quot;tier&quot;: &quot;CPU&quot;, &quot;profile&quot;: {...}}'&#10;&#10;# Submit job  &#10;curl -X POST http://localhost:5000/submit-job \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;id&quot;: &quot;job_001&quot;, &quot;docker_image&quot;: &quot;hello-world&quot;}'&#10;&#10;# Get system status&#10;curl http://localhost:5000/status&#10;```&#10;&#10;## Integration Patterns&#10;&#10;### Webhook Notifications (Future)&#10;&#10;Configure webhooks to receive job status updates:&#10;&#10;```json&#10;{&#10;  &quot;webhook_url&quot;: &quot;https://your-service.com/nexapod-webhook&quot;,&#10;  &quot;events&quot;: [&quot;job.completed&quot;, &quot;job.failed&quot;, &quot;node.offline&quot;],&#10;  &quot;secret&quot;: &quot;webhook_signature_secret&quot;&#10;}&#10;```&#10;&#10;### Streaming API (Future)&#10;&#10;WebSocket endpoint for real-time updates:&#10;&#10;```javascript&#10;const ws = new WebSocket('ws://localhost:5000/stream');&#10;ws.onmessage = (event) =&gt; {&#10;  const update = JSON.parse(event.data);&#10;  // Handle real-time job/node updates&#10;};&#10;```&#10;&#10;---&#10;&#10;*This API reference covers the current implementation.*  &#10;*For message schemas, see [PROTOCOL.md](PROTOCOL.md).*  &#10;*For system architecture, see [ARCHITECTURE.md](ARCHITECTURE.md).*  &#10;*For complete documentation, see [Doc.md](Doc.md).*&#10;" />
              <option name="updatedContent" value="# NEXAPod API Reference (Alpha v1)&#10;&#10;## Overview&#10;&#10;The NEXAPod REST API provides core endpoints for node registration, job distribution, and result submission.&#10;&#10;**Base URL**: `http://&lt;your-server-ip&gt;:8000`  &#10;**Content-Type**: `application/json`&#10;&#10;---&#10;&#10;## Endpoints&#10;&#10;### Node Management&#10;&#10;#### `POST /register`&#10;Registers a new compute node with the coordinator. The node sends its profile, including a unique ID and hardware capabilities.&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;node_id&quot;: &quot;unique_node_identifier_string&quot;,&#10;  &quot;profile&quot;: {&#10;    &quot;os&quot;: &quot;Linux&quot;,&#10;    &quot;cpu&quot;: &quot;Intel Core i7-12700K&quot;,&#10;    &quot;gpu&quot;: &quot;NVIDIA RTX 3080&quot;,&#10;    &quot;hashes&quot;: {&#10;        &quot;runner&quot;: &quot;sha256_hash_of_runner.py&quot;,&#10;        &quot;weights&quot;: &quot;sha256_hash_of_model.safetensors&quot;&#10;    }&#10;  }&#10;}&#10;```&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;status&quot;: &quot;registered&quot;,&#10;  &quot;node_id&quot;: &quot;unique_node_identifier_string&quot;&#10;}&#10;```&#10;&#10;### Job Management&#10;&#10;#### `GET /job`&#10;A registered node polls this endpoint to request a compute job.&#10;&#10;**Request Headers**:&#10;- `X-Node-ID`: The ID of the node requesting the job.&#10;&#10;**Response** (200 OK, with a job):&#10;```json&#10;{&#10;  &quot;job_id&quot;: &quot;job_uuid_string&quot;,&#10;  &quot;status&quot;: &quot;assigned&quot;,&#10;  &quot;data&quot;: { ...job specific data... },&#10;  &quot;assigned_to&quot;: &quot;requesting_node_id&quot;&#10;}&#10;```&#10;&#10;**Response** (200 OK, no job available):&#10;```json&#10;{&#10;  &quot;status&quot;: &quot;no_job_available&quot;&#10;}&#10;```&#10;&#10;#### `POST /result`&#10;A node submits the result of a completed job.&#10;&#10;**Request Body**:&#10;```json&#10;{&#10;  &quot;job_id&quot;: &quot;job_uuid_string&quot;,&#10;  &quot;result&quot;: { ...job result data... },&#10;  &quot;node_id&quot;: &quot;completing_node_id&quot;&#10;}&#10;```&#10;&#10;**Response** (200 OK):&#10;```json&#10;{&#10;  &quot;status&quot;: &quot;result_received&quot;,&#10;  &quot;job_id&quot;: &quot;job_uuid_string&quot;&#10;}&#10;```&#10;&#10;### Dashboard Endpoints&#10;&#10;#### `GET /nodes`&#10;Provides a list of all registered nodes for display on the dashboard.&#10;&#10;**Response** (200 OK):&#10;```json&#10;[&#10;  {&#10;    &quot;id&quot;: &quot;node_123&quot;,&#10;    &quot;profile&quot;: { ... }&#10;  }&#10;]&#10;```&#10;&#10;#### `GET /jobs`&#10;Provides a list of all jobs (pending, assigned, completed) for the dashboard.&#10;&#10;**Response** (200 OK):&#10;```json&#10;[&#10;  {&#10;    &quot;job_id&quot;: &quot;job_abc&quot;,&#10;    &quot;status&quot;: &quot;completed&quot;,&#10;    &quot;assigned_to&quot;: &quot;node_123&quot;,&#10;    &quot;result&quot;: { ... }&#10;  }&#10;]&#10;```" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Docs/ARCHITECTURE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Docs/ARCHITECTURE.md" />
              <option name="originalContent" value="# NEXAPod Architecture&#10;&#10;## Overview&#10;&#10;NEXAPod is a distributed compute fabric for scientific workloads. It coordinates jobs between a central server and many client nodes, using cryptographically signed logs and a credit system for fair compute allocation.&#10;&#10;## System Architecture&#10;&#10;### High-Level Component Diagram&#10;&#10;```&#10;┌─────────────────────────────────────────────────────────────────┐&#10;│                        NEXAPod System                           │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Frontend Layer                                                │&#10;│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │&#10;│  │   Dashboard     │  │ Contributor Wall│  │   API Client    │ │&#10;│  │   (Streamlit)   │  │     (HTML)      │  │   (REST/CLI)    │ │&#10;│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  API Layer                                                     │&#10;│  ┌─────────────────────────────────────────────────────────────┐ │&#10;│  │                    REST API (Flask)                        │ │&#10;│  │  /register  │  /submit-job  │  /status  │  /credits        │ │&#10;│  └─────────────────────────────────────────────────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Core Engine                                                   │&#10;│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────┐ │&#10;│  │  Scheduler  │  │  Validator  │  │   Security  │  │ Ledger  │ │&#10;│  │   Queue     │  │   Engine    │  │   Manager   │  │ System  │ │&#10;│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Data Layer                                                    │&#10;│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │&#10;│  │   Node DB       │  │    Job DB       │  │   Credit DB     │ │&#10;│  │   (SQLite)      │  │   (SQLite)      │  │   (SQLite)      │ │&#10;│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Execution Layer                                               │&#10;│  ┌─────────────────────────────────────────────────────────────┐ │&#10;│  │                Container Runtime (Docker)                   │ │&#10;│  │  Input Fetcher │ Job Executor │ Output Archiver │ Logger   │ │&#10;│  └─────────────────────────────────────────────────────────────┘ │&#10;└─────────────────────────────────────────────────────────────────┘&#10;```&#10;&#10;### Detailed Dataflow&#10;&#10;```&#10;1. Node Registration Flow&#10;   Client Node → [Hardware Profile] → API → Database&#10;                                     ↓&#10;   Client Node ← [Node ID + Status] ← API ← Scheduler&#10;&#10;2. Job Submission Flow&#10;   Researcher → [Job Descriptor] → API → Job Queue&#10;                                         ↓&#10;   Node Pool ← [Job Assignment] ← Scheduler ← Job Matcher&#10;&#10;3. Job Execution Flow&#10;   Node → [Pull Image] → Docker → [Execute] → [Results]&#10;    ↓                                            ↓&#10;   [Log Job] ← [Sign Result] ← [Validate] ← [Hash Check]&#10;&#10;4. Result Verification Flow&#10;   Node A → [Result Hash A] → Validator ← [Result Hash B] ← Node B&#10;                                ↓&#10;   Database ← [Store Result] ← [Compare Hashes] → [Update Reputation]&#10;&#10;5. Credit Distribution Flow&#10;   Verified Result → Credit Calculator → Ledger → [Update Balances]&#10;                                         ↓&#10;   Dashboard ← [Display Credits] ← API ← Database&#10;```&#10;&#10;### Node State Machine&#10;&#10;```&#10;[Unregistered] → register() → [Registered]&#10;      ↓                           ↓&#10;[Verification] ← verify() ← [Pending Verification]&#10;      ↓                           ↓&#10;[Available] → assign_job() → [Busy]&#10;      ↑                           ↓&#10;[Complete] ← submit_result() ← [Executing]&#10;      ↓&#10;[Reputation Update] → [Available]&#10;```&#10;&#10;## Component Details&#10;&#10;### 1. Scheduler Architecture&#10;&#10;```python&#10;class Scheduler:&#10;    - job_queue: Queue[JobDescriptor]&#10;    - node_pool: Dict[str, Node]&#10;    - busy_nodes: Set[str]&#10;    &#10;    Methods:&#10;    - match_nodes(job) → List[Node]&#10;    - assign_job(job, nodes) → JobAssignment&#10;    - handle_result(result) → ValidationResult&#10;```&#10;&#10;### 2. Security Architecture&#10;&#10;```&#10;Ed25519 Keypairs:&#10;├── Node Identity Keys (per node)&#10;├── Job Signing Keys (per job)&#10;└── Coordinator Master Key&#10;&#10;Validation Chain:&#10;Job → Sign(NodeKey) → Verify(CoordinatorKey) → Store(Database)&#10;```&#10;&#10;### 3. Database Schema&#10;&#10;```sql&#10;-- Nodes table&#10;CREATE TABLE nodes (&#10;    id TEXT PRIMARY KEY,&#10;    tier TEXT,&#10;    profile TEXT,&#10;    reputation REAL DEFAULT 1.0,&#10;    last_seen TIMESTAMP,&#10;    status TEXT DEFAULT 'available'&#10;);&#10;&#10;-- Jobs table  &#10;CREATE TABLE jobs (&#10;    id TEXT PRIMARY KEY,&#10;    descriptor TEXT,&#10;    status TEXT,&#10;    assigned_nodes TEXT,&#10;    result TEXT,&#10;    created_at TIMESTAMP&#10;);&#10;&#10;-- Ledger table (append-only)&#10;CREATE TABLE ledger (&#10;    id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;    event_type TEXT,&#10;    node_id TEXT,&#10;    job_id TEXT,&#10;    credits REAL,&#10;    signature TEXT,&#10;    timestamp TIMESTAMP&#10;);&#10;```&#10;&#10;## Scalability Considerations&#10;&#10;### Horizontal Scaling&#10;&#10;```&#10;Single Coordinator (MVP):&#10;API Server ← → Database ← → N Compute Nodes&#10;&#10;Multi-Coordinator (v2):&#10;Load Balancer → Multiple API Servers → Shared Database → N Compute Nodes&#10;&#10;P2P Mesh (v3):&#10;Coordinator Nodes ← → P2P Network ← → Compute Nodes&#10;```&#10;&#10;### Performance Bottlenecks&#10;&#10;1. **Database**: SQLite → PostgreSQL for concurrent access&#10;2. **Job Queue**: In-memory → Redis/RabbitMQ for persistence&#10;3. **File Transfer**: HTTP → IPFS/BitTorrent for large datasets&#10;4. **Validation**: Serial → Parallel hash verification&#10;&#10;## Security Model&#10;&#10;### Threat Model&#10;&#10;| Threat           | Mitigation                                |&#10;|------------------|-------------------------------------------|&#10;| Malicious Nodes  | Redundant execution + hash comparison     |&#10;| Result Tampering | Ed25519 signatures + immutable ledger     |&#10;| Sybil Attacks    | Reputation requirements + stake bonding   |&#10;| DoS Attacks      | Rate limiting + resource quotas           |&#10;| Data Leaks       | Container isolation + encrypted transport |&#10;&#10;### Trust Boundaries&#10;&#10;```&#10;Trusted:&#10;- Coordinator infrastructure&#10;- Cryptographic primitives&#10;- Container runtime isolation&#10;&#10;Semi-Trusted:&#10;- Registered nodes (reputation-based)&#10;- Job submitters (with validation)&#10;&#10;Untrusted:&#10;- Network transport&#10;- External data sources&#10;- Unregistered nodes&#10;```&#10;&#10;## Installation &amp; Setup&#10;&#10;### Quick Start (Docker Compose)&#10;```bash&#10;# Clone and change directory&#10;git clone https://github.com/your-org/nexapod.git&#10;cd nexapod&#10;&#10;# Build and start all services&#10;docker-compose up --build -d&#10;```&#10;&#10;### Quick Start (Kubernetes)&#10;```bash&#10;# Apply all manifests&#10;kubectl apply -f Infrastruture/k8s/&#10;&#10;# Verify deployments&#10;kubectl rollout status deployment/nexapod-server&#10;kubectl rollout status deployment/nexapod-client&#10;```&#10;&#10;## Onboarding &amp; First Contribution&#10;&#10;1. Generate or ensure you have an Ed25519 private key:&#10;   ```bash&#10;   python Client/nexapod_client.py join&#10;   ```&#10;   - This will generate `~/.nexapod/client_ed25519.key` and register your node with the coordinator.&#10;2. Start the client runner:&#10;   ```bash&#10;   python Client/nexapod_client.py run&#10;   ```&#10;3. Monitor your node metrics:&#10;   - Prometheus scrape at http://&lt;client-host&gt;:9000/metrics&#10;4. Submit your first job as a researcher:&#10;   ```bash&#10;   curl -X POST http://&lt;coordinator-host&gt;:8000/jobs \&#10;     -H &quot;Content-Type: application/json&quot; \&#10;     -d '{&quot;job_id&quot;:&quot;job_001&quot;,&quot;docker_image&quot;:&quot;python:3.9&quot;,&quot;requirements&quot;:{&quot;ram_gb&quot;:1.0}}'&#10;   ```&#10;5. Observe assignment, execution, and quorum finalization in logs and dashboard.&#10;````&#10;" />
              <option name="updatedContent" value="# NEXAPod Architecture&#10;&#10;## Overview&#10;&#10;NEXAPod is a distributed compute fabric for scientific workloads. It coordinates jobs between a central server and many client nodes, using cryptographically signed logs and a credit system for fair compute allocation.&#10;&#10;## System Architecture&#10;&#10;### High-Level Component Diagram&#10;&#10;```&#10;┌─────────────────────────────────────────────────────────────────┐&#10;│                        NEXAPod System                           │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Frontend Layer                                                │&#10;│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │&#10;│  │   Dashboard     │  │ Contributor Wall│  │   API Client    │ │&#10;│  │   (Streamlit)   │  │     (HTML)      │  │   (REST/CLI)    │ │&#10;│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  API Layer                                                     │&#10;│  ┌─────────────────────────────────────────────────────────────┐ │&#10;│  │                    REST API (Flask)                        │ │&#10;│  │  /register  │  /submit-job  │  /status  │  /credits        │ │&#10;│  └─────────────────────────────────────────────────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Core Engine                                                   │&#10;│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────┐ │&#10;│  │  Scheduler  │  │  Validator  │  │   Security  │  │ Ledger  │ │&#10;│  │   Queue     │  │   Engine    │  │   Manager   │  │ System  │ │&#10;│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Data Layer                                                    │&#10;│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │&#10;│  │   Node DB       │  │    Job DB       │  │   Credit DB     │ │&#10;│  │   (SQLite)      │  │   (SQLite)      │  │   (SQLite)      │ │&#10;│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │&#10;├─────────────────────────────────────────────────────────────────┤&#10;│  Execution Layer                                               │&#10;│  ┌─────────────────────────────────────────────────────────────┐ │&#10;│  │                Container Runtime (Docker)                   │ │&#10;│  │  Input Fetcher │ Job Executor │ Output Archiver │ Logger   │ │&#10;│  └─────────────────────────────────────────────────────────────┘ │&#10;└─────────────────────────────────────────────────────────────────┘&#10;```&#10;&#10;### Detailed Dataflow&#10;&#10;```&#10;1. Node Registration Flow&#10;   Client Node → [Hardware Profile] → API → Database&#10;                                     ↓&#10;   Client Node ← [Node ID + Status] ← API ← Scheduler&#10;&#10;2. Job Submission Flow&#10;   Researcher → [Job Descriptor] → API → Job Queue&#10;                                         ↓&#10;   Node Pool ← [Job Assignment] ← Scheduler ← Job Matcher&#10;&#10;3. Job Execution Flow&#10;   Node → [Pull Image] → Docker → [Execute] → [Results]&#10;    ↓                                            ↓&#10;   [Log Job] ← [Sign Result] ← [Validate] ← [Hash Check]&#10;&#10;4. Result Verification Flow&#10;   Node A → [Result Hash A] → Validator ← [Result Hash B] ← Node B&#10;                                ↓&#10;   Database ← [Store Result] ← [Compare Hashes] → [Update Reputation]&#10;&#10;5. Credit Distribution Flow&#10;   Verified Result → Credit Calculator → Ledger → [Update Balances]&#10;                                         ↓&#10;   Dashboard ← [Display Credits] ← API ← Database&#10;```&#10;&#10;### Node State Machine&#10;&#10;```&#10;[Unregistered] → register() → [Registered]&#10;      ↓                           ↓&#10;[Verification] ← verify() ← [Pending Verification]&#10;      ↓                           ↓&#10;[Available] → assign_job() → [Busy]&#10;      ↑                           ↓&#10;[Complete] ← submit_result() ← [Executing]&#10;      ↓&#10;[Reputation Update] → [Available]&#10;```&#10;&#10;## Component Details&#10;&#10;### 1. Scheduler Architecture&#10;&#10;```python&#10;class Scheduler:&#10;    - job_queue: Queue[JobDescriptor]&#10;    - node_pool: Dict[str, Node]&#10;    - busy_nodes: Set[str]&#10;    &#10;    Methods:&#10;    - match_nodes(job) → List[Node]&#10;    - assign_job(job, nodes) → JobAssignment&#10;    - handle_result(result) → ValidationResult&#10;```&#10;&#10;### 2. Security Architecture&#10;&#10;```&#10;Ed25519 Keypairs:&#10;├── Node Identity Keys (per node)&#10;├── Job Signing Keys (per job)&#10;└── Coordinator Master Key&#10;&#10;Validation Chain:&#10;Job → Sign(NodeKey) → Verify(CoordinatorKey) → Store(Database)&#10;```&#10;&#10;### 3. Database Schema&#10;&#10;```sql&#10;-- Nodes table&#10;CREATE TABLE nodes (&#10;    id TEXT PRIMARY KEY,&#10;    tier TEXT,&#10;    profile TEXT,&#10;    reputation REAL DEFAULT 1.0,&#10;    last_seen TIMESTAMP,&#10;    status TEXT DEFAULT 'available'&#10;);&#10;&#10;-- Jobs table  &#10;CREATE TABLE jobs (&#10;    id TEXT PRIMARY KEY,&#10;    descriptor TEXT,&#10;    status TEXT,&#10;    assigned_nodes TEXT,&#10;    result TEXT,&#10;    created_at TIMESTAMP&#10;);&#10;&#10;-- Ledger table (append-only)&#10;CREATE TABLE ledger (&#10;    id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;    event_type TEXT,&#10;    node_id TEXT,&#10;    job_id TEXT,&#10;    credits REAL,&#10;    signature TEXT,&#10;    timestamp TIMESTAMP&#10;);&#10;```&#10;&#10;## Scalability Considerations&#10;&#10;### Horizontal Scaling&#10;&#10;```&#10;Single Coordinator (MVP):&#10;API Server ← → Database ← → N Compute Nodes&#10;&#10;Multi-Coordinator (v2):&#10;Load Balancer → Multiple API Servers → Shared Database → N Compute Nodes&#10;&#10;P2P Mesh (v3):&#10;Coordinator Nodes ← → P2P Network ← → Compute Nodes&#10;```&#10;&#10;### Performance Bottlenecks&#10;&#10;1. **Database**: SQLite → PostgreSQL for concurrent access&#10;2. **Job Queue**: In-memory → Redis/RabbitMQ for persistence&#10;3. **File Transfer**: HTTP → IPFS/BitTorrent for large datasets&#10;4. **Validation**: Serial → Parallel hash verification&#10;&#10;## Security Model&#10;&#10;### Threat Model&#10;&#10;| Threat           | Mitigation                                |&#10;|------------------|-------------------------------------------|&#10;| Malicious Nodes  | Redundant execution + hash comparison     |&#10;| Result Tampering | Ed25519 signatures + immutable ledger     |&#10;| Sybil Attacks    | Reputation requirements + stake bonding   |&#10;| DoS Attacks      | Rate limiting + resource quotas           |&#10;| Data Leaks       | Container isolation + encrypted transport |&#10;&#10;### Trust Boundaries&#10;&#10;```&#10;Trusted:&#10;- Coordinator infrastructure&#10;- Cryptographic primitives&#10;- Container runtime isolation&#10;&#10;Semi-Trusted:&#10;- Registered nodes (reputation-based)&#10;- Job submitters (with validation)&#10;&#10;Untrusted:&#10;- Network transport&#10;- External data sources&#10;- Unregistered nodes&#10;```&#10;&#10;## Installation &amp; Setup&#10;&#10;For detailed instructions on how to join the network as a contributor, please see the official **[ONBOARDING.md](ONBOARDING.md)** guide. The primary method is using the `nexapod` CLI tool.&#10;&#10;### Quick Start (Docker Compose)&#10;```bash&#10;# Clone and change directory&#10;git clone https://github.com/your-org/nexapod.git&#10;cd nexapod&#10;&#10;# Build and start all services&#10;docker-compose up --build -d&#10;```&#10;&#10;### Quick Start (Kubernetes)&#10;```bash&#10;# Apply all manifests&#10;kubectl apply -f Infrastruture/k8s/&#10;&#10;# Verify deployments&#10;kubectl rollout status deployment/nexapod-server&#10;kubectl rollout status deployment/nexapod-client&#10;```&#10;&#10;## Onboarding &amp; First Contribution&#10;&#10;The official onboarding process is detailed in **[ONBOARDING.md](ONBOARDING.md)**. The steps below are a technical summary.&#10;&#10;1. Install the `nexapod` CLI from the root of this repository.&#10;2. Configure your coordinator endpoint in `~/.nexapod/config.yaml`.&#10;3. Pull the latest client image:&#10;   ```bash&#10;   nexapod --pull&#10;   ```&#10;4. Start the client runner:&#10;   ```bash&#10;   nexapod&#10;   ```&#10;5. Monitor your node via the dashboard and local logs.&#10;6. To submit a job (as a researcher), use the API:&#10;   ```bash&#10;   # This is a conceptual example. The server needs a mechanism to create jobs.&#10;   curl -X POST http://&lt;coordinator-host&gt;:8000/jobs \&#10;     -H &quot;Content-Type: application/json&quot; \&#10;     -d '{&quot;job_id&quot;:&quot;job_001&quot;,&quot;docker_image&quot;:&quot;python:3.9&quot;,&quot;requirements&quot;:{&quot;ram_gb&quot;:1.0}}'&#10;   ```&#10;7. Observe assignment, execution, and finalization in logs and the dashboard." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Docs/Doc.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Docs/Doc.md" />
              <option name="originalContent" value="# NexaPod: Comprehensive Technical Documentation&#10;&#10;## Table of Contents&#10;1. [Project Overview](#project-overview)&#10;2. [Architecture](#architecture)&#10;3. [Core Components](#core-components)&#10;4. [System Workflow](#system-workflow)&#10;5. [Development Phases](#development-phases)&#10;6. [API Reference](#api-reference)&#10;7. [Security Model](#security-model)&#10;8. [Installation &amp; Setup](#installation--setup)&#10;9. [Testing](#testing)&#10;10. [Contributing](#contributing)&#10;&#10;---&#10;&#10;## Project Overview&#10;&#10;**NexaPod** is a decentralized, field-agnostic compute mesh designed to coordinate heterogeneous computational resources for large-scale scientific problems. The system enables researchers to submit computational jobs that are distributed across a network of volunteer nodes, ranging from consumer GPUs to high-performance computing clusters.&#10;&#10;### Mission Statement&#10;*Democratize access to computational resources for scientific research by creating a permissionless, trustless, and incentivized distributed computing platform.*&#10;&#10;### Key Features&#10;- **Field Agnostic**: Supports any scientific workload (weather modeling, quantum simulation, materials science, etc.)&#10;- **Decentralized**: No single point of failure, peer-to-peer communication&#10;- **Trustless**: Cryptographic verification and redundant execution&#10;- **Incentivized**: Token-based reward system for compute contributors&#10;- **Scalable**: Automatic node profiling and intelligent job scheduling&#10;&#10;---&#10;&#10;## Architecture&#10;&#10;For detailed architecture documentation, see **[ARCHITECTURE.md](ARCHITECTURE.md)**.&#10;&#10;### High-Level System Overview&#10;&#10;```&#10;┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐&#10;│   Job Submitter │    │   Coordinator   │    │  Compute Nodes  │&#10;│                 │    │                 │    │                 │&#10;│ • Scientists    │◄──►│ • Job Queue     │◄──►│ • CPU Nodes     │&#10;│ • Researchers   │    │ • Scheduler     │    │ • GPU Nodes     │&#10;│ • Institutions  │    │ • Validator     │    │ • HPC Clusters  │&#10;└─────────────────┘    └─────────────────┘    └─────────────────┘&#10;         │                       │                       │&#10;         │                       │                       │&#10;         └───────────────────────┼───────────────────────┘&#10;                                 │&#10;                    ┌─────────────────┐&#10;                    │   Dashboard     │&#10;                    │                 │&#10;                    │ • Live Status   │&#10;                    │ • Contributor   │&#10;                    │   Wall          │&#10;                    │ • Network Graph │&#10;                    └─────────────────┘&#10;```&#10;&#10;### Technology Stack Summary&#10;&#10;| Layer             | Component        | Technology          |&#10;|-------------------|------------------|---------------------|&#10;| **Frontend**      | Dashboard        | Streamlit           |&#10;| **Frontend**      | Contributor Wall | HTML/CSS/JS         |&#10;| **API**           | REST Endpoints   | Flask               |&#10;| **Core**          | Job Scheduler    | Python + Threading  |&#10;| **Data**          | Database         | SQLite → PostgreSQL |&#10;| **Security**      | Cryptography     | Ed25519 + SHA-256   |&#10;| **Execution**     | Containers       | Docker              |&#10;| **Communication** | Mesh Network     | libp2p (planned)    |&#10;&#10;---&#10;&#10;## Core Components&#10;&#10;### 1. Tier System (`nexapod/tier.py`)&#10;Classifies compute nodes based on their capabilities:&#10;&#10;```python&#10;class Tier(Enum):&#10;    CPU = 'CPU'              # Basic CPU nodes&#10;    CONSUMER_GPU = 'Consumer GPU'  # Gaming/workstation GPUs&#10;    HPC = 'HPC'              # High-performance clusters&#10;```&#10;&#10;### 2. Node Management (`nexapod/node.py`)&#10;Handles node registration and profiling:&#10;- **Automatic Profiling**: Detects OS, CPU, memory, and GPU capabilities&#10;- **Registration**: Stores node metadata in the database&#10;- **Status Tracking**: Monitors node availability and health&#10;&#10;### 3. Job Scheduler (`nexapod/scheduler.py`)&#10;Core scheduling engine with the following features:&#10;- **Job Queue**: FIFO queue for incoming computational tasks&#10;- **Node Matching**: Intelligent assignment based on job requirements&#10;- **Redundancy**: Executes jobs on multiple nodes for verification&#10;- **Conflict Prevention**: Tracks busy nodes to prevent resource conflicts&#10;&#10;#### Scheduling Algorithm&#10;```&#10;1. Job arrives in queue&#10;2. Find 2+ available nodes matching job requirements&#10;3. Verify nodes are trusted and not busy&#10;4. Execute job on selected nodes in parallel&#10;5. Compare results via hash validation&#10;6. Accept result if hashes match, else flag discrepancy&#10;7. Mark nodes as available and log result&#10;```&#10;&#10;### 4. Database Layer (`nexapod/database.py`)&#10;SQLite-based persistence with three main tables:&#10;- **Nodes**: Store node profiles, tiers, and status&#10;- **Jobs**: Track job metadata and results&#10;- **Logs**: Append-only audit trail for all operations&#10;&#10;### 5. Validation System (`nexapod/validator.py`)&#10;Ensures job integrity through:&#10;- **HMAC Signatures**: Cryptographic verification of job logs&#10;- **Hash Comparison**: Validates identical results from redundant execution&#10;- **Result Verification**: User-defined checkers for output validation&#10;&#10;### 6. Container Runtime (`nexapod/runner.py`)&#10;Secure job execution environment:&#10;- **Docker Integration**: Runs jobs in isolated containers&#10;- **Volume Mounting**: Manages input/output file systems&#10;- **Resource Limits**: Enforces CPU/memory constraints&#10;- **Result Collection**: Captures job outputs and logs&#10;&#10;### 7. API Layer (`nexapod/api.py`)&#10;RESTful endpoints for system interaction. For complete API documentation, see **[API.md](API.md)**.&#10;&#10;| Endpoint             | Method | Purpose                   |&#10;|----------------------|--------|---------------------------|&#10;| `/register`          | POST   | Register new compute node |&#10;| `/submit-job`        | POST   | Submit computational job  |&#10;| `/status`            | GET    | Query system status       |&#10;| `/jobs/{id}`         | GET    | Get job details           |&#10;| `/nodes`             | GET    | List registered nodes     |&#10;| `/credits/{node_id}` | GET    | Get credit balance        |&#10;&#10;### 8. Dashboard (`dashboard.py`)&#10;Real-time system monitoring:&#10;- **Network Visualization**: Interactive graph of connected nodes&#10;- **Job Status**: Live tracking of submitted jobs&#10;- **Contributor Metrics**: Performance statistics and leaderboards&#10;- **Resource Utilization**: System-wide compute usage&#10;&#10;---&#10;&#10;## System Workflow&#10;&#10;For detailed protocol specifications, see **[PROTOCOL.md](PROTOCOL.md)**.&#10;&#10;### Node Registration Flow&#10;```&#10;1. Node starts up and profiles hardware&#10;2. Generates Ed25519 keypair for identity&#10;3. Sends registration request to coordinator&#10;4. Coordinator validates and stores node metadata&#10;5. Node begins polling for available jobs&#10;```&#10;&#10;### Job Submission Flow&#10;```&#10;1. Researcher submits job via API or dashboard&#10;2. Job enters scheduler queue with resource requirements&#10;3. Scheduler finds 2+ matching available nodes&#10;4. Job descriptor and input data distributed to nodes&#10;5. Nodes execute job in parallel using containers&#10;6. Results collected and hash-verified&#10;7. Successful result stored and credited to contributors&#10;```&#10;&#10;### Validation Flow&#10;```&#10;1. Job completes on multiple nodes&#10;2. Each node signs result with private key&#10;3. Coordinator verifies signatures and compares hashes&#10;4. If hashes match: accept result, update reputation&#10;5. If hashes differ: flag nodes, require additional execution&#10;6. Result archived with cryptographic proof&#10;```&#10;&#10;---&#10;&#10;## Development Phases&#10;&#10;### Phase 1: Core Infrastructure ✅&#10;- [x] Basic node registration and profiling&#10;- [x] Job queue and scheduler&#10;- [x] SQLite database backend&#10;- [x] REST API endpoints&#10;- [x] Hash validation and redundancy&#10;- [x] Live dashboard with network visualization&#10;&#10;### Phase 2: Field Agnostic Generalization ✅&#10;- [x] Generalized job descriptor schema&#10;- [x] Container-based execution sandbox&#10;- [x] Support for diverse workloads (weather, materials, quantum)&#10;- [x] Output validation framework&#10;- [x] Input fetching from S3/IPFS&#10;- [x] Signed result archives&#10;&#10;### Phase 3: Distributed Mesh &amp; Security ✅&#10;- [x] Ed25519 keypair per node&#10;- [x] Fully signed log trails&#10;- [x] P2P communication (libp2p prototype)&#10;- [x] Job replication and fallback&#10;- [x] Zero-knowledge proof research&#10;- [x] Abuse protection and rate limiting&#10;&#10;### Phase 4: Incentivization &amp; Reputation ✅&#10;- [x] Nexa Credits token system&#10;- [x] Job bounties and sponsorship&#10;- [x] Reputation tracking&#10;- [x] Compute marketplace dashboard&#10;&#10;### Phase 5: Public Launch (In Progress)&#10;- [ ] Public onboarding UI&#10;- [ ] Documentation portal&#10;- [ ] Scientific problem examples&#10;- [ ] Community challenge programs&#10;- [ ] Research publication pipeline&#10;&#10;---&#10;&#10;## API Reference&#10;&#10;For complete API documentation with examples, see **[API.md](API.md)**.&#10;&#10;### Quick Reference&#10;&#10;#### Node Registration&#10;```bash&#10;curl -X POST http://localhost:5000/register \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;id&quot;: &quot;node_001&quot;, &quot;tier&quot;: &quot;CPU&quot;, &quot;profile&quot;: {...}}'&#10;```&#10;&#10;#### Job Submission&#10;```bash&#10;curl -X POST http://localhost:5000/submit-job \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;id&quot;: &quot;job_001&quot;, &quot;docker_image&quot;: &quot;tensorflow/tensorflow:latest&quot;}'&#10;```&#10;&#10;#### System Status&#10;```bash&#10;curl http://localhost:5000/status&#10;```&#10;&#10;### Python SDK Example&#10;```python&#10;from nexapod_client import NexaPodClient&#10;&#10;client = NexaPodClient(&quot;http://localhost:5000&quot;)&#10;response = client.submit_job({&#10;    &quot;id&quot;: &quot;my_job&quot;,&#10;    &quot;docker_image&quot;: &quot;python:3.9&quot;,&#10;    &quot;requirements&quot;: {&quot;min_memory_gb&quot;: 4}&#10;})&#10;```&#10;&#10;---&#10;&#10;## Security Model&#10;&#10;For detailed security architecture, see **[ARCHITECTURE.md](ARCHITECTURE.md#security-model)**.&#10;&#10;### Cryptographic Foundation&#10;- **Ed25519 Signatures**: Each node has a unique keypair for identity&#10;- **HMAC Validation**: All job logs are cryptographically signed&#10;- **Hash Verification**: Results validated through SHA-256 comparison&#10;- **Append-Only Ledger**: Immutable audit trail of all operations&#10;&#10;### Trust Mechanisms&#10;1. **Redundant Execution**: Jobs run on multiple nodes&#10;2. **Consensus Validation**: Results accepted only with matching hashes&#10;3. **Reputation System**: Track node reliability over time&#10;4. **Rate Limiting**: Prevent spam and abuse&#10;&#10;### Threat Mitigation&#10;- **Byzantine Nodes**: Detected through result comparison&#10;- **Sybil Attacks**: Mitigated by reputation requirements&#10;- **Data Integrity**: Ensured through cryptographic proofs&#10;- **Availability**: Maintained through mesh redundancy&#10;&#10;---&#10;&#10;## Installation &amp; Setup&#10;&#10;Add detailed K8s and Docker Compose onboarding, plus CLI steps.&#10;&#10;### Quick Start (Docker Compose)&#10;```bash&#10;# Clone and change directory&#10;git clone https://github.com/your-org/nexapod.git&#10;cd nexapod&#10;&#10;# Build and start all services (server, client, Prometheus, Grafana)&#10;docker-compose up --build -d&#10;```&#10;&#10;### Quick Start (Kubernetes)&#10;```bash&#10;# Apply all manifests&#10;kubectl apply -f Infrastruture/k8s/&#10;&#10;# Verify deployments&#10;kubectl rollout status deployment/nexapod-server&#10;kubectl rollout status deployment/nexapod-client&#10;```&#10;&#10;### Register Your Node (Onboarding)&#10;1. Ensure you have a private key (generated on first join).&#10;2. Run the join command on your node machine:&#10;   ```bash&#10;   python Client/nexapod_client.py join&#10;   ```&#10;3. You should see &quot;Node registered with coordinator.&quot; and your `node_id` saved in config.&#10;&#10;### Start Polling for Jobs&#10;```bash&#10;python Client/nexapod_client.py run&#10;```&#10;The client will expose metrics on port 9000 (`/metrics`).&#10;&#10;### Access Dashboard &amp; Monitoring&#10;- **Streamlit Dashboard**: `streamlit run Client/dashboard.py`&#10;- **Prometheus**: http://localhost:9090&#10;- **Grafana**: http://localhost:3000 (add Prometheus data source)&#10;&#10;---&#10;&#10;## Onboarding &amp; First Contribution&#10;&#10;Follow these steps to get started as a compute contributor:&#10;&#10;1. Fork the NexaPod repository and clone your fork:&#10;   ```bash&#10;git clone https://github.com/&lt;your-username&gt;/nexapod.git&#10;cd nexapod&#10;   ```&#10;2. Install prerequisites:&#10;   ```bash&#10;pip install -r requirements.txt&#10;docker-compose --version&#10;kubectl version --client&#10;   ```&#10;3. Build and deploy the system with Docker Compose or Kubernetes (see above).&#10;4. Register your compute node:&#10;   ```bash&#10;python Client/nexapod_client.py join&#10;``` &#10;   - This generates your Ed25519 key if needed and registers you as a node.&#10;5. Start your runner:&#10;   ```bash&#10;python Client/nexapod_client.py run&#10;```&#10;6. Monitor your node:&#10;   - Check client logs and metrics on http://localhost:9000/metrics&#10;   - View overall system status in dashboard and Grafana.&#10;7. Submit your first job (as a researcher):&#10;   ```bash&#10;curl -X POST http://localhost:8000/jobs \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;job_id&quot;:&quot;job_001&quot;,&quot;docker_image&quot;:&quot;python:3.9&quot;,&quot;requirements&quot;:{&quot;ram_gb&quot;:1.0}}'&#10;```&#10;8. Observe job assignment and execution in logs, then result finalization via quorum on the server.&#10;&#10;Congratulations, you are now part of the NexaPod compute mesh!&#10;&#10;---&#10;&#10;## Testing&#10;&#10;### Running Tests&#10;```bash&#10;# Unit tests&#10;pytest tests/unit/&#10;&#10;# Integration tests&#10;pytest tests/integration/&#10;&#10;# Full system test&#10;pytest tests/test_client.py::test_integration&#10;```&#10;&#10;### Test Client&#10;The robust test client (`tests/test_client.py`) validates:&#10;- Node registration and profiling&#10;- Job submission and execution&#10;- Result validation and logging&#10;- Complete system integration&#10;&#10;### Expected Test Flow&#10;1. Start mock coordinator server&#10;2. Register test node (expect 200 OK)&#10;3. Submit test job (expect 200 OK)&#10;4. Poll for status (expect 200 OK)&#10;5. Validate result logging&#10;6. Verify system integrity&#10;&#10;### Continuous Integration&#10;```yaml&#10;# .github/workflows/test.yml&#10;name: Test Suite&#10;on: [push, pull_request]&#10;jobs:&#10;  test:&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - uses: actions/checkout@v2&#10;      - name: Set up Python&#10;        uses: actions/setup-python@v2&#10;        with:&#10;          python-version: 3.9&#10;      - name: Install dependencies&#10;        run: pip install -r requirements.txt&#10;      - name: Run tests&#10;        run: pytest&#10;```&#10;&#10;---&#10;&#10;## Contributing&#10;&#10;### Development Workflow&#10;1. Fork repository&#10;2. Create feature branch (`git checkout -b feature/amazing-feature`)&#10;3. Implement changes with tests&#10;4. Submit pull request&#10;5. Code review and merge&#10;&#10;### Code Standards&#10;- **Python 3.8+** with type hints&#10;- **Black** code formatting&#10;- **Pytest** for testing&#10;- **Docstrings** for all public APIs&#10;- **Type annotations** for function signatures&#10;&#10;### Documentation Standards&#10;- Update relevant documentation files&#10;- Include examples in docstrings&#10;- Cross-reference related documentation&#10;- Test all code examples&#10;&#10;### Pull Request Template&#10;```markdown&#10;## Description&#10;Brief description of changes&#10;&#10;## Testing&#10;- [ ] Unit tests pass&#10;- [ ] Integration tests pass&#10;- [ ] Manual testing completed&#10;&#10;## Documentation&#10;- [ ] Updated relevant docs&#10;- [ ] Added examples&#10;- [ ] Updated API reference&#10;&#10;## Breaking Changes&#10;- [ ] None&#10;- [ ] Migration guide provided&#10;```&#10;&#10;---&#10;&#10;## Related Documentation&#10;&#10;- **[ARCHITECTURE.md](ARCHITECTURE.md)**: Detailed system architecture and component diagrams&#10;- **[PROTOCOL.md](PROTOCOL.md)**: Message formats and communication protocols&#10;- **[API.md](API.md)**: Complete REST API reference with examples&#10;- **[README.md](../README.md)**: Project overview and quick start guide&#10;&#10;---&#10;&#10;## Troubleshooting&#10;&#10;### Common Issues&#10;&#10;**Docker Connection Error**:&#10;```bash&#10;# Check Docker daemon&#10;sudo systemctl status docker&#10;&#10;# Test Docker access&#10;docker ps&#10;```&#10;&#10;**Node Registration Fails**:&#10;```python&#10;# Check network connectivity&#10;import requests&#10;response = requests.get(&quot;http://localhost:5000/status&quot;)&#10;print(response.status_code)&#10;```&#10;&#10;**Job Execution Timeout**:&#10;- Increase timeout in job descriptor&#10;- Check node resource availability&#10;- Monitor Docker container logs&#10;&#10;### Getting Help&#10;&#10;1. Check the [documentation](#related-documentation)&#10;2. Search existing [GitHub issues](https://github.com/your-org/nexapod/issues)&#10;3. Join the [Discord community](https://discord.gg/nexapod)&#10;4. Email support: support@nexapod.org&#10;&#10;---&#10;&#10;*This documentation represents the current state and future roadmap of NexaPod. For the latest updates, see the project repository.*&#10;&#10;**Last Updated**: 2024-01-01  &#10;**Version**: 1.0.0  &#10;**Authors**: NexaPod Development Team&#10;" />
              <option name="updatedContent" value="# NexaPod: Comprehensive Technical Documentation&#10;&#10;## Table of Contents&#10;1. [Project Overview](#project-overview)&#10;2. [Architecture](#architecture)&#10;3. [Core Components](#core-components)&#10;4. [System Workflow](#system-workflow)&#10;5. [Development Phases](#development-phases)&#10;6. [API Reference](#api-reference)&#10;7. [Security Model](#security-model)&#10;8. [Installation &amp; Setup](#installation--setup)&#10;9. [Testing](#testing)&#10;10. [Contributing](#contributing)&#10;&#10;---&#10;&#10;## Project Overview&#10;&#10;**NexaPod** is a decentralized, field-agnostic compute mesh designed to coordinate heterogeneous computational resources for large-scale scientific problems. The system enables researchers to submit computational jobs that are distributed across a network of volunteer nodes, ranging from consumer GPUs to high-performance computing clusters.&#10;&#10;### Mission Statement&#10;*Democratize access to computational resources for scientific research by creating a permissionless, trustless, and incentivized distributed computing platform.*&#10;&#10;### Key Features&#10;- **Field Agnostic**: Supports any scientific workload (weather modeling, quantum simulation, materials science, etc.)&#10;- **Decentralized**: No single point of failure, peer-to-peer communication&#10;- **Trustless**: Cryptographic verification and redundant execution&#10;- **Incentivized**: Token-based reward system for compute contributors&#10;- **Scalable**: Automatic node profiling and intelligent job scheduling&#10;&#10;---&#10;&#10;## Architecture&#10;&#10;For detailed architecture documentation, see **[ARCHITECTURE.md](ARCHITECTURE.md)**.&#10;&#10;### High-Level System Overview&#10;&#10;```&#10;┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐&#10;│   Job Submitter │    │   Coordinator   │    │  Compute Nodes  │&#10;│                 │    │                 │    │                 │&#10;│ • Scientists    │◄──►│ • Job Queue     │◄──►│ • CPU Nodes     │&#10;│ • Researchers   │    │ • Scheduler     │    │ • GPU Nodes     │&#10;│ • Institutions  │    │ • Validator     │    │ • HPC Clusters  │&#10;└─────────────────┘    └─────────────────┘    └─────────────────┘&#10;         │                       │                       │&#10;         │                       │                       │&#10;         └───────────────────────┼───────────────────────┘&#10;                                 │&#10;                    ┌─────────────────┐&#10;                    │   Dashboard     │&#10;                    │                 │&#10;                    │ • Live Status   │&#10;                    │ • Contributor   │&#10;                    │   Wall          │&#10;                    │ • Network Graph │&#10;                    └─────────────────┘&#10;```&#10;&#10;### Technology Stack Summary&#10;&#10;| Layer             | Component        | Technology          |&#10;|-------------------|------------------|---------------------|&#10;| **Frontend**      | Dashboard        | Streamlit           |&#10;| **Frontend**      | Contributor Wall | HTML/CSS/JS         |&#10;| **API**           | REST Endpoints   | Flask               |&#10;| **Core**          | Job Scheduler    | Python + Threading  |&#10;| **Data**          | Database         | SQLite → PostgreSQL |&#10;| **Security**      | Cryptography     | Ed25519 + SHA-256   |&#10;| **Execution**     | Containers       | Docker              |&#10;| **Communication** | Mesh Network     | libp2p (planned)    |&#10;&#10;---&#10;&#10;## Core Components&#10;&#10;### 1. Tier System (`nexapod/tier.py`)&#10;Classifies compute nodes based on their capabilities:&#10;&#10;```python&#10;class Tier(Enum):&#10;    CPU = 'CPU'              # Basic CPU nodes&#10;    CONSUMER_GPU = 'Consumer GPU'  # Gaming/workstation GPUs&#10;    HPC = 'HPC'              # High-performance clusters&#10;```&#10;&#10;### 2. Node Management (`nexapod/node.py`)&#10;Handles node registration and profiling:&#10;- **Automatic Profiling**: Detects OS, CPU, memory, and GPU capabilities&#10;- **Registration**: Stores node metadata in the database&#10;- **Status Tracking**: Monitors node availability and health&#10;&#10;### 3. Job Scheduler (`nexapod/scheduler.py`)&#10;Core scheduling engine with the following features:&#10;- **Job Queue**: FIFO queue for incoming computational tasks&#10;- **Node Matching**: Intelligent assignment based on job requirements&#10;- **Redundancy**: Executes jobs on multiple nodes for verification&#10;- **Conflict Prevention**: Tracks busy nodes to prevent resource conflicts&#10;&#10;#### Scheduling Algorithm&#10;```&#10;1. Job arrives in queue&#10;2. Find 2+ available nodes matching job requirements&#10;3. Verify nodes are trusted and not busy&#10;4. Execute job on selected nodes in parallel&#10;5. Compare results via hash validation&#10;6. Accept result if hashes match, else flag discrepancy&#10;7. Mark nodes as available and log result&#10;```&#10;&#10;### 4. Database Layer (`nexapod/database.py`)&#10;SQLite-based persistence with three main tables:&#10;- **Nodes**: Store node profiles, tiers, and status&#10;- **Jobs**: Track job metadata and results&#10;- **Logs**: Append-only audit trail for all operations&#10;&#10;### 5. Validation System (`nexapod/validator.py`)&#10;Ensures job integrity through:&#10;- **HMAC Signatures**: Cryptographic verification of job logs&#10;- **Hash Comparison**: Validates identical results from redundant execution&#10;- **Result Verification**: User-defined checkers for output validation&#10;&#10;### 6. Container Runtime (`nexapod/runner.py`)&#10;Secure job execution environment:&#10;- **Docker Integration**: Runs jobs in isolated containers&#10;- **Volume Mounting**: Manages input/output file systems&#10;- **Resource Limits**: Enforces CPU/memory constraints&#10;- **Result Collection**: Captures job outputs and logs&#10;&#10;### 7. API Layer (`nexapod/api.py`)&#10;RESTful endpoints for system interaction. For complete API documentation, see **[API.md](API.md)**.&#10;&#10;| Endpoint             | Method | Purpose                   |&#10;|----------------------|--------|---------------------------|&#10;| `/register`          | POST   | Register new compute node |&#10;| `/submit-job`        | POST   | Submit computational job  |&#10;| `/status`            | GET    | Query system status       |&#10;| `/jobs/{id}`         | GET    | Get job details           |&#10;| `/nodes`             | GET    | List registered nodes     |&#10;| `/credits/{node_id}` | GET    | Get credit balance        |&#10;&#10;### 8. Dashboard (`dashboard.py`)&#10;Real-time system monitoring:&#10;- **Network Visualization**: Interactive graph of connected nodes&#10;- **Job Status**: Live tracking of submitted jobs&#10;- **Contributor Metrics**: Performance statistics and leaderboards&#10;- **Resource Utilization**: System-wide compute usage&#10;&#10;---&#10;&#10;## System Workflow&#10;&#10;For detailed protocol specifications, see **[PROTOCOL.md](PROTOCOL.md)**.&#10;&#10;### Node Registration Flow&#10;```&#10;1. Node starts up and profiles hardware&#10;2. Generates Ed25519 keypair for identity&#10;3. Sends registration request to coordinator&#10;4. Coordinator validates and stores node metadata&#10;5. Node begins polling for available jobs&#10;```&#10;&#10;### Job Submission Flow&#10;```&#10;1. Researcher submits job via API or dashboard&#10;2. Job enters scheduler queue with resource requirements&#10;3. Scheduler finds 2+ matching available nodes&#10;4. Job descriptor and input data distributed to nodes&#10;5. Nodes execute job in parallel using containers&#10;6. Results collected and hash-verified&#10;7. Successful result stored and credited to contributors&#10;```&#10;&#10;### Validation Flow&#10;```&#10;1. Job completes on multiple nodes&#10;2. Each node signs result with private key&#10;3. Coordinator verifies signatures and compares hashes&#10;4. If hashes match: accept result, update reputation&#10;5. If hashes differ: flag nodes, require additional execution&#10;6. Result archived with cryptographic proof&#10;```&#10;&#10;---&#10;&#10;## Development Phases&#10;&#10;### Phase 1: Core Infrastructure ✅&#10;- [x] Basic node registration and profiling&#10;- [x] Job queue and scheduler&#10;- [x] SQLite database backend&#10;- [x] REST API endpoints&#10;- [x] Hash validation and redundancy&#10;- [x] Live dashboard with network visualization&#10;&#10;### Phase 2: Field Agnostic Generalization ✅&#10;- [x] Generalized job descriptor schema&#10;- [x] Container-based execution sandbox&#10;- [x] Support for diverse workloads (weather, materials, quantum)&#10;- [x] Output validation framework&#10;- [x] Input fetching from S3/IPFS&#10;- [x] Signed result archives&#10;&#10;### Phase 3: Distributed Mesh &amp; Security ✅&#10;- [x] Ed25519 keypair per node&#10;- [x] Fully signed log trails&#10;- [x] P2P communication (libp2p prototype)&#10;- [x] Job replication and fallback&#10;- [x] Zero-knowledge proof research&#10;- [x] Abuse protection and rate limiting&#10;&#10;### Phase 4: Incentivization &amp; Reputation ✅&#10;- [x] Nexa Credits token system&#10;- [x] Job bounties and sponsorship&#10;- [x] Reputation tracking&#10;- [x] Compute marketplace dashboard&#10;&#10;### Phase 5: Public Launch (In Progress)&#10;- [ ] Public onboarding UI&#10;- [ ] Documentation portal&#10;- [ ] Scientific problem examples&#10;- [ ] Community challenge programs&#10;- [ ] Research publication pipeline&#10;&#10;---&#10;&#10;## API Reference&#10;&#10;For complete API documentation with examples, see **[API.md](API.md)**.&#10;&#10;### Quick Reference&#10;&#10;#### Node Registration&#10;```bash&#10;curl -X POST http://localhost:5000/register \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;id&quot;: &quot;node_001&quot;, &quot;tier&quot;: &quot;CPU&quot;, &quot;profile&quot;: {...}}'&#10;```&#10;&#10;#### Job Submission&#10;```bash&#10;curl -X POST http://localhost:5000/submit-job \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&quot;id&quot;: &quot;job_001&quot;, &quot;docker_image&quot;: &quot;tensorflow/tensorflow:latest&quot;}'&#10;```&#10;&#10;#### System Status&#10;```bash&#10;curl http://localhost:5000/status&#10;```&#10;&#10;### Python SDK Example&#10;```python&#10;from nexapod_client import NexaPodClient&#10;&#10;client = NexaPodClient(&quot;http://localhost:5000&quot;)&#10;response = client.submit_job({&#10;    &quot;id&quot;: &quot;my_job&quot;,&#10;    &quot;docker_image&quot;: &quot;python:3.9&quot;,&#10;    &quot;requirements&quot;: {&quot;min_memory_gb&quot;: 4}&#10;})&#10;```&#10;&#10;---&#10;&#10;## Security Model&#10;&#10;For detailed security architecture, see **[ARCHITECTURE.md](ARCHITECTURE.md#security-model)**.&#10;&#10;### Cryptographic Foundation&#10;- **Ed25519 Signatures**: Each node has a unique keypair for identity&#10;- **HMAC Validation**: All job logs are cryptographically signed&#10;- **Hash Verification**: Results validated through SHA-256 comparison&#10;- **Append-Only Ledger**: Immutable audit trail of all operations&#10;&#10;---&#10;&#10;## Installation &amp; Setup&#10;&#10;For the primary guide on joining the network, see **[ONBOARDING.md](ONBOARDING.md)**.&#10;&#10;### Quick Start (Docker Compose)&#10;```bash&#10;# Clone and change directory&#10;git clone https://github.com/your-org/nexapod.git&#10;cd nexapod&#10;&#10;# Build and start all services (server, client, Prometheus, Grafana)&#10;docker-compose up --build -d&#10;```&#10;&#10;### Quick Start (Kubernetes)&#10;```bash&#10;# Apply all manifests&#10;kubectl apply -f Infrastruture/k8s/&#10;&#10;# Verify deployments&#10;kubectl rollout status deployment/nexapod-server&#10;kubectl rollout status deployment/nexapod-client&#10;```&#10;&#10;### Register Your Node (Onboarding)&#10;The node registration process is handled automatically by the `nexapod` client. Please follow the steps in **[ONBOARDING.md](ONBOARDING.md)** to configure and run the client.&#10;&#10;### Start Polling for Jobs&#10;```bash&#10;# This command starts the client, which polls for jobs.&#10;nexapod&#10;```&#10;The client will expose metrics on port 9000 (`/metrics`).&#10;&#10;### Access Dashboard &amp; Monitoring&#10;- **Streamlit Dashboard**: `streamlit run Client/dashboard.py`&#10;- **Prometheus**: http://localhost:9090&#10;- **Grafana**: http://localhost:3000 (add Prometheus data source)&#10;&#10;---&#10;&#10;## Onboarding &amp; First Contribution&#10;&#10;Follow the steps in **[ONBOARDING.md](ONBOARDING.md)** to get started as a compute contributor. The process uses the `nexapod` CLI tool to simplify joining the network.&#10;&#10;Congratulations, you are now part of the NexaPod compute mesh!&#10;&#10;---&#10;&#10;## Testing&#10;&#10;### Running Tests&#10;```bash&#10;# Unit tests&#10;pytest tests/unit/&#10;&#10;# Integration tests&#10;pytest tests/integration/&#10;&#10;# Full system test&#10;pytest tests/test_client.py::test_integration&#10;```&#10;&#10;### Test Client&#10;The robust test client (`tests/test_client.py`) validates:&#10;- Node registration and profiling&#10;- Job submission and execution&#10;- Result validation and logging&#10;- Complete system integration&#10;&#10;### Expected Test Flow&#10;1. Start mock coordinator server&#10;2. Register test node (expect 200 OK)&#10;3. Submit test job (expect 200 OK)&#10;4. Poll for status (expect 200 OK)&#10;5. Validate result logging&#10;6. Verify system integrity&#10;&#10;### Continuous Integration&#10;```yaml&#10;# .github/workflows/test.yml&#10;name: Test Suite&#10;on: [push, pull_request]&#10;jobs:&#10;  test:&#10;    runs-on: ubuntu-latest&#10;    steps:&#10;      - uses: actions/checkout@v2&#10;      - name: Set up Python&#10;        uses: actions/setup-python@v2&#10;        with:&#10;          python-version: 3.9&#10;      - name: Install dependencies&#10;        run: pip install -r requirements.txt&#10;      - name: Run tests&#10;        run: pytest&#10;```&#10;&#10;---&#10;&#10;## Contributing&#10;&#10;### Development Workflow&#10;1. Fork repository&#10;2. Create feature branch (`git checkout -b feature/amazing-feature`)&#10;3. Implement changes with tests&#10;4. Submit pull request&#10;5. Code review and merge&#10;&#10;### Code Standards&#10;- **Python 3.8+** with type hints&#10;- **Black** code formatting&#10;- **Pytest** for testing&#10;- **Docstrings** for all public APIs&#10;- **Type annotations** for function signatures&#10;&#10;### Documentation Standards&#10;- Update relevant documentation files&#10;- Include examples in docstrings&#10;- Cross-reference related documentation&#10;- Test all code examples&#10;&#10;### Pull Request Template&#10;```markdown&#10;## Description&#10;Brief description of changes&#10;&#10;## Testing&#10;- [ ] Unit tests pass&#10;- [ ] Integration tests pass&#10;- [ ] Manual testing completed&#10;&#10;## Documentation&#10;- [ ] Updated relevant docs&#10;- [ ] Added examples&#10;- [ ] Updated API reference&#10;&#10;## Breaking Changes&#10;- [ ] None&#10;- [ ] Migration guide provided&#10;```&#10;&#10;---&#10;&#10;## Related Documentation&#10;&#10;- **[ARCHITECTURE.md](ARCHITECTURE.md)**: Detailed system architecture and component diagrams&#10;- **[PROTOCOL.md](PROTOCOL.md)**: Message formats and communication protocols&#10;- **[API.md](API.md)**: Complete REST API reference with examples&#10;- **[README.md](../README.md)**: Project overview and quick start guide&#10;&#10;---&#10;&#10;## Troubleshooting&#10;&#10;### Common Issues&#10;&#10;**Docker Connection Error**:&#10;```bash&#10;# Check Docker daemon&#10;sudo systemctl status docker&#10;&#10;# Test Docker access&#10;docker ps&#10;```&#10;&#10;**Node Registration Fails**:&#10;```python&#10;# Check network connectivity&#10;import requests&#10;response = requests.get(&quot;http://localhost:5000/status&quot;)&#10;print(response.status_code)&#10;```&#10;&#10;**Job Execution Timeout**:&#10;- Increase timeout in job descriptor&#10;- Check node resource availability&#10;- Monitor Docker container logs&#10;&#10;### Getting Help&#10;&#10;1. Check the [documentation](#related-documentation)&#10;2. Search existing [GitHub issues](https://github.com/your-org/nexapod/issues)&#10;3. Join the [Discord community](https://discord.gg/nexapod)&#10;4. Email support: support@nexapod.org&#10;&#10;---&#10;&#10;*This documentation represents the current state and future roadmap of NexaPod. For the latest updates, see the project repository.*&#10;&#10;**Last Updated**: 2024-01-01  &#10;**Version**: 1.0.0  &#10;**Authors**: NexaPod Development Team" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Docs/ONBOARDING.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Docs/ONBOARDING.md" />
              <option name="originalContent" value="# Onboarding &amp; First Contribution&#10;&#10;Welcome to NexaPod! This guide walks you through the steps to join the compute mesh as a contributor and submit your first job.&#10;&#10;## Prerequisites&#10;&#10;- Python 3.8+ and pip&#10;- Docker &amp; Docker Compose (for local dev)&#10;- kubectl and access to a Kubernetes cluster (for production)&#10;- Git and a GitHub account&#10;&#10;## 1. Fork &amp; Clone Repository&#10;&#10;```bash&#10;# Fork https://github.com/your-org/nexapod to your GitHub account&#10;git clone https://github.com/&lt;your-username&gt;/nexapod.git&#10;cd nexapod&#10;```&#10;&#10;## 2. Install Dependencies&#10;&#10;```bash&#10;pip install -r requirements.txt&#10;docker-compose --version&#10;kubectl version --client&#10;```&#10;&#10;## 3. Build &amp; Deploy NexaPod&#10;&#10;### a) Docker Compose (Local Dev)&#10;```bash&#10;# Build and start server, client, Prometheus, Grafana&#10;docker-compose up --build -d&#10;```  &#10;Services:&#10;- **nexapod-server** – API, scheduler, DB  &#10;- **nexapod-client** – runner polling for jobs  &#10;- **prometheus** – metrics collection  &#10;- **grafana**  – dashboards&#10;&#10;### b) Kubernetes (Production/Staging)&#10;```bash&#10;# Apply core manifests&#10;kubectl apply -f Infrastruture/k8s/&#10;# Apply Prometheus scrape config (namespace: monitoring)&#10;kubectl apply -f Infrastruture/k8s/prometheus-scrape-nexapod.yaml&#10;# Verify deployments&#10;kubectl rollout status deployment/nexapod-server&#10;kubectl rollout status deployment/nexapod-client&#10;```&#10;&#10;## 4. Register Your Compute Node&#10;&#10;On the node machine where you want to run jobs:&#10;&#10;```bash&#10;python Client/nexapod_client.py join&#10;```&#10;- Generates (or loads) your Ed25519 key at `~/.nexapod/client_ed25519.key`  &#10;- Sign and register your hardware profile with the coordinator  &#10;- Updates `Client/config.yaml` with your `node_id`&#10;&#10;## 5. Start the Runner&#10;&#10;```bash&#10;python Client/nexapod_client.py run&#10;```&#10;- Polls the server for jobs  &#10;- Executes **Docker** containers for each job  &#10;- Signs, logs, and submit results back to the coordinator&#10;&#10;## 6. Monitor Your Node&#10;&#10;- **Client metrics**: http://&lt;client-host&gt;:9000/metrics  &#10;- **Server metrics**: http://&lt;server-host&gt;:8000/metrics  &#10;- **Streamlit dashboard**: `streamlit run Client/dashboard.py`  &#10;- **Prometheus**: http://localhost:9090  &#10;- **Grafana**: http://localhost:3000&#10;&#10;## 7. Submit Your First Job&#10;&#10;As a researcher, submit a sample job:&#10;&#10;```bash&#10;curl -X POST https://&lt;server-host&gt;:8000/jobs \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&#10;    &quot;job_id&quot;: &quot;job_001&quot;,&#10;    &quot;docker_image&quot;: &quot;python:3.9&quot;,&#10;    &quot;requirements&quot;: {&quot;ram_gb&quot;: 1.0},&#10;    &quot;input_files&quot;: []&#10;}'&#10;```&#10;&#10;- Watch your node pick up **job_001**  &#10;- See execution logs in the client console  &#10;- Check finalization once quorum of nodes agrees on the result&#10;&#10;## 8. Visibility &amp; Next Steps&#10;&#10;- View live job graph, node health, and contributor leaderboard on the dashboard  &#10;- Explore the full API in **API.md**  &#10;- Join community discussions and file issues on GitHub&#10;&#10;Congratulations, you’re now a NexaPod contributor! &#10;" />
              <option name="updatedContent" value="# NEXAPod Alpha: Contributor Onboarding&#10;&#10;Welcome! This guide explains how to join the NEXAPod compute mesh by running a client &quot;pod&quot; on your local machine.&#10;&#10;## Prerequisites&#10;&#10;- **Docker:** You must have Docker installed and running.&#10;- **A shell environment:** (e.g., Bash, Zsh, PowerShell with Git Bash).&#10;&#10;## Step 1: Install the `nexapod` CLI&#10;&#10;The `nexapod` script is the simplest way to interact with the network.&#10;&#10;**For Linux/macOS:**&#10;```bash&#10;# Download the script (replace with the correct raw URL if needed)&#10;curl -o nexapod https://raw.githubusercontent.com/kunya66/NexaPod/main/nexapod&#10;# Make it executable&#10;chmod +x nexapod&#10;# Move it to a location in your PATH&#10;sudo mv nexapod /usr/local/bin/&#10;```&#10;&#10;**For Windows:**&#10;1.  Download the `nexapod` script.&#10;2.  Place it in a directory that is included in your system's `PATH`.&#10;3.  Ensure you run the commands from a shell that supports bash scripts (like Git Bash).&#10;&#10;## Step 2: Configure Your Pod&#10;&#10;The pod needs to know which coordinator server to connect to.&#10;&#10;1.  Create a directory in your home folder named `.nexapod`:&#10;    ```bash&#10;    mkdir -p ~/.nexapod&#10;    ```&#10;2.  Inside that directory, create a file named `config.yaml`:&#10;    ```yaml&#10;    # File: ~/.nexapod/config.yaml&#10;    coordinator_url: &quot;http://YOUR_SERVER_IP:8000&quot; # &lt;-- Replace with the public IP of the coordinator&#10;    poll_interval: 10 # Optional: seconds between polling for new jobs&#10;    ```&#10;This directory is securely mounted into the container as read-only.&#10;&#10;## Step 3: Pull the Client Image&#10;&#10;Download the official, read-only client image from the GitHub Container Registry:&#10;&#10;```bash&#10;nexapod --pull&#10;```&#10;&#10;## Step 4: Launch Your Pod&#10;&#10;With configuration complete, start your pod to join the network:&#10;&#10;```bash&#10;nexapod&#10;```&#10;&#10;On startup, your pod will automatically:&#10;1.  Log basic, non-personal system info (OS, CPU, GPU).&#10;2.  Generate a unique identity hash from its own code.&#10;3.  Register with the coordinator.&#10;4.  Begin polling for compute jobs.&#10;&#10;The pod runs in a **sandboxed, read-only environment**. To stop it, press `Ctrl+C`.&#10;&#10;## Step 5: Monitor the Network&#10;&#10;- **Dashboard**: The primary way to see your node and others is through the Streamlit dashboard, typically hosted by the coordinator.&#10;- **Local Logs**: Your terminal will show logs from your running pod.&#10;&#10;Congratulations, you are now a contributor to the NEXAPod compute mesh! " />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastructure/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastructure/__init__.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Infrastructure package for NEXAPod.&#10;&quot;&quot;&quot;&#10;from .api import app, scheduler, db&#10;&#10;__all__ = [&#10;    &quot;app&quot;,&#10;    &quot;scheduler&quot;,&#10;    &quot;db&quot;&#10;]&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Infrastructure package for NEXAPod.&#10;&quot;&quot;&quot;&#10;from .api import app, scheduler, db&#10;from .database import Database&#10;from .Descriptor import RateLimitDescriptor&#10;from .node import Node&#10;from .scheduler import Scheduler&#10;from .validator import generate_signature, validate_log&#10;&#10;__all__ = [&#10;    &quot;app&quot;,&#10;    &quot;db&quot;,&#10;    &quot;Database&quot;,&#10;    &quot;generate_signature&quot;,&#10;    &quot;Node&quot;,&#10;    &quot;RateLimitDescriptor&quot;,&#10;    &quot;scheduler&quot;,&#10;    &quot;Scheduler&quot;,&#10;    &quot;validate_log&quot;,&#10;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastructure/api.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastructure/api.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;from flask import Flask, request, jsonify&#10;from .node import Node&#10;from .tier import Tier&#10;from .scheduler import Scheduler&#10;from .database import Database&#10;&#10;app = Flask(__name__)&#10;scheduler = Scheduler()&#10;db = Database()&#10;&#10;&#10;@app.route('/register', methods=['POST'])&#10;def register():&#10;    &quot;&quot;&quot;Register a new node with its profile and tier.&quot;&quot;&quot;&#10;    data = request.get_json()&#10;    node = Node(data.get('id'), Tier(data.get('tier')))&#10;    db.store_node({&#10;        &quot;id&quot;: node.id,&#10;        &quot;tier&quot;: node.tier.value,&#10;        &quot;profile&quot;: node.profile&#10;    })&#10;    return jsonify({&quot;status&quot;: &quot;registered&quot;, &quot;node&quot;: node.id})&#10;&#10;&#10;@app.route('/submit-job', methods=['POST'])&#10;def submit_job():&#10;    &quot;&quot;&quot;Accept a job submission and enqueue it for scheduling.&quot;&quot;&quot;&#10;    job = request.get_json()&#10;    scheduler.submit_job(job)&#10;    return jsonify({&quot;status&quot;: &quot;job submitted&quot;, &quot;job_id&quot;: job.get('id')})&#10;&#10;&#10;@app.route('/status', methods=['GET'])&#10;def status():&#10;    &quot;&quot;&quot;Return current nodes and jobs stored in the database.&quot;&quot;&quot;&#10;    return jsonify({&#10;        &quot;nodes&quot;: db.get_nodes(),&#10;        &quot;jobs&quot;: db.get_jobs()&#10;    })&#10;&#10;&#10;if __name__ == '__main__':&#10;    host = os.getenv('HOST', '0.0.0.0')&#10;    port = int(os.getenv('PORT', 8000))&#10;    app.run(host=host, port=port)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import json&#10;from flask import Flask, request, jsonify&#10;from .scheduler import Scheduler&#10;from .database import Database&#10;&#10;app = Flask(__name__)&#10;scheduler = Scheduler()&#10;db = Database()&#10;&#10;&#10;@app.route('/register', methods=['POST'])&#10;def register():&#10;    &quot;&quot;&quot;Register a new node with its profile.&quot;&quot;&quot;&#10;    data = request.get_json()&#10;    if not data or 'node_id' not in data or 'profile' not in data:&#10;        return jsonify({&quot;error&quot;: &quot;Invalid registration data&quot;}), 400&#10;    &#10;    node_id = data['node_id']&#10;    profile = data['profile']&#10;    &#10;    db.store_node(node_id, json.dumps(profile))&#10;    return jsonify({&quot;status&quot;: &quot;registered&quot;, &quot;node_id&quot;: node_id})&#10;&#10;&#10;@app.route('/job', methods=['GET'])&#10;def get_job():&#10;    &quot;&quot;&quot;A node requests a job from the scheduler.&quot;&quot;&quot;&#10;    node_id = request.headers.get('X-Node-ID')&#10;    if not node_id:&#10;        return jsonify({&quot;error&quot;: &quot;X-Node-ID header is required&quot;}), 400&#10;&#10;    job = scheduler.get_job(node_id)&#10;    if job:&#10;        db.store_job(job)&#10;        return jsonify(job)&#10;    &#10;    return jsonify({&quot;status&quot;: &quot;no_job_available&quot;})&#10;&#10;&#10;@app.route('/result', methods=['POST'])&#10;def submit_result():&#10;    &quot;&quot;&quot;A node submits the result of a completed job.&quot;&quot;&quot;&#10;    data = request.get_json()&#10;    if not data or 'job_id' not in data or 'result' not in data:&#10;        return jsonify({&quot;error&quot;: &quot;Invalid result submission&quot;}), 400&#10;    &#10;    db.update_job_result(data['job_id'], json.dumps(data['result']))&#10;    return jsonify({&quot;status&quot;: &quot;result_received&quot;, &quot;job_id&quot;: data['job_id']})&#10;&#10;&#10;@app.route('/nodes', methods=['GET'])&#10;def get_nodes():&#10;    &quot;&quot;&quot;Return a list of all registered nodes.&quot;&quot;&quot;&#10;    nodes = db.get_nodes()&#10;    return jsonify(nodes)&#10;&#10;&#10;@app.route('/jobs', methods=['GET'])&#10;def get_jobs():&#10;    &quot;&quot;&quot;Return a list of all jobs.&quot;&quot;&quot;&#10;    jobs = db.get_jobs()&#10;    return jsonify(jobs)&#10;&#10;&#10;if __name__ == '__main__':&#10;    host = os.getenv('HOST', '0.0.0.0')&#10;    port = int(os.getenv('PORT', 8000))&#10;    app.run(host=host, port=port)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastructure/database.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastructure/database.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Database access layer for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import sqlite3&#10;&#10;&#10;class Database:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, and logs.&quot;&quot;&quot;&#10;    def __init__(self, path: str = &quot;nexapod.db&quot;):&#10;        self.conn = sqlite3.connect(path, check_same_thread=False)&#10;        self.create_tables()&#10;&#10;    def create_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, and logs if they do not exist.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS nodes (&#10;               id TEXT PRIMARY KEY,&#10;               tier TEXT,&#10;               profile TEXT)&quot;&quot;&quot;&#10;        )&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS jobs (&#10;               id TEXT PRIMARY KEY,&#10;               data TEXT,&#10;               result TEXT)&quot;&quot;&quot;&#10;        )&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS logs (&#10;               id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;               job_id TEXT,&#10;               log TEXT)&quot;&quot;&quot;&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_node(self, node: dict):&#10;        &quot;&quot;&quot;Insert or update a node record.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO nodes VALUES (?,?,?)&quot;,&#10;            (node[&quot;id&quot;], node[&quot;tier&quot;], str(node[&quot;profile&quot;]))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_job(self, job: dict, result: dict):&#10;        &quot;&quot;&quot;Insert or update a job record with its result.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO jobs VALUES (?,?,?)&quot;,&#10;            (job[&quot;id&quot;], str(job), str(result))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_nodes(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored nodes.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT * FROM nodes&quot;)&#10;        return cursor.fetchall()&#10;&#10;    def get_jobs(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored jobs.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT * FROM jobs&quot;)&#10;        return cursor.fetchall()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Database access layer for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import sqlite3&#10;import json&#10;&#10;&#10;class Database:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, and logs.&quot;&quot;&quot;&#10;    def __init__(self, path: str = &quot;nexapod.db&quot;):&#10;        self.conn = sqlite3.connect(path, check_same_thread=False)&#10;        self.create_tables()&#10;&#10;    def create_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, and logs if they do not exist.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS nodes (&#10;               id TEXT PRIMARY KEY,&#10;               profile TEXT)&quot;&quot;&quot;&#10;        )&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS jobs (&#10;               job_id TEXT PRIMARY KEY,&#10;               status TEXT,&#10;               assigned_to TEXT,&#10;               data TEXT,&#10;               result TEXT)&quot;&quot;&quot;&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_node(self, node_id: str, profile: str):&#10;        &quot;&quot;&quot;Insert or update a node record.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO nodes (id, profile) VALUES (?,?)&quot;,&#10;            (node_id, profile)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_job(self, job: dict):&#10;        &quot;&quot;&quot;Insert or update a job record.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO jobs (job_id, status, assigned_to, data, result) VALUES (?,?,?,?,?)&quot;,&#10;            (job[&quot;job_id&quot;], job[&quot;status&quot;], job[&quot;assigned_to&quot;], json.dumps(job.get(&quot;data&quot;)), json.dumps(job.get(&quot;result&quot;)))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def update_job_result(self, job_id: str, result: str):&#10;        &quot;&quot;&quot;Update a job with its result and set status to completed.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;UPDATE jobs SET result = ?, status = 'completed' WHERE job_id = ?&quot;,&#10;            (result, job_id)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_nodes(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored nodes.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT id, profile FROM nodes&quot;)&#10;        # Return a list of dicts for easier JSON serialization&#10;        return [{&quot;id&quot;: row[0], &quot;profile&quot;: json.loads(row[1])} for row in cursor.fetchall()]&#10;&#10;    def get_jobs(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored jobs.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT job_id, status, assigned_to, data, result FROM jobs&quot;)&#10;        # Return a list of dicts&#10;        return [&#10;            {&#10;                &quot;job_id&quot;: row[0],&#10;                &quot;status&quot;: row[1],&#10;                &quot;assigned_to&quot;: row[2],&#10;                &quot;data&quot;: json.loads(row[3]) if row[3] else None,&#10;                &quot;result&quot;: json.loads(row[4]) if row[4] else None,&#10;            }&#10;            for row in cursor.fetchall()&#10;        ]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastructure/k8s/client.yaml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastructure/k8s/client.yaml" />
              <option name="originalContent" value="apiVersion: apps/v1&#10;kind: Deployment&#10;metadata:&#10;  name: nexapod-client&#10;spec:&#10;  replicas: 1&#10;  selector:&#10;    matchLabels:&#10;      app: nexapod-client&#10;  template:&#10;    metadata:&#10;      labels:&#10;        app: nexapod-client&#10;    spec:&#10;      containers:&#10;      - name: client&#10;        image: nexapod-client:latest&#10;        env:&#10;        - name: COORDINATOR_URL&#10;          value: &quot;http://nexapod-server:8000&quot;&#10;        volumeMounts:&#10;        - name: config&#10;          mountPath: /app/config.yaml&#10;          subPath: config.yaml&#10;        resources:&#10;          requests:&#10;            cpu: &quot;50m&quot;&#10;            memory: &quot;64Mi&quot;&#10;          limits:&#10;            cpu: &quot;200m&quot;&#10;            memory: &quot;256Mi&quot;&#10;        livenessProbe:&#10;          exec:&#10;            command: [&quot;pgrep&quot;, &quot;-f&quot;, &quot;nexapod_client.py&quot;]&#10;          initialDelaySeconds: 10&#10;          periodSeconds: 10&#10;        readinessProbe:&#10;          exec:&#10;            command: [&quot;pgrep&quot;, &quot;-f&quot;, &quot;nexapod_client.py&quot;]&#10;          initialDelaySeconds: 5&#10;          periodSeconds: 5&#10;      volumes:&#10;      - name: config&#10;        configMap:&#10;          name: nexapod-client-config&#10;---&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: nexapod-client&#10;spec:&#10;  selector:&#10;    app: nexapod-client&#10;  ports:&#10;    - protocol: TCP&#10;      port: 9000&#10;      targetPort: 9000&#10;" />
              <option name="updatedContent" value="apiVersion: apps/v1&#10;kind: Deployment&#10;metadata:&#10;  name: nexapod-client&#10;spec:&#10;  replicas: 1&#10;  selector:&#10;    matchLabels:&#10;      app: nexapod-client&#10;  template:&#10;    metadata:&#10;      labels:&#10;        app: nexapod-client&#10;    spec:&#10;      containers:&#10;      - name: client&#10;        image: nexapod-client:latest&#10;        env:&#10;        - name: COORDINATOR_URL&#10;          value: &quot;http://nexapod-server:8000&quot;&#10;        ports:&#10;        - containerPort: 8501&#10;        volumeMounts:&#10;        - name: config&#10;          mountPath: /app/config.yaml&#10;          subPath: config.yaml&#10;        resources:&#10;          requests:&#10;            cpu: &quot;50m&quot;&#10;            memory: &quot;64Mi&quot;&#10;          limits:&#10;            cpu: &quot;200m&quot;&#10;            memory: &quot;256Mi&quot;&#10;        livenessProbe:&#10;          exec:&#10;            command: [&quot;pgrep&quot;, &quot;-f&quot;, &quot;streamlit&quot;]&#10;          initialDelaySeconds: 10&#10;          periodSeconds: 10&#10;        readinessProbe:&#10;          exec:&#10;            command: [&quot;pgrep&quot;, &quot;-f&quot;, &quot;streamlit&quot;]&#10;          initialDelaySeconds: 5&#10;          periodSeconds: 5&#10;      volumes:&#10;      - name: config&#10;        configMap:&#10;          name: nexapod-client-config&#10;---&#10;apiVersion: v1&#10;kind: Service&#10;metadata:&#10;  name: nexapod-client&#10;spec:&#10;  selector:&#10;    app: nexapod-client&#10;  ports:&#10;    - protocol: TCP&#10;      port: 8501&#10;      targetPort: 8501" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/Descriptor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/Descriptor.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Module defining descriptors for infrastructure components.&#10;&quot;&quot;&quot;&#10;from dataclasses import dataclass&#10;&#10;@dataclass&#10;class RateLimitDescriptor:&#10;    &quot;&quot;&quot;Descriptor for rate limiter configuration.&quot;&quot;&quot;&#10;    max_calls: int&#10;    period_seconds: float&#10;&#10;__all__ = [&quot;RateLimitDescriptor&quot;]&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Module defining descriptors for infrastructure components.&#10;&quot;&quot;&quot;&#10;from dataclasses import dataclass&#10;&#10;&#10;@dataclass&#10;class RateLimitDescriptor:&#10;    &quot;&quot;&quot;Descriptor for rate limiter configuration.&quot;&quot;&quot;&#10;    max_calls: int&#10;    period_seconds: float&#10;&#10;&#10;__all__ = [&quot;RateLimitDescriptor&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/Dockerfile.client">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/Dockerfile.client" />
              <option name="originalContent" value="FROM python:3.11-slim&#10;ENV APP_USER=nexapod&#10;RUN addgroup --system $APP_USER &amp;&amp; adduser --system --ingroup $APP_USER $APP_USER&#10;WORKDIR /app&#10;&#10;# bring in client code, model weights, and runner&#10;COPY Client/ /app/&#10;COPY Runner/ /app/        # &lt;— ensure your runner.py lands in /app/&#10;COPY safe_tensors/ /app/safe_tensors/&#10;COPY requirements.txt /app/&#10;RUN pip install --no-cache-dir -r requirements.txt&#10;&#10;USER $APP_USER&#10;ENV CLIENT_CONFIG=/app/config.yaml&#10;CMD [&quot;python&quot;, &quot;runner.py&quot;]&#10;" />
              <option name="updatedContent" value="FROM python:3.11-slim&#13;&#10;ENV APP_USER=nexapod&#13;&#10;RUN addgroup --system $APP_USER &amp;&amp; adduser --system --ingroup $APP_USER $APP_USER&#13;&#10;WORKDIR /app&#13;&#10;&#13;&#10;# Copy client-specific requirements and install them&#13;&#10;COPY ./Client/requirements.txt .&#13;&#10;RUN python -m pip install --upgrade pip &amp;&amp; \&#13;&#10;    pip install --no-cache-dir -r requirements.txt&#13;&#10;&#13;&#10;# Copy application code&#13;&#10;COPY ./Client /app/Client&#13;&#10;COPY ./Runner /app/Runner&#13;&#10;COPY ./safe_tensors /app/safe_tensors&#13;&#10;&#13;&#10;USER $APP_USER&#13;&#10;# The client is run via the nexapod CLI, which specifies the command.&#13;&#10;# We can have a default command for dashboard or runner.&#13;&#10;CMD [&quot;python&quot;, &quot;-m&quot;, &quot;Client.dashboard&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/Dockerfile.server">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/Dockerfile.server" />
              <option name="originalContent" value="FROM python:3.11-slim&#10;ENV APP_USER=nexapod&#10;RUN addgroup --system $APP_USER &amp;&amp; adduser --system --ingroup $APP_USER $APP_USER&#10;WORKDIR /app&#10;&#10;# Copy requirements first to leverage Docker layer caching&#10;COPY requirements.txt .&#10;# Upgrade pip and then install packages&#10;RUN python -m pip install --upgrade pip &amp;&amp; \&#10;    pip install --no-cache-dir -r requirements.txt&#10;&#10;# Copy application code&#10;COPY ./Infrastructure /app/Infrastructure&#10;COPY ./Server /app/Server&#10;&#10;USER $APP_USER&#10;EXPOSE 8000&#10;CMD [&quot;python&quot;, &quot;-m&quot;, &quot;Server.app&quot;]&#10;" />
              <option name="updatedContent" value="FROM python:3.11-slim&#10;ENV APP_USER=nexapod&#10;RUN addgroup --system $APP_USER &amp;&amp; adduser --system --ingroup $APP_USER $APP_USER&#10;WORKDIR /app&#10;&#10;# Copy server-specific requirements and install them&#10;COPY ./Server/requirements.txt .&#10;# Upgrade pip and then install packages&#10;RUN python -m pip install --upgrade pip &amp;&amp; \&#10;    pip install --no-cache-dir -r requirements.txt&#10;&#10;# Copy application code&#10;COPY ./Infrastructure /app/Infrastructure&#10;COPY ./Server /app/Server&#10;&#10;USER $APP_USER&#10;EXPOSE 8000&#10;CMD [&quot;python&quot;, &quot;-m&quot;, &quot;Server.app&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/__init__.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Infrastructure package for NEXAPod.&#10;&quot;&quot;&quot;&#10;from .api import app, scheduler, db&#10;&#10;__all__ = [&#10;    &quot;app&quot;,&#10;    &quot;scheduler&quot;,&#10;    &quot;db&quot;&#10;]&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#13;&#10;Infrastructure package for NEXAPod.&#13;&#10;&quot;&quot;&quot;&#13;&#10;from .api import app, scheduler, db&#13;&#10;&#13;&#10;__all__ = [&#13;&#10;    &quot;app&quot;,&#13;&#10;    &quot;scheduler&quot;,&#13;&#10;    &quot;db&quot;&#13;&#10;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/api.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/api.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;from flask import Flask, request, jsonify&#10;from .node import Node&#10;from .tier import Tier&#10;from .scheduler import Scheduler&#10;from .database import Database&#10;&#10;app = Flask(__name__)&#10;scheduler = Scheduler()&#10;db = Database()&#10;&#10;@app.route('/register', methods=['POST'])&#10;def register():&#10;    &quot;&quot;&quot;Register a new node with its profile and tier.&quot;&quot;&quot;&#10;    data = request.get_json()&#10;    node = Node(data.get('id'), Tier(data.get('tier')))&#10;    db.store_node({&#10;        &quot;id&quot;: node.id,&#10;        &quot;tier&quot;: node.tier.value,&#10;        &quot;profile&quot;: node.profile&#10;    })&#10;    return jsonify({&quot;status&quot;: &quot;registered&quot;, &quot;node&quot;: node.id})&#10;&#10;@app.route('/submit-job', methods=['POST'])&#10;def submit_job():&#10;    &quot;&quot;&quot;Accept a job submission and enqueue it for scheduling.&quot;&quot;&quot;&#10;    job = request.get_json()&#10;    scheduler.submit_job(job)&#10;    return jsonify({&quot;status&quot;: &quot;job submitted&quot;, &quot;job_id&quot;: job.get('id')})&#10;&#10;@app.route('/status', methods=['GET'])&#10;def status():&#10;    &quot;&quot;&quot;Return current nodes and jobs stored in the database.&quot;&quot;&quot;&#10;    return jsonify({&#10;        &quot;nodes&quot;: db.get_nodes(),&#10;        &quot;jobs&quot;: db.get_jobs()&#10;    })&#10;&#10;if __name__ == '__main__':&#10;    host = os.getenv('HOST', '0.0.0.0')&#10;    port = int(os.getenv('PORT', 8000))&#10;    app.run(host=host, port=port)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;from flask import Flask, request, jsonify&#10;from .node import Node&#10;from .tier import Tier&#10;from .scheduler import Scheduler&#10;from .database import Database&#10;&#10;app = Flask(__name__)&#10;scheduler = Scheduler()&#10;db = Database()&#10;&#10;&#10;@app.route('/register', methods=['POST'])&#10;def register():&#10;    &quot;&quot;&quot;Register a new node with its profile and tier.&quot;&quot;&quot;&#10;    data = request.get_json()&#10;    node = Node(data.get('id'), Tier(data.get('tier')))&#10;    db.store_node({&#10;        &quot;id&quot;: node.id,&#10;        &quot;tier&quot;: node.tier.value,&#10;        &quot;profile&quot;: node.profile&#10;    })&#10;    return jsonify({&quot;status&quot;: &quot;registered&quot;, &quot;node&quot;: node.id})&#10;&#10;&#10;@app.route('/submit-job', methods=['POST'])&#10;def submit_job():&#10;    &quot;&quot;&quot;Accept a job submission and enqueue it for scheduling.&quot;&quot;&quot;&#10;    job = request.get_json()&#10;    scheduler.submit_job(job)&#10;    return jsonify({&quot;status&quot;: &quot;job submitted&quot;, &quot;job_id&quot;: job.get('id')})&#10;&#10;&#10;@app.route('/status', methods=['GET'])&#10;def status():&#10;    &quot;&quot;&quot;Return current nodes and jobs stored in the database.&quot;&quot;&quot;&#10;    return jsonify({&#10;        &quot;nodes&quot;: db.get_nodes(),&#10;        &quot;jobs&quot;: db.get_jobs()&#10;    })&#10;&#10;&#10;if __name__ == '__main__':&#10;    host = os.getenv('HOST', '0.0.0.0')&#10;    port = int(os.getenv('PORT', 8000))&#10;    app.run(host=host, port=port)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/database.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/database.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Database access layer for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;import sqlite3&#10;&#10;&#10;class Database:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, and logs.&quot;&quot;&quot;&#10;    def __init__(self, path: str = 'nexapod.db'):&#10;        self.conn = sqlite3.connect(path, check_same_thread=False)&#10;        self.create_tables()&#10;&#10;    def create_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, and logs if they do not exist.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            '''CREATE TABLE IF NOT EXISTS nodes (&#10;               id TEXT PRIMARY KEY,&#10;               tier TEXT,&#10;               profile TEXT)'''&#10;        )&#10;        cursor.execute(&#10;            '''CREATE TABLE IF NOT EXISTS jobs (&#10;               id TEXT PRIMARY KEY,&#10;               data TEXT,&#10;               result TEXT)'''&#10;        )&#10;        cursor.execute(&#10;            '''CREATE TABLE IF NOT EXISTS logs (&#10;               id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;               job_id TEXT,&#10;               log TEXT)'''&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_node(self, node: dict):&#10;        &quot;&quot;&quot;Insert or update a node record.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            'INSERT OR REPLACE INTO nodes VALUES (?,?,?)',&#10;            (node['id'], node['tier'], str(node['profile']))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_job(self, job: dict, result: dict):&#10;        &quot;&quot;&quot;Insert or update a job record with its result.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            'INSERT OR REPLACE INTO jobs VALUES (?,?,?)',&#10;            (job['id'], str(job), str(result))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_nodes(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored nodes.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute('SELECT * FROM nodes')&#10;        return cursor.fetchall()&#10;&#10;    def get_jobs(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored jobs.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute('SELECT * FROM jobs')&#10;        return cursor.fetchall()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Database access layer for NEXAPod coordinator.&#10;&quot;&quot;&quot;&#10;&#10;import sqlite3&#10;&#10;&#10;class Database:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, and logs.&quot;&quot;&quot;&#10;    def __init__(self, path: str = &quot;nexapod.db&quot;):&#10;        self.conn = sqlite3.connect(path, check_same_thread=False)&#10;        self.create_tables()&#10;&#10;    def create_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, and logs if they do not exist.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS nodes (&#10;               id TEXT PRIMARY KEY,&#10;               tier TEXT,&#10;               profile TEXT)&quot;&quot;&quot;&#10;        )&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS jobs (&#10;               id TEXT PRIMARY KEY,&#10;               data TEXT,&#10;               result TEXT)&quot;&quot;&quot;&#10;        )&#10;        cursor.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS logs (&#10;               id INTEGER PRIMARY KEY AUTOINCREMENT,&#10;               job_id TEXT,&#10;               log TEXT)&quot;&quot;&quot;&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_node(self, node: dict):&#10;        &quot;&quot;&quot;Insert or update a node record.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO nodes VALUES (?,?,?)&quot;,&#10;            (node[&quot;id&quot;], node[&quot;tier&quot;], str(node[&quot;profile&quot;]))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def store_job(self, job: dict, result: dict):&#10;        &quot;&quot;&quot;Insert or update a job record with its result.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&#10;            &quot;INSERT OR REPLACE INTO jobs VALUES (?,?,?)&quot;,&#10;            (job[&quot;id&quot;], str(job), str(result))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_nodes(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored nodes.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT * FROM nodes&quot;)&#10;        return cursor.fetchall()&#10;&#10;    def get_jobs(self) -&gt; list:&#10;        &quot;&quot;&quot;Retrieve all stored jobs.&quot;&quot;&quot;&#10;        cursor = self.conn.cursor()&#10;        cursor.execute(&quot;SELECT * FROM jobs&quot;)&#10;        return cursor.fetchall()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/replication.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/replication.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Module for job replication strategies.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import hashlib&#10;import json&#10;import logging&#10;import time&#10;from concurrent.futures import ThreadPoolExecutor&#10;from dataclasses import dataclass&#10;from enum import Enum&#10;from typing import List, Dict, Optional, Callable, Any&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;logger = logging.getLogger(__name__)&#10;&#10;&#10;class ReplicationStrategy(Enum):&#10;    &quot;&quot;&quot;Available replication strategies.&quot;&quot;&quot;&#10;    NONE = &quot;none&quot;&#10;    SIMPLE = &quot;simple&quot;&#10;    CONSENSUS = &quot;consensus&quot;&#10;    CHECKPOINT = &quot;checkpoint&quot;&#10;    REDUNDANT = &quot;redundant&quot;&#10;&#10;&#10;class ReplicationStatus(Enum):&#10;    &quot;&quot;&quot;Replication status for jobs.&quot;&quot;&quot;&#10;    PENDING = &quot;pending&quot;&#10;    IN_PROGRESS = &quot;in_progress&quot;&#10;    COMPLETED = &quot;completed&quot;&#10;    FAILED = &quot;failed&quot;&#10;    VERIFIED = &quot;verified&quot;&#10;&#10;&#10;@dataclass&#10;class JobDescriptor:&#10;    &quot;&quot;&quot;Job descriptor with replication metadata.&quot;&quot;&quot;&#10;    id: str&#10;    task_type: str&#10;    payload: Dict[str, Any]&#10;    priority: int = 1&#10;    needs_replication: bool = False&#10;    replication_strategy: ReplicationStrategy = ReplicationStrategy.SIMPLE&#10;    replication_factor: int = 2&#10;    verification_threshold: float = 0.8&#10;    checksum: Optional[str] = None&#10;    timestamp: float = 0.0&#10;&#10;    def __post_init__(self):&#10;        if self.timestamp == 0.0:&#10;            self.timestamp = time.time()&#10;        if self.checksum is None:&#10;            self.checksum = self._calculate_checksum()&#10;&#10;    def _calculate_checksum(self) -&gt; str:&#10;        &quot;&quot;&quot;Calculate checksum for job integrity verification.&quot;&quot;&quot;&#10;        data = f&quot;{self.id}{self.task_type}{json.dumps(self.payload, sort_keys=True)}&quot;&#10;        return hashlib.sha256(data.encode()).hexdigest()&#10;&#10;    def verify_integrity(self) -&gt; bool:&#10;        &quot;&quot;&quot;Verify job data integrity using checksum.&quot;&quot;&quot;&#10;        return self.checksum == self._calculate_checksum()&#10;&#10;&#10;@dataclass&#10;class ReplicationResult:&#10;    &quot;&quot;&quot;Result of a replication operation.&quot;&quot;&quot;&#10;    job_id: str&#10;    node_id: str&#10;    status: ReplicationStatus&#10;    result_hash: Optional[str] = None&#10;    execution_time: float = 0.0&#10;    error_message: Optional[str] = None&#10;    timestamp: float = 0.0&#10;&#10;    def __post_init__(self):&#10;        if self.timestamp == 0.0:&#10;            self.timestamp = time.time()&#10;&#10;&#10;class ReplicationNode:&#10;    &quot;&quot;&quot;Represents a compute node for replication.&quot;&quot;&quot;&#10;&#10;    def __init__(self, node_id: str, compute_func: Callable):&#10;        self.node_id = node_id&#10;        self.compute_func = compute_func&#10;        self.is_available = True&#10;        self.load = 0.0&#10;        self.reliability_score = 1.0&#10;&#10;    async def execute_job(self, job: JobDescriptor) -&gt; ReplicationResult:&#10;        &quot;&quot;&quot;Execute job on this node and return result.&quot;&quot;&quot;&#10;        start_time = time.time()&#10;&#10;        try:&#10;            if not self.is_available:&#10;                raise RuntimeError(f&quot;Node {self.node_id} is not available&quot;)&#10;&#10;            if not job.verify_integrity():&#10;                raise ValueError(f&quot;Job {job.id} failed integrity check&quot;)&#10;&#10;            # Simulate job execution&#10;            result = await self._simulate_computation(job)&#10;            execution_time = time.time() - start_time&#10;&#10;            result_hash = hashlib.sha256(str(result).encode()).hexdigest()&#10;&#10;            return ReplicationResult(&#10;                job_id=job.id,&#10;                node_id=self.node_id,&#10;                status=ReplicationStatus.COMPLETED,&#10;                result_hash=result_hash,&#10;                execution_time=execution_time&#10;            )&#10;&#10;        except Exception as e:&#10;            execution_time = time.time() - start_time&#10;            logger.error(f&quot;Job execution failed on node {self.node_id}: {e}&quot;)&#10;&#10;            return ReplicationResult(&#10;                job_id=job.id,&#10;                node_id=self.node_id,&#10;                status=ReplicationStatus.FAILED,&#10;                execution_time=execution_time,&#10;                error_message=str(e)&#10;            )&#10;&#10;    @staticmethod&#10;    async def _simulate_computation(job: JobDescriptor) -&gt; Any:&#10;        &quot;&quot;&quot;Simulate computation based on job type.&quot;&quot;&quot;&#10;        await asyncio.sleep(0.1)  # Simulate processing time&#10;&#10;        if job.task_type == &quot;protein_folding&quot;:&#10;            return {&quot;conformation&quot;: &quot;folded&quot;, &quot;energy&quot;: -42.5}&#10;        elif job.task_type == &quot;weather_simulation&quot;:&#10;            return {&quot;temperature&quot;: 23.5, &quot;humidity&quot;: 65.2}&#10;        elif job.task_type == &quot;quantum_computation&quot;:&#10;            return {&quot;qubits&quot;: [0, 1, 0, 1], &quot;entanglement&quot;: True}&#10;        else:&#10;            return {&quot;result&quot;: f&quot;computed_{job.id}&quot;}&#10;&#10;&#10;class ConsensusValidator:&#10;    &quot;&quot;&quot;Validates results using consensus mechanisms.&quot;&quot;&quot;&#10;&#10;    def __init__(self, threshold: float = 0.67):&#10;        self.threshold = threshold&#10;&#10;    def validate_results(self, results: List[ReplicationResult]) -&gt; ReplicationResult:&#10;        &quot;&quot;&quot;Validate results using majority consensus.&quot;&quot;&quot;&#10;        if not results:&#10;            raise ValueError(&quot;No results to validate&quot;)&#10;        # Group results by hash&#10;        hash_groups: Dict[str, List[ReplicationResult]] = {}&#10;        for result in results:&#10;            if result.status == ReplicationStatus.COMPLETED and result.result_hash:&#10;                hash_groups.setdefault(result.result_hash, []).append(result)&#10;        if not hash_groups:&#10;            # No successful results&#10;            failed_result = next(&#10;                (r for r in results if r.status == ReplicationStatus.FAILED),&#10;                results[0]&#10;            )&#10;            failed_result.status = ReplicationStatus.FAILED&#10;            return failed_result&#10;        # Find consensus&#10;        total_successful = sum(len(group) for group in hash_groups.values())&#10;        consensus_hash, consensus_results = max(&#10;            hash_groups.items(), key=lambda x: len(x[1])&#10;        )&#10;        consensus_ratio = len(consensus_results) / total_successful&#10;        # Select best result from consensus group&#10;        best_result = min(consensus_results, key=lambda r: r.execution_time)&#10;        if consensus_ratio &gt;= self.threshold:&#10;            best_result.status = ReplicationStatus.VERIFIED&#10;        else:&#10;            best_result.status = ReplicationStatus.COMPLETED&#10;        logger.info(&#10;            f&quot;Consensus reached for job {best_result.job_id}: {consensus_ratio:.2%}&quot;&#10;        )&#10;        return best_result&#10;&#10;&#10;class Replicator:&#10;    &quot;&quot;&quot;Performs replication logic for computed jobs.&quot;&quot;&quot;&#10;&#10;    def __init__(self, nodes: Optional[List[ReplicationNode]] = None):&#10;        self.nodes = nodes or []&#10;        self.validator = ConsensusValidator()&#10;        self.executor = ThreadPoolExecutor(max_workers=10)&#10;        self.replication_history: Dict[str, List[ReplicationResult]] = {}&#10;&#10;    def add_node(self, node: ReplicationNode):&#10;        &quot;&quot;&quot;Add a replication node.&quot;&quot;&quot;&#10;        self.nodes.append(node)&#10;        logger.info(f&quot;Added replication node: {node.node_id}&quot;)&#10;&#10;    def remove_node(self, node_id: str):&#10;        &quot;&quot;&quot;Remove a replication node.&quot;&quot;&quot;&#10;        self.nodes = [n for n in self.nodes if n.node_id != node_id]&#10;        logger.info(f&quot;Removed replication node: {node_id}&quot;)&#10;&#10;    def select_nodes(self, job: JobDescriptor) -&gt; List[ReplicationNode]:&#10;        &quot;&quot;&quot;Select optimal nodes for job replication.&quot;&quot;&quot;&#10;        available_nodes = [n for n in self.nodes if n.is_available]&#10;        if len(available_nodes) &lt; job.replication_factor:&#10;            logger.warning(&#10;                f&quot;Insufficient nodes for replication factor {job.replication_factor}&quot;&#10;            )&#10;            return available_nodes&#10;        # Sort by reliability and load&#10;        sorted_nodes = sorted(&#10;            available_nodes,&#10;            key=lambda n: (n.reliability_score, -n.load),&#10;            reverse=True&#10;        )&#10;        return sorted_nodes[:job.replication_factor]&#10;&#10;    def replicate(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Replicate computation based on job descriptor.&quot;&quot;&quot;&#10;        logger.info(f&quot;Starting replication for job: {job.id}&quot;)&#10;        if not job.needs_replication:&#10;            logger.info(f&quot;No replication required for job: {job.id}&quot;)&#10;            return True&#10;        try:&#10;            if job.replication_strategy == ReplicationStrategy.SIMPLE:&#10;                return self._simple_replication(job)&#10;            elif job.replication_strategy == ReplicationStrategy.CONSENSUS:&#10;                return self._consensus_replication(job)&#10;            elif job.replication_strategy == ReplicationStrategy.REDUNDANT:&#10;                return self._redundant_replication(job)&#10;            else:&#10;                logger.error(&#10;                    f&quot;Unsupported replication strategy: {job.replication_strategy}&quot;&#10;                )&#10;                return False&#10;        except Exception as e:&#10;            logger.error(f&quot;Replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def _simple_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Simple replication strategy - execute once with backup.&quot;&quot;&quot;&#10;        selected_nodes = self.select_nodes(job)&#10;&#10;        if not selected_nodes:&#10;            logger.error(f&quot;No available nodes for job {job.id}&quot;)&#10;            return False&#10;&#10;        primary_node = selected_nodes[0]&#10;        backup_nodes = selected_nodes[1:] if len(selected_nodes) &gt; 1 else []&#10;&#10;        try:&#10;            # Execute on primary node&#10;            result = asyncio.run(primary_node.execute_job(job))&#10;&#10;            if result.status == ReplicationStatus.COMPLETED:&#10;                self.replication_history[job.id] = [result]&#10;                logger.info(f&quot;Simple replication successful for job {job.id}&quot;)&#10;                return True&#10;&#10;            # Try backup nodes if primary fails&#10;            for backup_node in backup_nodes:&#10;                backup_result = asyncio.run(backup_node.execute_job(job))&#10;                if backup_result.status == ReplicationStatus.COMPLETED:&#10;                    self.replication_history[job.id] = [backup_result]&#10;                    logger.info(f&quot;Backup replication successful for job {job.id}&quot;)&#10;                    return True&#10;&#10;            logger.error(f&quot;All replication attempts failed for job {job.id}&quot;)&#10;            return False&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Simple replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def _consensus_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Consensus-based replication strategy.&quot;&quot;&quot;&#10;        selected_nodes = self.select_nodes(job)&#10;        if len(selected_nodes) &lt; 2:&#10;            logger.warning(&#10;                f&quot;Insufficient nodes for consensus replication of job {job.id}&quot;&#10;            )&#10;            return self._simple_replication(job)&#10;        try:&#10;            # Execute on all selected nodes concurrently&#10;            async def run_consensus():&#10;                tasks = [node.execute_job(job) for node in selected_nodes]&#10;                return await asyncio.gather(*tasks, return_exceptions=True)&#10;            results = asyncio.run(run_consensus())&#10;            # Filter out exceptions and failed results&#10;            valid_results = [&#10;                r for r in results&#10;                if isinstance(r, ReplicationResult) and r.status in [&#10;                    ReplicationStatus.COMPLETED, ReplicationStatus.FAILED&#10;                ]&#10;            ]&#10;            if not valid_results:&#10;                logger.error(&#10;                    f&quot;No valid results for consensus replication of job {job.id}&quot;&#10;                )&#10;                return False&#10;            # Validate using consensus&#10;            consensus_result = self.validator.validate_results(valid_results)&#10;            self.replication_history[job.id] = valid_results&#10;            success = consensus_result.status in [&#10;                ReplicationStatus.COMPLETED, ReplicationStatus.VERIFIED&#10;            ]&#10;            if success:&#10;                logger.info(f&quot;Consensus replication successful for job {job.id}&quot;)&#10;            else:&#10;                logger.error(f&quot;Consensus replication failed for job {job.id}&quot;)&#10;            return success&#10;        except Exception as e:&#10;            logger.error(&#10;                f&quot;Consensus replication failed for job {job.id}: &quot;&#10;                f&quot;{e}&quot;&#10;            )&#10;            return False&#10;&#10;    def _redundant_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Redundant replication strategy - execute on all available nodes.&quot;&quot;&quot;&#10;        available_nodes = [n for n in self.nodes if n.is_available]&#10;        if not available_nodes:&#10;            logger.error(&#10;                f&quot;No available nodes for redundant replication of job {job.id}&quot;&#10;            )&#10;            return False&#10;        try:&#10;            async def run_redundant():&#10;                tasks = [node.execute_job(job) for node in available_nodes]&#10;                return await asyncio.gather(*tasks, return_exceptions=True)&#10;            results = asyncio.run(run_redundant())&#10;            valid_results = [&#10;                r for r in results&#10;                if isinstance(r, ReplicationResult)&#10;            ]&#10;            successful_results = [&#10;                r for r in valid_results&#10;                if r.status == ReplicationStatus.COMPLETED&#10;            ]&#10;            self.replication_history[job.id] = valid_results&#10;            if successful_results:&#10;                logger.info(&#10;                    f&quot;Redundant replication successful for job {job.id}: &quot;&#10;                    f&quot;{len(successful_results)}/{len(valid_results)} succeeded&quot;&#10;                )&#10;                return True&#10;            else:&#10;                logger.error(&#10;                    f&quot;Redundant replication failed for job {job.id}: no successful executions&quot;&#10;                )&#10;                return False&#10;        except Exception as e:&#10;            logger.error(f&quot;Redundant replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def get_replication_status(self, job_id: str) -&gt; Optional[List[ReplicationResult]]:&#10;        &quot;&quot;&quot;Get replication history for a job.&quot;&quot;&quot;&#10;        return self.replication_history.get(job_id)&#10;&#10;    def cleanup_history(self, max_age_hours: int = 24):&#10;        &quot;&quot;&quot;Clean up old replication history.&quot;&quot;&quot;&#10;        cutoff_time = time.time() - (max_age_hours * 3600)&#10;&#10;        for job_id, results in list(self.replication_history.items()):&#10;            if all(r.timestamp &lt; cutoff_time for r in results):&#10;                del self.replication_history[job_id]&#10;&#10;        logger.info(f&quot;Cleaned up replication history older than {max_age_hours} hours&quot;)&#10;&#10;&#10;def replicate_data():&#10;    &quot;&quot;&quot;Function to handle data replication logic.&quot;&quot;&quot;&#10;    logger.info(&quot;Data replication process started.&quot;)&#10;    # Here you would implement the actual data replication logic&#10;    # For now, we just log the action&#10;    logger.info(&quot;Data replication process completed.&quot;)&#10;    return True&#10;&#10;&#10;def replicate_job(job: JobDescriptor, replicator: Optional[Replicator] = None):&#10;    &quot;&quot;&quot;Function to handle job replication logic.&quot;&quot;&quot;&#10;    logger.info(f&quot;Job replication process started for job: {job.id}&quot;)&#10;&#10;    if replicator is None:&#10;        # Create default replicator with mock nodes&#10;        replicator = Replicator()&#10;        # Add some mock nodes for demonstration&#10;        for i in range(3):&#10;            node = ReplicationNode(f&quot;node_{i}&quot;, lambda x: f&quot;result_{i}&quot;)&#10;            replicator.add_node(node)&#10;&#10;    success = replicator.replicate(job)&#10;&#10;    if success:&#10;        logger.info(f&quot;Job replication successful for job: {job.id}&quot;)&#10;    else:&#10;        logger.error(f&quot;Job replication failed for job: {job.id}&quot;)&#10;&#10;    return success&#10;&#10;&#10;def replicate_jobs(jobs: List[JobDescriptor], replicator: Optional[Replicator] = None):&#10;    &quot;&quot;&quot;Function to handle replication of multiple jobs.&quot;&quot;&quot;&#10;    logger.info(&quot;Starting replication for multiple jobs.&quot;)&#10;    if replicator is None:&#10;        replicator = Replicator()&#10;        # Add mock nodes&#10;        for i in range(5):&#10;            node = ReplicationNode(f&quot;node_{i}&quot;, lambda x: f&quot;result_{i}&quot;)&#10;            replicator.add_node(node)&#10;    results = []&#10;    for job in jobs:&#10;        result = replicate_job(job, replicator)&#10;        results.append(result)&#10;    successful_count = sum(results)&#10;    logger.info(&#10;        f&quot;Completed replication for {successful_count}/{len(jobs)} jobs successfully.&quot;&#10;    )&#10;    return all(results)&#10;&#10;&#10;# Example usage and testing&#10;def create_demo_jobs() -&gt; List[JobDescriptor]:&#10;    &quot;&quot;&quot;Create demo jobs for testing.&quot;&quot;&quot;&#10;    jobs = [&#10;        JobDescriptor(&#10;            id=&quot;job_001&quot;,&#10;            task_type=&quot;protein_folding&quot;,&#10;            payload={&quot;protein_sequence&quot;: &quot;ACDEFGHIKLMNPQRSTVWY&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.CONSENSUS,&#10;            replication_factor=3&#10;        ),&#10;        JobDescriptor(&#10;            id=&quot;job_002&quot;,&#10;            task_type=&quot;weather_simulation&quot;,&#10;            payload={&quot;region&quot;: &quot;northwest&quot;, &quot;time_range&quot;: &quot;24h&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.SIMPLE,&#10;            replication_factor=2&#10;        ),&#10;        JobDescriptor(&#10;            id=&quot;job_003&quot;,&#10;            task_type=&quot;quantum_computation&quot;,&#10;            payload={&quot;qubits&quot;: 8, &quot;algorithm&quot;: &quot;shor&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.REDUNDANT,&#10;            replication_factor=5&#10;        )&#10;    ]&#10;    return jobs&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Demo usage&#10;    demo_jobs = create_demo_jobs()&#10;&#10;    # Create replicator with nodes&#10;    replicator = Replicator()&#10;    for i in range(5):&#10;        node = ReplicationNode(f&quot;compute_node_{i}&quot;, lambda x: f&quot;computed_result_{i}&quot;)&#10;        node.reliability_score = 0.8 + (i * 0.05)  # Varying reliability&#10;        replicator.add_node(node)&#10;&#10;    # Test replication&#10;    print(&quot;Testing job replication...&quot;)&#10;    for job in demo_jobs:&#10;        success = replicator.replicate(job)&#10;        status = replicator.get_replication_status(job.id)&#10;        print(f&quot;Job {job.id}: {'SUCCESS' if success else 'FAILED'}&quot;)&#10;        if status:&#10;            print(f&quot;  Results: {len(status)} executions&quot;)&#10;            for result in status:&#10;                print(f&quot;    Node {result.node_id}: {result.status.value}&quot;)&#10;&#10;    # Cleanup&#10;    replicator.cleanup_history()&#10;    print(&quot;Replication history cleaned up.&quot;)&#10;    replicate_data()  # Example data replication call&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Module for job replication strategies.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import hashlib&#10;import json&#10;import logging&#10;import time&#10;from concurrent.futures import ThreadPoolExecutor&#10;from dataclasses import dataclass&#10;from enum import Enum&#10;from typing import List, Dict, Optional, Callable, Any&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;logger = logging.getLogger(__name__)&#10;&#10;&#10;class ReplicationStrategy(Enum):&#10;    &quot;&quot;&quot;Available replication strategies.&quot;&quot;&quot;&#10;    NONE = &quot;none&quot;&#10;    SIMPLE = &quot;simple&quot;&#10;    CONSENSUS = &quot;consensus&quot;&#10;    CHECKPOINT = &quot;checkpoint&quot;&#10;    REDUNDANT = &quot;redundant&quot;&#10;&#10;&#10;class ReplicationStatus(Enum):&#10;    &quot;&quot;&quot;Replication status for jobs.&quot;&quot;&quot;&#10;    PENDING = &quot;pending&quot;&#10;    IN_PROGRESS = &quot;in_progress&quot;&#10;    COMPLETED = &quot;completed&quot;&#10;    FAILED = &quot;failed&quot;&#10;    VERIFIED = &quot;verified&quot;&#10;&#10;&#10;@dataclass&#10;class JobDescriptor:&#10;    &quot;&quot;&quot;Job descriptor with replication metadata.&quot;&quot;&quot;&#10;    id: str&#10;    task_type: str&#10;    payload: Dict[str, Any]&#10;    priority: int = 1&#10;    needs_replication: bool = False&#10;    replication_strategy: ReplicationStrategy = ReplicationStrategy.SIMPLE&#10;    replication_factor: int = 2&#10;    verification_threshold: float = 0.8&#10;    checksum: Optional[str] = None&#10;    timestamp: float = 0.0&#10;&#10;    def __post_init__(self):&#10;        if self.timestamp == 0.0:&#10;            self.timestamp = time.time()&#10;        if self.checksum is None:&#10;            self.checksum = self._calculate_checksum()&#10;&#10;    def _calculate_checksum(self) -&gt; str:&#10;        &quot;&quot;&quot;Calculate checksum for job integrity verification.&quot;&quot;&quot;&#10;        data = f&quot;{self.id}{self.task_type}{json.dumps(self.payload, sort_keys=True)}&quot;&#10;        return hashlib.sha256(data.encode()).hexdigest()&#10;&#10;    def verify_integrity(self) -&gt; bool:&#10;        &quot;&quot;&quot;Verify job data integrity using checksum.&quot;&quot;&quot;&#10;        return self.checksum == self._calculate_checksum()&#10;&#10;&#10;@dataclass&#10;class ReplicationResult:&#10;    &quot;&quot;&quot;Result of a replication operation.&quot;&quot;&quot;&#10;    job_id: str&#10;    node_id: str&#10;    status: ReplicationStatus&#10;    result_hash: Optional[str] = None&#10;    execution_time: float = 0.0&#10;    error_message: Optional[str] = None&#10;    timestamp: float = 0.0&#10;&#10;    def __post_init__(self):&#10;        if self.timestamp == 0.0:&#10;            self.timestamp = time.time()&#10;&#10;&#10;class ReplicationNode:&#10;    &quot;&quot;&quot;Represents a compute node for replication.&quot;&quot;&quot;&#10;&#10;    def __init__(self, node_id: str, compute_func: Callable):&#10;        self.node_id = node_id&#10;        self.compute_func = compute_func&#10;        self.is_available = True&#10;        self.load = 0.0&#10;        self.reliability_score = 1.0&#10;&#10;    async def execute_job(self, job: JobDescriptor) -&gt; ReplicationResult:&#10;        &quot;&quot;&quot;Execute job on this node and return result.&quot;&quot;&quot;&#10;        start_time = time.time()&#10;&#10;        try:&#10;            if not self.is_available:&#10;                raise RuntimeError(f&quot;Node {self.node_id} is not available&quot;)&#10;&#10;            if not job.verify_integrity():&#10;                raise ValueError(f&quot;Job {job.id} failed integrity check&quot;)&#10;&#10;            # Simulate job execution&#10;            result = await self._simulate_computation(job)&#10;            execution_time = time.time() - start_time&#10;&#10;            result_hash = hashlib.sha256(str(result).encode()).hexdigest()&#10;&#10;            return ReplicationResult(&#10;                job_id=job.id,&#10;                node_id=self.node_id,&#10;                status=ReplicationStatus.COMPLETED,&#10;                result_hash=result_hash,&#10;                execution_time=execution_time&#10;            )&#10;&#10;        except Exception as e:&#10;            execution_time = time.time() - start_time&#10;            logger.error(f&quot;Job execution failed on node {self.node_id}: {e}&quot;)&#10;&#10;            return ReplicationResult(&#10;                job_id=job.id,&#10;                node_id=self.node_id,&#10;                status=ReplicationStatus.FAILED,&#10;                execution_time=execution_time,&#10;                error_message=str(e)&#10;            )&#10;&#10;    @staticmethod&#10;    async def _simulate_computation(job: JobDescriptor) -&gt; Any:&#10;        &quot;&quot;&quot;Simulate computation based on job type.&quot;&quot;&quot;&#10;        await asyncio.sleep(0.1)  # Simulate processing time&#10;&#10;        if job.task_type == &quot;protein_folding&quot;:&#10;            return {&quot;conformation&quot;: &quot;folded&quot;, &quot;energy&quot;: -42.5}&#10;        elif job.task_type == &quot;weather_simulation&quot;:&#10;            return {&quot;temperature&quot;: 23.5, &quot;humidity&quot;: 65.2}&#10;        elif job.task_type == &quot;quantum_computation&quot;:&#10;            return {&quot;qubits&quot;: [0, 1, 0, 1], &quot;entanglement&quot;: True}&#10;        else:&#10;            return {&quot;result&quot;: f&quot;computed_{job.id}&quot;}&#10;&#10;&#10;class ConsensusValidator:&#10;    &quot;&quot;&quot;Validates results using consensus mechanisms.&quot;&quot;&quot;&#10;&#10;    def __init__(self, threshold: float = 0.67):&#10;        self.threshold = threshold&#10;&#10;    def validate_results(self, results: List[ReplicationResult]) -&gt; ReplicationResult:&#10;        &quot;&quot;&quot;Validate results using majority consensus.&quot;&quot;&quot;&#10;        if not results:&#10;            raise ValueError(&quot;No results to validate&quot;)&#10;        # Group results by hash&#10;        hash_groups: Dict[str, List[ReplicationResult]] = {}&#10;        for result in results:&#10;            if result.status == ReplicationStatus.COMPLETED and result.result_hash:&#10;                hash_groups.setdefault(result.result_hash, []).append(result)&#10;        if not hash_groups:&#10;            # No successful results&#10;            failed_result = next(&#10;                (r for r in results if r.status == ReplicationStatus.FAILED),&#10;                results[0]&#10;            )&#10;            failed_result.status = ReplicationStatus.FAILED&#10;            return failed_result&#10;        # Find consensus&#10;        total_successful = sum(len(group) for group in hash_groups.values())&#10;        consensus_hash, consensus_results = max(&#10;            hash_groups.items(), key=lambda x: len(x[1])&#10;        )&#10;        consensus_ratio = len(consensus_results) / total_successful&#10;        # Select best result from consensus group&#10;        best_result = min(consensus_results, key=lambda r: r.execution_time)&#10;        if consensus_ratio &gt;= self.threshold:&#10;            best_result.status = ReplicationStatus.VERIFIED&#10;        else:&#10;            best_result.status = ReplicationStatus.COMPLETED&#10;        logger.info(&#10;            f&quot;Consensus reached for job {best_result.job_id}: {consensus_ratio:.2%}&quot;&#10;        )&#10;        return best_result&#10;&#10;&#10;class Replicator:&#10;    &quot;&quot;&quot;Performs replication logic for computed jobs.&quot;&quot;&quot;&#10;&#10;    def __init__(self, nodes: Optional[List[ReplicationNode]] = None):&#10;        self.nodes = nodes or []&#10;        self.validator = ConsensusValidator()&#10;        self.executor = ThreadPoolExecutor(max_workers=10)&#10;        self.replication_history: Dict[str, List[ReplicationResult]] = {}&#10;&#10;    def add_node(self, node: ReplicationNode):&#10;        &quot;&quot;&quot;Add a replication node.&quot;&quot;&quot;&#10;        self.nodes.append(node)&#10;        logger.info(f&quot;Added replication node: {node.node_id}&quot;)&#10;&#10;    def remove_node(self, node_id: str):&#10;        &quot;&quot;&quot;Remove a replication node.&quot;&quot;&quot;&#10;        self.nodes = [n for n in self.nodes if n.node_id != node_id]&#10;        logger.info(f&quot;Removed replication node: {node_id}&quot;)&#10;&#10;    def select_nodes(self, job: JobDescriptor) -&gt; List[ReplicationNode]:&#10;        &quot;&quot;&quot;Select optimal nodes for job replication.&quot;&quot;&quot;&#10;        available_nodes = [n for n in self.nodes if n.is_available]&#10;        if len(available_nodes) &lt; job.replication_factor:&#10;            logger.warning(&#10;                f&quot;Insufficient nodes for replication factor {job.replication_factor}&quot;&#10;            )&#10;            return available_nodes&#10;        # Sort by reliability and load&#10;        sorted_nodes = sorted(&#10;            available_nodes,&#10;            key=lambda n: (n.reliability_score, -n.load),&#10;            reverse=True&#10;        )&#10;        return sorted_nodes[:job.replication_factor]&#10;&#10;    def replicate(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Replicate computation based on job descriptor.&quot;&quot;&quot;&#10;        logger.info(f&quot;Starting replication for job: {job.id}&quot;)&#10;        if not job.needs_replication:&#10;            logger.info(f&quot;No replication required for job: {job.id}&quot;)&#10;            return True&#10;        try:&#10;            if job.replication_strategy == ReplicationStrategy.SIMPLE:&#10;                return self._simple_replication(job)&#10;            elif job.replication_strategy == ReplicationStrategy.CONSENSUS:&#10;                return self._consensus_replication(job)&#10;            elif job.replication_strategy == ReplicationStrategy.REDUNDANT:&#10;                return self._redundant_replication(job)&#10;            else:&#10;                logger.error(&#10;                    f&quot;Unsupported replication strategy: {job.replication_strategy}&quot;&#10;                )&#10;                return False&#10;        except Exception as e:&#10;            logger.error(f&quot;Replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def _simple_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Simple replication strategy - execute once with backup.&quot;&quot;&quot;&#10;        selected_nodes = self.select_nodes(job)&#10;&#10;        if not selected_nodes:&#10;            logger.error(f&quot;No available nodes for job {job.id}&quot;)&#10;            return False&#10;&#10;        primary_node = selected_nodes[0]&#10;        backup_nodes = selected_nodes[1:] if len(selected_nodes) &gt; 1 else []&#10;&#10;        try:&#10;            # Execute on primary node&#10;            result = asyncio.run(primary_node.execute_job(job))&#10;&#10;            if result.status == ReplicationStatus.COMPLETED:&#10;                self.replication_history[job.id] = [result]&#10;                logger.info(f&quot;Simple replication successful for job {job.id}&quot;)&#10;                return True&#10;&#10;            # Try backup nodes if primary fails&#10;            for backup_node in backup_nodes:&#10;                backup_result = asyncio.run(backup_node.execute_job(job))&#10;                if backup_result.status == ReplicationStatus.COMPLETED:&#10;                    self.replication_history[job.id] = [backup_result]&#10;                    logger.info(f&quot;Backup replication successful for job {job.id}&quot;)&#10;                    return True&#10;&#10;            logger.error(f&quot;All replication attempts failed for job {job.id}&quot;)&#10;            return False&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Simple replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def _consensus_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Consensus-based replication strategy.&quot;&quot;&quot;&#10;        selected_nodes = self.select_nodes(job)&#10;        if len(selected_nodes) &lt; 2:&#10;            logger.warning(&#10;                f&quot;Insufficient nodes for consensus replication of job {job.id}&quot;&#10;            )&#10;            return self._simple_replication(job)&#10;        try:&#10;            # Execute on all selected nodes concurrently&#10;            async def run_consensus():&#10;                tasks = [node.execute_job(job) for node in selected_nodes]&#10;                return await asyncio.gather(*tasks, return_exceptions=True)&#10;            results = asyncio.run(run_consensus())&#10;            # Filter out exceptions and failed results&#10;            valid_results = [&#10;                r for r in results&#10;                if isinstance(r, ReplicationResult) and r.status in [&#10;                    ReplicationStatus.COMPLETED, ReplicationStatus.FAILED&#10;                ]&#10;            ]&#10;            if not valid_results:&#10;                logger.error(&#10;                    f&quot;No valid results for consensus replication of job {job.id}&quot;&#10;                )&#10;                return False&#10;            # Validate using consensus&#10;            consensus_result = self.validator.validate_results(valid_results)&#10;            self.replication_history[job.id] = valid_results&#10;            success = consensus_result.status in [&#10;                ReplicationStatus.COMPLETED, ReplicationStatus.VERIFIED&#10;            ]&#10;            if success:&#10;                logger.info(f&quot;Consensus replication successful for job {job.id}&quot;)&#10;            else:&#10;                logger.error(f&quot;Consensus replication failed for job {job.id}&quot;)&#10;            return success&#10;        except Exception as e:&#10;            logger.error(&#10;                f&quot;Consensus replication failed for job {job.id}: {e}&quot;&#10;            )&#10;            return False&#10;&#10;    def _redundant_replication(self, job: JobDescriptor) -&gt; bool:&#10;        &quot;&quot;&quot;Redundant replication strategy - execute on all available nodes.&quot;&quot;&quot;&#10;        available_nodes = [n for n in self.nodes if n.is_available]&#10;        if not available_nodes:&#10;            logger.error(&#10;                f&quot;No available nodes for redundant replication of job {job.id}&quot;&#10;            )&#10;            return False&#10;        try:&#10;            async def run_redundant():&#10;                tasks = [node.execute_job(job) for node in available_nodes]&#10;                return await asyncio.gather(*tasks, return_exceptions=True)&#10;            results = asyncio.run(run_redundant())&#10;            valid_results = [&#10;                r for r in results&#10;                if isinstance(r, ReplicationResult)&#10;            ]&#10;            successful_results = [&#10;                r for r in valid_results&#10;                if r.status == ReplicationStatus.COMPLETED&#10;            ]&#10;            self.replication_history[job.id] = valid_results&#10;            if successful_results:&#10;                logger.info(&#10;                    f&quot;Redundant replication successful for job {job.id}: &quot;&#10;                    f&quot;{len(successful_results)}/{len(valid_results)} succeeded&quot;&#10;                )&#10;                return True&#10;            else:&#10;                logger.error(&#10;                    f&quot;Redundant replication failed for job {job.id}: no successful executions&quot;&#10;                )&#10;                return False&#10;        except Exception as e:&#10;            logger.error(f&quot;Redundant replication failed for job {job.id}: {e}&quot;)&#10;            return False&#10;&#10;    def get_replication_status(self, job_id: str) -&gt; Optional[List[ReplicationResult]]:&#10;        &quot;&quot;&quot;Get replication history for a job.&quot;&quot;&quot;&#10;        return self.replication_history.get(job_id)&#10;&#10;    def cleanup_history(self, max_age_hours: int = 24):&#10;        &quot;&quot;&quot;Clean up old replication history.&quot;&quot;&quot;&#10;        cutoff_time = time.time() - (max_age_hours * 3600)&#10;&#10;        for job_id, results in list(self.replication_history.items()):&#10;            if all(r.timestamp &lt; cutoff_time for r in results):&#10;                del self.replication_history[job_id]&#10;&#10;        logger.info(f&quot;Cleaned up replication history older than {max_age_hours} hours&quot;)&#10;&#10;&#10;def replicate_data():&#10;    &quot;&quot;&quot;Function to handle data replication logic.&quot;&quot;&quot;&#10;    logger.info(&quot;Data replication process started.&quot;)&#10;    # Here you would implement the actual data replication logic&#10;    # For now, we just log the action&#10;    logger.info(&quot;Data replication process completed.&quot;)&#10;    return True&#10;&#10;&#10;def replicate_job(job: JobDescriptor, replicator: Optional[Replicator] = None):&#10;    &quot;&quot;&quot;Function to handle job replication logic.&quot;&quot;&quot;&#10;    logger.info(f&quot;Job replication process started for job: {job.id}&quot;)&#10;&#10;    if replicator is None:&#10;        # Create default replicator with mock nodes&#10;        replicator = Replicator()&#10;        # Add some mock nodes for demonstration&#10;        for i in range(3):&#10;            node = ReplicationNode(f&quot;node_{i}&quot;, lambda x: f&quot;result_{i}&quot;)&#10;            replicator.add_node(node)&#10;&#10;    success = replicator.replicate(job)&#10;&#10;    if success:&#10;        logger.info(f&quot;Job replication successful for job: {job.id}&quot;)&#10;    else:&#10;        logger.error(f&quot;Job replication failed for job: {job.id}&quot;)&#10;&#10;    return success&#10;&#10;&#10;def replicate_jobs(jobs: List[JobDescriptor], replicator: Optional[Replicator] = None):&#10;    &quot;&quot;&quot;Function to handle replication of multiple jobs.&quot;&quot;&quot;&#10;    logger.info(&quot;Starting replication for multiple jobs.&quot;)&#10;    if replicator is None:&#10;        replicator = Replicator()&#10;        # Add mock nodes&#10;        for i in range(5):&#10;            node = ReplicationNode(f&quot;node_{i}&quot;, lambda x: f&quot;result_{i}&quot;)&#10;            replicator.add_node(node)&#10;    results = []&#10;    for job in jobs:&#10;        result = replicate_job(job, replicator)&#10;        results.append(result)&#10;    successful_count = sum(results)&#10;    logger.info(&#10;        f&quot;Completed replication for {successful_count}/{len(jobs)} jobs successfully.&quot;&#10;    )&#10;    return all(results)&#10;&#10;&#10;# Example usage and testing&#10;def create_demo_jobs() -&gt; List[JobDescriptor]:&#10;    &quot;&quot;&quot;Create demo jobs for testing.&quot;&quot;&quot;&#10;    jobs = [&#10;        JobDescriptor(&#10;            id=&quot;job_001&quot;,&#10;            task_type=&quot;protein_folding&quot;,&#10;            payload={&quot;protein_sequence&quot;: &quot;ACDEFGHIKLMNPQRSTVWY&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.CONSENSUS,&#10;            replication_factor=3&#10;        ),&#10;        JobDescriptor(&#10;            id=&quot;job_002&quot;,&#10;            task_type=&quot;weather_simulation&quot;,&#10;            payload={&quot;region&quot;: &quot;northwest&quot;, &quot;time_range&quot;: &quot;24h&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.SIMPLE,&#10;            replication_factor=2&#10;        ),&#10;        JobDescriptor(&#10;            id=&quot;job_003&quot;,&#10;            task_type=&quot;quantum_computation&quot;,&#10;            payload={&quot;qubits&quot;: 8, &quot;algorithm&quot;: &quot;shor&quot;},&#10;            needs_replication=True,&#10;            replication_strategy=ReplicationStrategy.REDUNDANT,&#10;            replication_factor=5&#10;        )&#10;    ]&#10;    return jobs&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Demo usage&#10;    demo_jobs = create_demo_jobs()&#10;&#10;    # Create replicator with nodes&#10;    replicator = Replicator()&#10;    for i in range(5):&#10;        node = ReplicationNode(f&quot;compute_node_{i}&quot;, lambda x: f&quot;computed_result_{i}&quot;)&#10;        node.reliability_score = 0.8 + (i * 0.05)  # Varying reliability&#10;        replicator.add_node(node)&#10;&#10;    # Test replication&#10;    print(&quot;Testing job replication...&quot;)&#10;    for job in demo_jobs:&#10;        success = replicator.replicate(job)&#10;        status = replicator.get_replication_status(job.id)&#10;        print(f&quot;Job {job.id}: {'SUCCESS' if success else 'FAILED'}&quot;)&#10;        if status:&#10;            print(f&quot;  Results: {len(status)} executions&quot;)&#10;            for result in status:&#10;                print(f&quot;    Node {result.node_id}: {result.status.value}&quot;)&#10;&#10;    # Cleanup&#10;    replicator.cleanup_history()&#10;    print(&quot;Replication history cleaned up.&quot;)&#10;    replicate_data()  # Example data replication call" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/runner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/runner.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Container runner using Docker to execute jobs in isolation.&#10;&quot;&quot;&quot;&#10;import docker&#10;from nexapod.descriptor import JobDescriptor&#10;&#10;class ContainerRunner:&#10;    &quot;&quot;&quot;Executes job containers based on job descriptors.&quot;&quot;&quot;&#10;    def __init__(self):&#10;        self.client = docker.from_env()&#10;&#10;    def run(self, desc: JobDescriptor) -&gt; dict:&#10;        &quot;&quot;&quot;Run the container and return execution status and logs.&quot;&quot;&quot;&#10;        volumes = {&#10;            host_path: {'bind': container_path, 'mode': 'rw'}&#10;            for container_path, host_path in desc.outputs.items()&#10;        }&#10;        container = self.client.containers.run(&#10;            desc.image,&#10;            detach=True,&#10;            read_only=True,&#10;            cap_drop=[&quot;ALL&quot;],&#10;            security_opt=[&quot;no-new-privileges&quot;],&#10;            volumes=volumes&#10;        )&#10;        result = container.wait()&#10;        logs = container.logs().decode()&#10;        return {&quot;status&quot;: result.get('StatusCode') == 0, &quot;logs&quot;: logs}&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#13;&#10;Container runner using Docker to execute jobs in isolation.&#13;&#10;&quot;&quot;&quot;&#13;&#10;import docker&#13;&#10;from nexapod.descriptor import JobDescriptor&#13;&#10;&#13;&#10;&#13;&#10;class ContainerRunner:&#13;&#10;    &quot;&quot;&quot;Executes job containers based on job descriptors.&quot;&quot;&quot;&#13;&#10;    def __init__(self):&#13;&#10;        self.client = docker.from_env()&#13;&#10;&#13;&#10;    def run(self, desc: JobDescriptor) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Run the container and return execution status and logs.&quot;&quot;&quot;&#13;&#10;        volumes = {&#13;&#10;            host_path: {'bind': container_path, 'mode': 'rw'}&#13;&#10;            for container_path, host_path in desc.outputs.items()&#13;&#10;        }&#13;&#10;        container = self.client.containers.run(&#13;&#10;            desc.image,&#13;&#10;            detach=True,&#13;&#10;            read_only=True,&#13;&#10;            cap_drop=[&quot;ALL&quot;],&#13;&#10;            security_opt=[&quot;no-new-privileges&quot;],&#13;&#10;            volumes=volumes&#13;&#10;        )&#13;&#10;        result = container.wait()&#13;&#10;        logs = container.logs().decode()&#13;&#10;        return {&quot;status&quot;: result.get('StatusCode') == 0, &quot;logs&quot;: logs}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/scheduler.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/scheduler.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Scheduler module for matching and executing jobs on nodes.&#10;&quot;&quot;&quot;&#10;import logging&#10;import threading&#10;import queue&#10;import time&#10;import hashlib&#10;import ast&#10;from .database import Database&#10;from .validator import validate_log, generate_signature&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;logger = logging.getLogger(__name__)&#10;&#10;job_queue = queue.Queue()&#10;&#10;class Scheduler:&#10;    &quot;&quot;&quot;Responsible for job scheduling and execution across nodes.&quot;&quot;&quot;&#10;    def __init__(self):&#10;        self.db = Database()&#10;        self.node_busy = {}&#10;&#10;    @staticmethod&#10;    def submit_job(job: dict):&#10;        &quot;&quot;&quot;Add a job to the scheduling queue.&quot;&quot;&quot;&#10;        job_queue.put(job)&#10;        logger.info(&quot;Job %s submitted to the queue.&quot;, job['id'])&#10;&#10;    def match_and_schedule(self):&#10;        &quot;&quot;&quot;Continuously match jobs to available nodes and schedule execution.&quot;&quot;&quot;&#10;        while True:&#10;            job = job_queue.get()&#10;            node1, node2 = self._find_two_nodes_for_job()&#10;            if not node1 or not node2:&#10;                logger.warning(&quot;Insufficient nodes for job %s.&quot;, job['id'])&#10;                job_queue.task_done()&#10;                continue&#10;&#10;            result1 = self._execute_job(job, node1)&#10;            result2 = self._execute_job(job, node2)&#10;&#10;            if validate_log(result1) and validate_log(result2):&#10;                if result1['hash'] == result2['hash']:&#10;                    self.db.store_job(job, result1)&#10;                else:&#10;                    logger.error(&quot;Hash mismatch for job %s.&quot;, job['id'])&#10;            else:&#10;                logger.error(&quot;Validation failed for job %s.&quot;, job['id'])&#10;            job_queue.task_done()&#10;&#10;    def _find_two_nodes_for_job(self) -&gt; tuple:&#10;        &quot;&quot;&quot;Select two available and verified nodes for a given job.&quot;&quot;&quot;&#10;        records = self.db.get_nodes()&#10;        candidates = [&#10;            rec[0]&#10;            for rec in records&#10;            if self._verify_node(rec) and self._is_node_available(rec[0])&#10;        ]&#10;        if len(candidates) &lt; 2:&#10;            return None, None&#10;        return candidates[0], candidates[1]&#10;&#10;    def _verify_node(self, node_record: tuple) -&gt; bool:&#10;        &quot;&quot;&quot;Verify node profile integrity.&quot;&quot;&quot;&#10;        try:&#10;            profile = ast.literal_eval(node_record[2])&#10;            return isinstance(profile, dict) and 'os' in profile&#10;        except Exception:&#10;            return False&#10;&#10;    def _is_node_available(self, node_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Check if the node is currently free.&quot;&quot;&quot;&#10;        return not self.node_busy.get(node_id, False)&#10;&#10;    def _execute_job(self, job: dict, node_id: str) -&gt; dict:&#10;        &quot;&quot;&quot;Execute a job on a node and return execution metadata.&quot;&quot;&quot;&#10;        self.node_busy[node_id] = True&#10;        try:&#10;            time.sleep(1)&#10;            combined = f&quot;{job['id']}_{node_id}&quot;&#10;            hash_result = hashlib.sha256(combined.encode()).hexdigest()&#10;            signature = generate_signature(str(job['id']).encode())&#10;            return {&quot;id&quot;: job['id'], &quot;hash&quot;: hash_result, &quot;signature&quot;: signature}&#10;        finally:&#10;            self.node_busy[node_id] = False&#10;&#10;def start_scheduler() -&gt; threading.Thread:&#10;    &quot;&quot;&quot;Initialize and start the scheduler in a background thread.&quot;&quot;&quot;&#10;    scheduler = Scheduler()&#10;    thread = threading.Thread(target=scheduler.match_and_schedule, daemon=True)&#10;    thread.start()&#10;    return thread&#10;&#10;def main():&#10;    &quot;&quot;&quot;Entry point to start the scheduler module.&quot;&quot;&quot;&#10;    thread = start_scheduler()&#10;    thread.join()&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#13;&#10;Scheduler module for matching and executing jobs on nodes.&#13;&#10;&quot;&quot;&quot;&#13;&#10;import logging&#13;&#10;import threading&#13;&#10;import queue&#13;&#10;import time&#13;&#10;import hashlib&#13;&#10;import ast&#13;&#10;from .database import Database&#13;&#10;from .validator import validate_log, generate_signature&#13;&#10;&#13;&#10;logging.basicConfig(level=logging.INFO)&#13;&#10;logger = logging.getLogger(__name__)&#13;&#10;&#13;&#10;job_queue = queue.Queue()&#13;&#10;&#13;&#10;&#13;&#10;class Scheduler:&#13;&#10;    &quot;&quot;&quot;Responsible for job scheduling and execution across nodes.&quot;&quot;&quot;&#13;&#10;    def __init__(self):&#13;&#10;        self.db = Database()&#13;&#10;        self.node_busy = {}&#13;&#10;&#13;&#10;    @staticmethod&#13;&#10;    def submit_job(job: dict):&#13;&#10;        &quot;&quot;&quot;Add a job to the scheduling queue.&quot;&quot;&quot;&#13;&#10;        job_queue.put(job)&#13;&#10;        logger.info(&quot;Job %s submitted to the queue.&quot;, job['id'])&#13;&#10;&#13;&#10;    def match_and_schedule(self):&#13;&#10;        &quot;&quot;&quot;Continuously match jobs to available nodes and schedule execution.&quot;&quot;&quot;&#13;&#10;        while True:&#13;&#10;            job = job_queue.get()&#13;&#10;            node1, node2 = self._find_two_nodes_for_job()&#13;&#10;            if not node1 or not node2:&#13;&#10;                logger.warning(&quot;Insufficient nodes for job %s.&quot;, job['id'])&#13;&#10;                job_queue.task_done()&#13;&#10;                continue&#13;&#10;&#13;&#10;            result1 = self._execute_job(job, node1)&#13;&#10;            result2 = self._execute_job(job, node2)&#13;&#10;&#13;&#10;            if validate_log(result1) and validate_log(result2):&#13;&#10;                if result1['hash'] == result2['hash']:&#13;&#10;                    self.db.store_job(job, result1)&#13;&#10;                else:&#13;&#10;                    logger.error(&quot;Hash mismatch for job %s.&quot;, job['id'])&#13;&#10;            else:&#13;&#10;                logger.error(&quot;Validation failed for job %s.&quot;, job['id'])&#13;&#10;            job_queue.task_done()&#13;&#10;&#13;&#10;    def _find_two_nodes_for_job(self) -&gt; tuple:&#13;&#10;        &quot;&quot;&quot;Select two available and verified nodes for a given job.&quot;&quot;&quot;&#13;&#10;        records = self.db.get_nodes()&#13;&#10;        candidates = [&#13;&#10;            rec[0]&#13;&#10;            for rec in records&#13;&#10;            if self._verify_node(rec) and self._is_node_available(rec[0])&#13;&#10;        ]&#13;&#10;        if len(candidates) &lt; 2:&#13;&#10;            return None, None&#13;&#10;        return candidates[0], candidates[1]&#13;&#10;&#13;&#10;    def _verify_node(self, node_record: tuple) -&gt; bool:&#13;&#10;        &quot;&quot;&quot;Verify node profile integrity.&quot;&quot;&quot;&#13;&#10;        try:&#13;&#10;            profile = ast.literal_eval(node_record[2])&#13;&#10;            return isinstance(profile, dict) and 'os' in profile&#13;&#10;        except Exception:&#13;&#10;            return False&#13;&#10;&#13;&#10;    def _is_node_available(self, node_id: str) -&gt; bool:&#13;&#10;        &quot;&quot;&quot;Check if the node is currently free.&quot;&quot;&quot;&#13;&#10;        return not self.node_busy.get(node_id, False)&#13;&#10;&#13;&#10;    def _execute_job(self, job: dict, node_id: str) -&gt; dict:&#13;&#10;        &quot;&quot;&quot;Execute a job on a node and return execution metadata.&quot;&quot;&quot;&#13;&#10;        self.node_busy[node_id] = True&#13;&#10;        try:&#13;&#10;            time.sleep(1)&#13;&#10;            combined = f&quot;{job['id']}_{node_id}&quot;&#13;&#10;            hash_result = hashlib.sha256(combined.encode()).hexdigest()&#13;&#10;            signature = generate_signature(str(job['id']).encode())&#13;&#10;            return {&quot;id&quot;: job['id'], &quot;hash&quot;: hash_result,&#13;&#10;                    &quot;signature&quot;: signature}&#13;&#10;        finally:&#13;&#10;            self.node_busy[node_id] = False&#13;&#10;&#13;&#10;&#13;&#10;def start_scheduler() -&gt; threading.Thread:&#13;&#10;    &quot;&quot;&quot;Initialize and start the scheduler in a background thread.&quot;&quot;&quot;&#13;&#10;    scheduler = Scheduler()&#13;&#10;    thread = threading.Thread(target=scheduler.match_and_schedule,&#13;&#10;                              daemon=True)&#13;&#10;    thread.start()&#13;&#10;    return thread&#13;&#10;&#13;&#10;&#13;&#10;def main():&#13;&#10;    &quot;&quot;&quot;Entry point to start the scheduler module.&quot;&quot;&quot;&#13;&#10;    thread = start_scheduler()&#13;&#10;    thread.join()&#13;&#10;&#13;&#10;&#13;&#10;if __name__ == '__main__':&#13;&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Infrastruture/validator.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Infrastruture/validator.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Module for HMAC-based signature generation and log validation.&#10;&quot;&quot;&quot;&#10;&#10;import hashlib&#10;import hmac&#10;&#10;SECRET_KEY = b'supersecret'&#10;&#10;def generate_signature(message: bytes) -&gt; str:&#10;    &quot;&quot;&quot;Generate HMAC-SHA256 signature for a message.&quot;&quot;&quot;&#10;    return hmac.new(SECRET_KEY, message, hashlib.sha256).hexdigest()&#10;&#10;def validate_log(log: dict) -&gt; bool:&#10;    &quot;&quot;&quot;Validate the signature of a log entry.&quot;&quot;&quot;&#10;    expected = generate_signature(str(log.get('id', '')).encode())&#10;    return log.get('signature') == expected&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Module for HMAC-based signature generation and log validation.&#10;&quot;&quot;&quot;&#10;&#10;import hashlib&#10;import hmac&#10;&#10;SECRET_KEY = b'supersecret'&#10;&#10;&#10;def generate_signature(message: bytes) -&gt; str:&#10;    &quot;&quot;&quot;Generate HMAC-SHA256 signature for a message.&quot;&quot;&quot;&#10;    return hmac.new(SECRET_KEY, message, hashlib.sha256).hexdigest()&#10;&#10;&#10;def validate_log(log: dict) -&gt; bool:&#10;    &quot;&quot;&quot;Validate the signature of a log entry.&quot;&quot;&quot;&#10;    expected = generate_signature(str(log.get('id', '')).encode())&#10;    return log.get('signature') == expected" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Protocol/incentives/bounties.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Protocol/incentives/bounties.py" />
              <option name="originalContent" value="# Coming soon&#10;&#10;" />
              <option name="updatedContent" value="# Coming soon" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Protocol/incentives/credits.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Protocol/incentives/credits.py" />
              <option name="originalContent" value="# Coming Soon&#10;&#10;" />
              <option name="updatedContent" value="# Coming Soon" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Protocol/protocol.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Protocol/protocol.py" />
              <option name="originalContent" value="import pydantic&#10;&#10;class NodeProfile(pydantic.BaseModel):&#10;    cpu: str&#10;    cores: int&#10;    threads: int&#10;    ram_gb: float&#10;    os: str&#10;    gpu: list&#10;&#10;class JobRequest(pydantic.BaseModel):&#10;    node_id: str&#10;&#10;class JobDescriptor(pydantic.BaseModel):&#10;    schema_version: str&#10;    job_id: str&#10;    type: str&#10;    input_files: list&#10;    docker_image: str&#10;    estimated_flops: float&#10;    tier: int&#10;    requirements: dict&#10;    input_uri: str&#10;    tolerance: float&#10;    credit_rate: float&#10;&#10;class JobResult(pydantic.BaseModel):&#10;    job_id: str&#10;    node_id: str&#10;    output: str&#10;    status: str&#10;    timestamp: int&#10;    sha256: str&#10;    signature: str&#10;&#10;" />
              <option name="updatedContent" value="import pydantic&#13;&#10;&#13;&#10;&#13;&#10;class NodeProfile(pydantic.BaseModel):&#13;&#10;    cpu: str&#13;&#10;    cores: int&#13;&#10;    threads: int&#13;&#10;    ram_gb: float&#13;&#10;    os: str&#13;&#10;    gpu: list&#13;&#10;&#13;&#10;&#13;&#10;class JobRequest(pydantic.BaseModel):&#13;&#10;    node_id: str&#13;&#10;&#13;&#10;&#13;&#10;class JobDescriptor(pydantic.BaseModel):&#13;&#10;    schema_version: str&#13;&#10;    job_id: str&#13;&#10;    type: str&#13;&#10;    input_files: list&#13;&#10;    docker_image: str&#13;&#10;    estimated_flops: float&#13;&#10;    tier: int&#13;&#10;    requirements: dict&#13;&#10;    input_uri: str&#13;&#10;    tolerance: float&#13;&#10;    credit_rate: float&#13;&#10;&#13;&#10;&#13;&#10;class JobResult(pydantic.BaseModel):&#13;&#10;    job_id: str&#13;&#10;    node_id: str&#13;&#10;    output: str&#13;&#10;    status: str&#13;&#10;    timestamp: int&#13;&#10;    sha256: str&#13;&#10;    signature: str" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Protocol/security/security.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Protocol/security/security.py" />
              <option name="originalContent" value="from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey&#10;from cryptography.hazmat.primitives import serialization, hashes&#10;&#10;def generate_node_keypair():&#10;    sk = Ed25519PrivateKey.generate()&#10;    pk = sk.public_key()&#10;    # ...serialize &amp; store in DB...&#10;    return sk, pk&#10;&#10;def sign_log(sk, message: bytes) -&gt; bytes:&#10;    return sk.sign(message)&#10;&#10;" />
              <option name="updatedContent" value="from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey&#10;&#10;&#10;def generate_node_keypair():&#10;    sk = Ed25519PrivateKey.generate()&#10;    pk = sk.public_key()&#10;    # ...serialize &amp; store in DB...&#10;    return sk, pk&#10;&#10;&#10;def sign_log(sk, message: bytes) -&gt; bytes:&#10;    return sk.sign(message)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="&lt;!-- Tags: #DistributedComputing, #ScientificComputing, #HeterogeneousResources, #ScalableArchitecture, #Innovation, #OpenSource --&gt;&#10;&#10;# NEXAPod: Distributed Compute Fabric for Scientific Problems&#10;&#10;[![CI/CD Pipeline](https://github.com/DarkStarStrix/NexaPod/actions/workflows/ci.yml/badge.svg)](https://github.com/DarkStarStrix/NexaPod/actions/workflows/ci.yml)&#10;[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&#10;&#10;*NEXAPod seamlessly unites diverse computing resources—from consumer GPUs to high-end clusters—to tackle large-scale scientific challenges.*&#10;&#10;---&#10;&#10;## 1. Mission&#10;&#10;**NEXAPod** is a distributed computing system designed to coordinate heterogeneous compute resources—ranging from consumer GPUs to high-performance clusters—to solve large-scale scientific problems. It is, in essence, **Folding@home for the AI era.**&#10;&#10;---&#10;&#10;## 2. Frequently Asked Questions (FAQ)&#10;&#10;**What is NexaPod?**  &#10;NexaPod is a modern take on distributed scientific computing. It allows anyone to contribute their computer's idle processing power to help solve complex scientific problems, starting with molecular science and protein structure prediction.&#10;&#10;**What was the inspiration for NexaPod?**  &#10;NexaPod is a synthesis of three ideas:&#10;1.  **Decentralized Compute at Scale (e.g., Prime Intellect):** Inspired by the vision of training large AI models on a decentralized network of nodes.&#10;2.  **Mesh Networking (e.g., Meshtastic):** Built on the concept of a resilient, decentralized network of peers.&#10;3.  **Scientific Mission (e.g., Folding@home):** Focused on applying this compute power to solve real-world scientific challenges.&#10;&#10;**Is this project affiliated with Prime Intellect?**  &#10;No. NexaPod is an independent, open-source project. While inspired by the ambitious goals of projects like Prime Intellect, it is not formally associated with them. NexaPod's focus is on scientific computing and inference, not general-purpose LLM training.&#10;&#10;**How is NexaPod different from Folding@home?**  &#10;NexaPod aims to be a modern successor. Key differences include:&#10;-   **AI-Native:** Designed for modern machine learning inference tasks.&#10;-   **Heterogeneous Compute:** Built from the ground up to support diverse hardware (CPU, GPU).&#10;-   **Job Agnostic:** The architecture can be adapted to any scientific problem, not just a single one.&#10;-   **Modern Tooling:** Uses containers, modern CI/CD, and robust orchestration for security and scalability.&#10;&#10;---&#10;&#10;## 3. Project Roadmap&#10;&#10;### **Phase 1: Alpha (Launched)**&#10;-   **Goal:** Ship a working proof-of-concept. Test the core system and validate that the distributed mesh works in the wild.&#10;-   **Actions:** Launched the first public alpha running a *secondary structure prediction* job. Onboarding technical users to gather feedback, observe bugs, and fix obvious blockers.&#10;&#10;### **Phase 2: Beta (Next 2–4 Weeks)**&#10;-   **Goal:** Iterate on user feedback, harden the system, and expand the network.&#10;-   **Actions:** Bugfixes and infrastructure upgrades (better logging, validation, robust VPS). Refine onboarding and documentation. Begin groundwork for ZK proofs, incentives, and improved scheduling.&#10;&#10;### **Phase 3: Full Launch (Post-Beta, ~1–2 Months Out)**&#10;-   **Goal:** A production-grade, incentivized scientific compute mesh ready to tackle a &quot;grand challenge&quot; problem.&#10;-   **Actions:** Implement **ZK proofs** for trustless validation. Roll out more robust job scheduling. Launch **incentive mechanisms** (token/reputation). Target a large-scale challenge like **DreamMS** (inference on 201 million molecular datapoints).&#10;&#10;---&#10;&#10;## 4. Deployment (CI/CD)&#10;&#10;This project uses a GitHub Actions workflow for continuous integration and deployment. The pipeline is defined in `.github/workflows/ci.yml` and consists of three stages:&#10;&#10;1.  **Test:** Runs automated tests to ensure code quality.&#10;2.  **Build &amp; Push:** Builds the `server` and `client` Docker images and pushes them to GitHub Container Registry (GHCR). This makes the client available for public download.&#10;3.  **Deploy:** Automatically deploys the latest server image to a DigitalOcean VPS.&#10;&#10;### DigitalOcean Server Setup&#10;&#10;To enable automated deployments, you need to set up a DigitalOcean droplet and add the following secrets to your GitHub repository's &quot;Actions secrets&quot; settings:&#10;&#10;-   `DO_HOST`: The public IP address of your droplet.&#10;-   `DO_USERNAME`: The username for SSH access (e.g., `root`).&#10;-   `DO_SSH_KEY`: The private SSH key used to access your droplet.&#10;-   `GITHUB_TOKEN`: A GitHub Personal Access Token with `read:packages` and `write:packages` scopes to allow the droplet to pull images from GHCR.&#10;&#10;The deployment script on the server will handle stopping the old container, pulling the new image, and starting the updated container.&#10;&#10;---&#10;&#10;## 5. Getting Started&#10;&#10;There are two main ways to run the project:&#10;&#10;### For Contributors (Joining the Public Mesh)&#10;To contribute your compute power to the public network, please follow the official guide:&#10;-   **[Docs/ONBOARDING.md](Docs/ONBOARDING.md)**&#10;&#10;This uses the `nexapod` CLI to securely download and run the client.&#10;&#10;### For Developers (Running Locally)&#10;To run the entire stack (server, client, monitoring) locally for development:&#10;1.  **Prerequisites:** Docker and Docker Compose.&#10;2.  **Install Dependencies:**&#10;    ```bash&#10;    pip install -r requirements.txt&#10;    ```&#10;3.  **Launch Services:**&#10;    ```bash&#10;    docker-compose up --build -d&#10;    ```&#10;This will start the server, a local client, Prometheus, and Grafana.&#10;&#10;---&#10;&#10;## 6. Project Structure&#10;&#10;```&#10;nexapod/&#10;├── client/           # Worker node agent code&#10;├── server/           # Coordinator server (API, scheduler)&#10;├── Infrastructure/   # Dockerfiles and Kubernetes manifests&#10;├── docs/             # Architecture, API, and Onboarding documentation&#10;├── scripts/          # Utility scripts&#10;├── tests/            # Unit and integration tests&#10;├── .github/          # CI/CD workflows&#10;├── nexapod           # The user-facing CLI tool&#10;└── docker-compose.yaml # Local development setup&#10;```&#10;&#10;---&#10;&#10;## 7. Core Components &amp; Tech Stack&#10;&#10;| Layer                | Component                | Tech / Libs                                           |&#10;|----------------------|--------------------------|-------------------------------------------------------|&#10;| **Comms**            | HTTP API                 | FastAPI (server) + requests (client)                  |&#10;| **Profiling**        | Hardware detection       | `psutil`, `nvidia-ml-py`, `subprocess` (`nvidia-smi`) |&#10;| **Execution**        | Container runtime        | Docker (`nexapod` CLI)                                |&#10;| **Scheduling**       | Job queue &amp; matching     | In-memory queue (Alpha)                               |&#10;| **Data storage**     | Metadata &amp; logs          | SQLite (Alpha) → Postgres                             |&#10;| **Security**         | Cryptographic signatures | `cryptography` (Ed25519)                              |&#10;| **Orchestration**    | Single-node MVP          | Python scripts + Docker                               |&#10;|                      | Multi-node (v2)          | Kubernetes (k8s) manifests                            |&#10;| **Monitoring**       | Metrics &amp; logs           | Prometheus / Grafana                                  |&#10;| **Testing**          | Unit &amp; integration tests | pytest                                                |&#10;&#10;---&#10;&#10;## 8. Contributing&#10;&#10;PRs and issues are welcome! See **[Docs/CONTRIBUTING.md](Docs/CONTRIBUTING.md)** for detailed guidelines.&#10;&#10;---&#10;&#10;## 9. License&#10;&#10;MIT License. See [LICENSE](LICENSE).&#10;" />
              <option name="updatedContent" value="&lt;!-- Tags: #DistributedComputing, #ScientificComputing, #HeterogeneousResources, #ScalableArchitecture, #Innovation, #OpenSource --&gt;&#10;&#10;# NEXAPod: Distributed Compute Fabric for Scientific Problems&#10;&#10;[![CI/CD Pipeline](https://github.com/DarkStarStrix/NexaPod/actions/workflows/ci.yml/badge.svg)](https://github.com/DarkStarStrix/NexaPod/actions/workflows/ci.yml)&#10;[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&#10;&#10;*NEXAPod seamlessly unites diverse computing resources—from consumer GPUs to high-end clusters—to tackle large-scale scientific challenges.*&#10;&#10;---&#10;&#10;## 1. Mission&#10;&#10;**NEXAPod** is a distributed computing system designed to coordinate heterogeneous compute resources—ranging from consumer GPUs to high-performance clusters—to solve large-scale scientific problems. It is, in essence, **Folding@home for the AI era.**&#10;&#10;---&#10;&#10;## 2. Frequently Asked Questions (FAQ)&#10;&#10;**What is NexaPod?**  &#10;NexaPod is a modern take on distributed scientific computing. It allows anyone to contribute their computer's idle processing power to help solve complex scientific problems, starting with molecular science and protein structure prediction.&#10;&#10;**What was the inspiration for NexaPod?**  &#10;NexaPod is a synthesis of three ideas:&#10;1.  **Decentralized Compute at Scale (e.g., Prime Intellect):** Inspired by the vision of training large AI models on a decentralized network of nodes.&#10;2.  **Mesh Networking (e.g., Meshtastic):** Built on the concept of a resilient, decentralized network of peers.&#10;3.  **Scientific Mission (e.g., Folding@home):** Focused on applying this compute power to solve real-world scientific challenges.&#10;&#10;**Is this project affiliated with Prime Intellect?**  &#10;No. NexaPod is an independent, open-source project. While inspired by the ambitious goals of projects like Prime Intellect, it is not formally associated with them. NexaPod's focus is on scientific computing and inference, not general-purpose LLM training.&#10;&#10;**How is NexaPod different from Folding@home?**  &#10;NexaPod aims to be a modern successor. Key differences include:&#10;-   **AI-Native:** Designed for modern machine learning inference tasks.&#10;-   **Heterogeneous Compute:** Built from the ground up to support diverse hardware (CPU, GPU).&#10;-   **Job Agnostic:** The architecture can be adapted to any scientific problem, not just a single one.&#10;-   **Modern Tooling:** Uses containers, modern CI/CD, and robust orchestration for security and scalability.&#10;&#10;---&#10;&#10;## 3. Project Roadmap&#10;&#10;### **Phase 1: Alpha (Launched)**&#10;-   **Goal:** Ship a working proof-of-concept. Test the core system and validate that the distributed mesh works in the wild.&#10;-   **Actions:** Launched the first public alpha running a *secondary structure prediction* job. Onboarding technical users to gather feedback, observe bugs, and fix obvious blockers.&#10;&#10;### **Phase 2: Beta (Next 2–4 Weeks)**&#10;-   **Goal:** Iterate on user feedback, harden the system, and expand the network.&#10;-   **Actions:** Bugfixes and infrastructure upgrades (better logging, validation, robust VPS). Refine onboarding and documentation. Begin groundwork for ZK proofs, incentives, and improved scheduling.&#10;&#10;### **Phase 3: Full Launch (Post-Beta, ~1–2 Months Out)**&#10;-   **Goal:** A production-grade, incentivized scientific compute mesh ready to tackle a &quot;grand challenge&quot; problem.&#10;-   **Actions:** Implement **ZK proofs** for trustless validation. Roll out more robust job scheduling. Launch **incentive mechanisms** (token/reputation). Target a large-scale challenge like **DreamMS** (inference on 201 million molecular datapoints).&#10;&#10;---&#10;&#10;## 4. Deployment (CI/CD)&#10;&#10;This project uses a GitHub Actions workflow for continuous integration and deployment. The pipeline is defined in `.github/workflows/ci.yml` and consists of three stages:&#10;&#10;1.  **Test:** Runs automated tests to ensure code quality.&#10;2.  **Build &amp; Push:** Builds the `server` and `client` Docker images and pushes them to GitHub Container Registry (GHCR). This makes the client available for public download.&#10;3.  **Deploy:** Automatically deploys the latest server image to a DigitalOcean VPS.&#10;&#10;### DigitalOcean Server Setup&#10;&#10;To enable automated deployments, you need to set up a DigitalOcean droplet and add the following secrets to your GitHub repository's &quot;Actions secrets&quot; settings:&#10;&#10;-   `DO_HOST`: The public IP address of your droplet.&#10;-   `DO_USERNAME`: The username for SSH access (e.g., `root`).&#10;-   `DO_SSH_KEY`: The private SSH key used to access your droplet.&#10;-   `DEPLOY_TOKEN`: A GitHub Personal Access Token (classic) with `read:packages` scope. This is used by the server to pull images from GHCR and is more reliable for deployment than the default short-lived token.&#10;&#10;The deployment script on the server will handle stopping the old container, pulling the new image, and starting the updated container.&#10;&#10;---&#10;&#10;## 5. Getting Started&#10;&#10;There are two main ways to run the project:&#10;&#10;### For Contributors (Joining the Public Mesh)&#10;To contribute your compute power to the public network, please follow the official guide:&#10;-   **[Docs/ONBOARDING.md](Docs/ONBOARDING.md)**&#10;&#10;This uses the `nexapod` CLI to securely download and run the client.&#10;&#10;### For Developers (Running Locally)&#10;To run the entire stack (server, client, monitoring) locally for development:&#10;1.  **Prerequisites:** Docker and Docker Compose.&#10;2.  **Install Dependencies:**&#10;    ```bash&#10;    pip install -r requirements.txt&#10;    ```&#10;3.  **Launch Services:**&#10;    ```bash&#10;    docker-compose up --build -d&#10;    ```&#10;This will start the server, a local client, Prometheus, and Grafana.&#10;&#10;---&#10;&#10;## 6. Project Structure&#10;&#10;```&#10;nexapod/&#10;├── client/           # Worker node agent code&#10;├── server/           # Coordinator server (API, scheduler)&#10;├── Infrastructure/   # Dockerfiles and Kubernetes manifests&#10;├── docs/             # Architecture, API, and Onboarding documentation&#10;├── scripts/          # Utility scripts&#10;├── tests/            # Unit and integration tests&#10;├── .github/          # CI/CD workflows&#10;├── nexapod           # The user-facing CLI tool&#10;└── docker-compose.yaml # Local development setup&#10;```&#10;&#10;---&#10;&#10;## 7. Core Components &amp; Tech Stack&#10;&#10;| Layer                | Component                | Tech / Libs                                           |&#10;|----------------------|--------------------------|-------------------------------------------------------|&#10;| **Comms**            | HTTP API                 | FastAPI (server) + requests (client)                  |&#10;| **Profiling**        | Hardware detection       | `psutil`, `nvidia-ml-py`, `subprocess` (`nvidia-smi`) |&#10;| **Execution**        | Container runtime        | Docker (`nexapod` CLI)                                |&#10;| **Scheduling**       | Job queue &amp; matching     | In-memory queue (Alpha)                               |&#10;| **Data storage**     | Metadata &amp; logs          | SQLite (Alpha) → Postgres                             |&#10;| **Security**         | Cryptographic signatures | `cryptography` (Ed25519)                              |&#10;| **Orchestration**    | Single-node MVP          | Python scripts + Docker                               |&#10;|                      | Multi-node (v2)          | Kubernetes (k8s) manifests                            |&#10;| **Monitoring**       | Metrics &amp; logs           | Prometheus / Grafana                                  |&#10;| **Testing**          | Unit &amp; integration tests | pytest                                                |&#10;&#10;---&#10;&#10;## 8. Contributing&#10;&#10;PRs and issues are welcome! See **[Docs/CONTRIBUTING.md](Docs/CONTRIBUTING.md)** for detailed guidelines.&#10;&#10;---&#10;&#10;## 9. License&#10;&#10;MIT License. See [LICENSE](LICENSE)." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/RELEASE_NOTES.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/RELEASE_NOTES.md" />
              <option name="updatedContent" value="# How to Create a Release&#10;&#10;This project uses a CI/CD pipeline that automatically creates a GitHub Release when a new version tag is pushed to the repository.&#10;&#10;## Releasing Alpha v1&#10;&#10;To create the `v1.0.0-alpha` release, follow these steps:&#10;&#10;1.  Ensure your `main` branch is up-to-date with all the changes you want to include in the release.&#10;&#10;    ```bash&#10;    git checkout main&#10;    git pull origin main&#10;    ```&#10;&#10;2.  Create a new git tag. For the alpha, we will use a pre-release identifier.&#10;&#10;    ```bash&#10;    git tag -a v1.0.0-alpha -m &quot;NEXAPod Alpha v1 Release&quot;&#10;    ```&#10;&#10;3.  Push the tag to the GitHub repository.&#10;&#10;    ```bash&#10;    git push origin v1.0.0-alpha&#10;    ```&#10;&#10;Once the tag is pushed, the `release` job in the `.github/workflows/ci.yml` pipeline will trigger. It will automatically:&#10;- Build and publish the Docker images.&#10;- Create a new &quot;pre-release&quot; on GitHub with the tag `v1.0.0-alpha`.&#10;- Populate the release notes with a link to the onboarding documentation.&#10;&#10;You can then go to the &quot;Releases&quot; section of the GitHub repository to see the newly created release.&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Server/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Server/app.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod server coordinator.&#10;&quot;&quot;&quot;&#10;import os&#10;import json&#10;import yaml&#10;import uvicorn&#10;from fastapi import FastAPI, Request, Response, HTTPException&#10;from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey&#10;from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST&#10;from Server.scheduler import Scheduler&#10;from Server.db import DB&#10;from Server.reputation import Reputation&#10;from Infrastructure.output_validator import load_checker&#10;&#10;&#10;CONFIG_PATH = os.path.join(os.path.dirname(__file__), &quot;config.yaml&quot;)&#10;&#10;&#10;def load_config() -&gt; dict:&#10;    &quot;&quot;&quot;Load and return the server configuration from YAML.&quot;&quot;&quot;&#10;    with open(CONFIG_PATH, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def create_app() -&gt; FastAPI:&#10;    &quot;&quot;&quot;Create and configure the FastAPI application.&quot;&quot;&quot;&#10;    config = load_config()&#10;    validator = load_checker(config[&quot;validator_plugin&quot;])&#10;    quorum = config.get(&quot;quorum&quot;, 1)&#10;    db = DB(config)&#10;    scheduler = Scheduler(db, config)&#10;    reputation = Reputation(db, config)&#10;    app = FastAPI()&#10;&#10;    node_register_counter = Counter(&#10;        &quot;nexapod_node_register_total&quot;, &quot;Total number of node registrations&quot;&#10;    )&#10;    job_assigned_counter = Counter(&#10;        &quot;nexapod_job_assigned_total&quot;, &quot;Total number of jobs assigned&quot;&#10;    )&#10;    job_result_success_counter = Counter(&#10;        &quot;nexapod_job_result_success_total&quot;,&#10;        &quot;Total number of successful job results&quot;&#10;    )&#10;    job_result_failure_counter = Counter(&#10;        &quot;nexapod_job_result_failure_total&quot;,&#10;        &quot;Total number of failed job results&quot;&#10;    )&#10;    job_submitted_counter = Counter(&#10;        &quot;nexapod_job_submitted_total&quot;,&#10;        &quot;Total number of jobs submitted&quot;&#10;    )&#10;&#10;    @app.post(&quot;/register&quot;)&#10;    async def register_node(request: Request):&#10;        &quot;&quot;&quot;Verify node signature and register profile.&quot;&quot;&quot;&#10;        payload = await request.json()&#10;        signature_hex = payload.pop(&quot;signature&quot;, None)&#10;        public_key_hex = payload.pop(&quot;public_key&quot;, None)&#10;        if not signature_hex or not public_key_hex:&#10;            raise HTTPException(&#10;                status_code=400, detail=&quot;Missing signature or public_key&quot;&#10;            )&#10;        message = json.dumps(payload, sort_keys=True).encode()&#10;        public_key_bytes = bytes.fromhex(public_key_hex)&#10;        public_key = Ed25519PublicKey.from_public_bytes(public_key_bytes)&#10;        try:&#10;            public_key.verify(bytes.fromhex(signature_hex), message)&#10;        except Exception:&#10;            raise HTTPException(status_code=400, detail=&quot;Invalid signature&quot;)&#10;        payload[&quot;public_key&quot;] = public_key_hex&#10;        node_id = db.register_node(payload)&#10;        node_register_counter.inc()&#10;        return {&quot;node_id&quot;: node_id}&#10;&#10;    @app.get(&quot;/job&quot;)&#10;    async def get_job(node_id: str):&#10;        &quot;&quot;&quot;Assign and return a pending job for the given node.&quot;&quot;&quot;&#10;        job = scheduler.assign_job(node_id)&#10;        if job:&#10;            job_assigned_counter.inc()&#10;        return job or {}&#10;&#10;    @app.post(&quot;/result&quot;)&#10;    async def submit_result(request: Request):&#10;        &quot;&quot;&quot;Validate and record a job result, finalize when the quorum is reached.&quot;&quot;&quot;&#10;        result = await request.json()&#10;        try:&#10;            valid = validator(result)&#10;        except Exception as e:&#10;            job_result_failure_counter.inc()&#10;            raise HTTPException(&#10;                status_code=400, detail=f&quot;Validation error: {e}&quot;&#10;            )&#10;        if not valid:&#10;            job_result_failure_counter.inc()&#10;            raise HTTPException(&#10;                status_code=400, detail=&quot;Result validation failed&quot;&#10;            )&#10;        db.add_vote(result)&#10;        votes = db.count_votes(result[&quot;job_id&quot;], result[&quot;sha256&quot;])&#10;        if votes &gt;= quorum:&#10;            final = db.get_vote_result(result[&quot;job_id&quot;], result[&quot;sha256&quot;])&#10;            db.finalize_job(final)&#10;            reputation.update_credits(final)&#10;            job_result_success_counter.inc()&#10;            return {&quot;status&quot;: &quot;finalized&quot;, &quot;votes&quot;: votes}&#10;        return {&quot;status&quot;: &quot;vote recorded&quot;, &quot;votes&quot;: votes}&#10;&#10;    @app.post(&quot;/jobs&quot;)&#10;    async def submit_job(request: Request):&#10;        &quot;&quot;&quot;Submit a new job to the scheduling queue.&quot;&quot;&quot;&#10;        job = await request.json()&#10;        db.add_job(job)&#10;        job_submitted_counter.inc()&#10;        return {&quot;status&quot;: &quot;job added&quot;}&#10;&#10;    @app.get(&quot;/metrics&quot;)&#10;    async def metrics():&#10;        &quot;&quot;&quot;Expose Prometheus metrics endpoint.&quot;&quot;&quot;&#10;        data = generate_latest()&#10;        return Response(content=data, media_type=CONTENT_TYPE_LATEST)&#10;&#10;    @app.get(&quot;/health&quot;)&#10;    async def health():&#10;        &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;        return {&quot;status&quot;: &quot;ok&quot;}&#10;&#10;    return app&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Run the Uvicorn server.&quot;&quot;&quot;&#10;    app = create_app()&#10;    uvicorn.run(&#10;        app,&#10;        host=&quot;0.0.0.0&quot;,&#10;        port=8000&#10;    )&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;REST API for NEXAPod server coordinator.&#10;&quot;&quot;&quot;&#10;import os&#10;import json&#10;import yaml&#10;import uvicorn&#10;from fastapi import FastAPI, Request, Response, HTTPException&#10;from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey&#10;from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST&#10;from Server.scheduler import Scheduler&#10;from Server.db import DB&#10;from Server.reputation import Reputation&#10;from Infrastructure.output_validator import load_checker&#10;&#10;&#10;CONFIG_PATH = os.path.join(os.path.dirname(__file__), &quot;config.yaml&quot;)&#10;&#10;&#10;def load_config() -&gt; dict:&#10;    &quot;&quot;&quot;Load and return the server configuration from YAML.&quot;&quot;&quot;&#10;    with open(CONFIG_PATH, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def create_app() -&gt; FastAPI:&#10;    &quot;&quot;&quot;Create and configure the FastAPI application.&quot;&quot;&quot;&#10;    config = load_config()&#10;    validator = load_checker(config[&quot;validator_plugin&quot;])&#10;    quorum = config.get(&quot;quorum&quot;, 1)&#10;    db = DB(config)&#10;    scheduler = Scheduler(db, config)&#10;    reputation = Reputation(db, config)&#10;    app = FastAPI()&#10;&#10;    node_register_counter = Counter(&#10;        &quot;nexapod_node_register_total&quot;, &quot;Total number of node registrations&quot;&#10;    )&#10;    job_assigned_counter = Counter(&#10;        &quot;nexapod_job_assigned_total&quot;, &quot;Total number of jobs assigned&quot;&#10;    )&#10;    job_result_success_counter = Counter(&#10;        &quot;nexapod_job_result_success_total&quot;,&#10;        &quot;Total number of successful job results&quot;&#10;    )&#10;    job_result_failure_counter = Counter(&#10;        &quot;nexapod_job_result_failure_total&quot;,&#10;        &quot;Total number of failed job results&quot;&#10;    )&#10;    job_submitted_counter = Counter(&#10;        &quot;nexapod_job_submitted_total&quot;,&#10;        &quot;Total number of jobs submitted&quot;&#10;    )&#10;&#10;    @app.post(&quot;/register&quot;)&#10;    async def register_node(request: Request):&#10;        &quot;&quot;&quot;Verify node signature and register profile.&quot;&quot;&quot;&#10;        payload = await request.json()&#10;        signature_hex = payload.pop(&quot;signature&quot;, None)&#10;        public_key_hex = payload.pop(&quot;public_key&quot;, None)&#10;        if not signature_hex or not public_key_hex:&#10;            raise HTTPException(&#10;                status_code=400, detail=&quot;Missing signature or public_key&quot;&#10;            )&#10;        message = json.dumps(payload, sort_keys=True).encode()&#10;        public_key_bytes = bytes.fromhex(public_key_hex)&#10;        public_key = Ed25519PublicKey.from_public_bytes(public_key_bytes)&#10;        try:&#10;            public_key.verify(bytes.fromhex(signature_hex), message)&#10;        except Exception:&#10;            raise HTTPException(status_code=400, detail=&quot;Invalid signature&quot;)&#10;        payload[&quot;public_key&quot;] = public_key_hex&#10;        node_id = db.register_node(payload)&#10;        node_register_counter.inc()&#10;        return {&quot;node_id&quot;: node_id}&#10;&#10;    @app.get(&quot;/job&quot;)&#10;    async def get_job(node_id: str):&#10;        &quot;&quot;&quot;Assign and return a pending job for the given node.&quot;&quot;&quot;&#10;        job = scheduler.assign_job(node_id)&#10;        if job:&#10;            job_assigned_counter.inc()&#10;        return job or {}&#10;&#10;    @app.post(&quot;/result&quot;)&#10;    async def submit_result(request: Request):&#10;        &quot;&quot;&quot;Validate and record a job result, finalize when the quorum is reached.&quot;&quot;&quot;&#10;        result = await request.json()&#10;        try:&#10;            valid = validator(result)&#10;        except Exception as e:&#10;            job_result_failure_counter.inc()&#10;            raise HTTPException(&#10;                status_code=400, detail=f&quot;Validation error: {e}&quot;&#10;            )&#10;        if not valid:&#10;            job_result_failure_counter.inc()&#10;            raise HTTPException(&#10;                status_code=400, detail=&quot;Result validation failed&quot;&#10;            )&#10;        db.add_vote(result)&#10;        votes = db.count_votes(result[&quot;job_id&quot;], result[&quot;sha256&quot;])&#10;        if votes &gt;= quorum:&#10;            final = db.get_vote_result(result[&quot;job_id&quot;], result[&quot;sha256&quot;])&#10;            db.finalize_job(final)&#10;            reputation.update_credits(final)&#10;            job_result_success_counter.inc()&#10;            return {&quot;status&quot;: &quot;finalized&quot;, &quot;votes&quot;: votes}&#10;        return {&quot;status&quot;: &quot;vote recorded&quot;, &quot;votes&quot;: votes}&#10;&#10;    @app.post(&quot;/jobs&quot;)&#10;    async def submit_job(request: Request):&#10;        &quot;&quot;&quot;Submit a new job to the scheduling queue.&quot;&quot;&quot;&#10;        job = await request.json()&#10;        db.add_job(job)&#10;        job_submitted_counter.inc()&#10;        return {&quot;status&quot;: &quot;job added&quot;}&#10;&#10;    @app.get(&quot;/nodes&quot;)&#10;    async def get_all_nodes():&#10;        &quot;&quot;&quot;Return a list of all registered nodes for the dashboard.&quot;&quot;&quot;&#10;        return db.get_all_nodes()&#10;&#10;    @app.get(&quot;/jobs/all&quot;)&#10;    async def get_all_jobs():&#10;        &quot;&quot;&quot;Return a list of all jobs for the dashboard.&quot;&quot;&quot;&#10;        return db.get_all_jobs()&#10;&#10;    @app.get(&quot;/metrics&quot;)&#10;    async def metrics():&#10;        &quot;&quot;&quot;Expose Prometheus metrics endpoint.&quot;&quot;&quot;&#10;        data = generate_latest()&#10;        return Response(content=data, media_type=CONTENT_TYPE_LATEST)&#10;&#10;    @app.get(&quot;/health&quot;)&#10;    async def health():&#10;        &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;        return {&quot;status&quot;: &quot;ok&quot;}&#10;&#10;    return app&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Run the Uvicorn server.&quot;&quot;&quot;&#10;    app = create_app()&#10;    config = load_config()&#10;    port = int(os.environ.get(&quot;PORT&quot;, config.get(&quot;port&quot;, 8000)))&#10;    uvicorn.run(&#10;        app,&#10;        host=&quot;0.0.0.0&quot;,&#10;        port=port&#10;    )&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Server/config.yaml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Server/config.yaml" />
              <option name="originalContent" value="db_path: &quot;nexapod.db&quot;&#10;port: 8000&#10;log_level: INFO&#10;quorum: 3&#10;validator_plugin: &quot;../Infrastruture/output_validator.py&quot;&#10;" />
              <option name="updatedContent" value="db_path: &quot;nexapod.db&quot;&#10;port: 8000&#10;log_level: INFO&#10;quorum: 3&#10;validator_plugin: &quot;../Infrastructure/output_validator.py&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Server/db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Server/db.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Database layer for NEXAPod server coordinator.&#10;&quot;&quot;&quot;&#10;import sqlite3&#10;import os&#10;import json&#10;&#10;&#10;class DB:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, results, and votes.&quot;&quot;&quot;&#10;    def __init__(self, config):&#10;        &quot;&quot;&quot;Initialize database connection and ensure tables exist.&quot;&quot;&quot;&#10;        self.db_path = config.get(&quot;db_path&quot;, &quot;nexapod.db&quot;)&#10;        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)&#10;        self._init_tables()&#10;&#10;    def _init_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, results, and votes if they do not exist.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS nodes (&#10;            node_id TEXT PRIMARY KEY,&#10;            profile TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS jobs (&#10;            job_id TEXT PRIMARY KEY,&#10;            job TEXT,&#10;            assigned_node TEXT,&#10;            status TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS results (&#10;            job_id TEXT,&#10;            node_id TEXT,&#10;            result TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS votes (&#10;            job_id TEXT,&#10;            sha256 TEXT,&#10;            node_id TEXT,&#10;            result TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        self.conn.commit()&#10;&#10;    def register_node(self, profile):&#10;        &quot;&quot;&quot;Register a new node profile and return its generated node_id.&quot;&quot;&quot;&#10;        node_id = os.urandom(8).hex()&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO nodes (node_id, profile) VALUES (?, ?)&quot;,&#10;            (node_id, json.dumps(profile))&#10;        )&#10;        self.conn.commit()&#10;        return node_id&#10;&#10;    def add_job(self, job):&#10;        &quot;&quot;&quot;Insert a new job with pending status.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO jobs (job_id, job, assigned_node, status) &quot;&#10;            &quot;VALUES (?, ?, ?, ?)&quot;,&#10;            (job[&quot;job_id&quot;], json.dumps(job), None, &quot;pending&quot;)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def assign_job(self, node_id):&#10;        &quot;&quot;&quot;Assign the first pending job to the given node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job FROM jobs WHERE status = ? LIMIT 1&quot;,&#10;            (&quot;pending&quot;,)&#10;        )&#10;        row = c.fetchone()&#10;        if row:&#10;            job_id, job_json = row&#10;            c.execute(&#10;                &quot;UPDATE jobs SET assigned_node = ?, status = ? WHERE job_id = ?&quot;,&#10;                (node_id, &quot;assigned&quot;, job_id)&#10;            )&#10;            self.conn.commit()&#10;            return json.loads(job_json)&#10;        return None&#10;&#10;    def store_result(self, result):&#10;        &quot;&quot;&quot;Store job result and mark the job as completed.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO results (job_id, node_id, result) VALUES (?, ?, ?)&quot;,&#10;            (result[&quot;job_id&quot;], result.get(&quot;node_id&quot;, &quot;&quot;), json.dumps(result))&#10;        )&#10;        c.execute(&#10;            &quot;UPDATE jobs SET status = ? WHERE job_id = ?&quot;,&#10;            (&quot;completed&quot;, result[&quot;job_id&quot;])&#10;        )&#10;        self.conn.commit()&#10;&#10;    def add_vote(self, result):&#10;        &quot;&quot;&quot;Record a vote for a job result from a node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO votes (job_id, sha256, node_id, result) &quot;&#10;            &quot;VALUES (?, ?, ?, ?)&quot;,&#10;            (result[&quot;job_id&quot;], result[&quot;sha256&quot;], result.get(&quot;node_id&quot;, &quot;&quot;),&#10;             json.dumps(result))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def count_votes(self, job_id, sha256):&#10;        &quot;&quot;&quot;Count distinct votes for a given job_id and hash.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT COUNT(DISTINCT node_id) FROM votes &quot;&#10;            &quot;WHERE job_id = ? AND sha256 = ?&quot;,&#10;            (job_id, sha256)&#10;        )&#10;        row = c.fetchone()&#10;        return row[0] if row else 0&#10;&#10;    def get_vote_result(self, job_id, sha256):&#10;        &quot;&quot;&quot;Retrieve the recorded result for a job and hash.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT result FROM votes &quot;&#10;            &quot;WHERE job_id = ? AND sha256 = ? LIMIT 1&quot;,&#10;            (job_id, sha256)&#10;        )&#10;        row = c.fetchone()&#10;        return json.loads(row[0]) if row else None&#10;&#10;    def finalize_job(self, result):&#10;        &quot;&quot;&quot;Finalize a job by storing its final result.&quot;&quot;&quot;&#10;        self.store_result(result)&#10;&#10;    def get_node_profile(self, node_id):&#10;        &quot;&quot;&quot;Retrieve stored node profile for a given node_id.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT profile FROM nodes WHERE node_id = ?&quot;,&#10;            (node_id,)&#10;        )&#10;        row = c.fetchone()&#10;        return json.loads(row[0]) if row else None&#10;&#10;    def get_pending_jobs(self):&#10;        &quot;&quot;&quot;Return a list of (job_id, job dict) for all pending jobs.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job FROM jobs WHERE status = ?&quot;,&#10;            (&quot;pending&quot;,)&#10;        )&#10;        rows = c.fetchall()&#10;        return [(job_id, json.loads(job_json)) for job_id, job_json in rows]&#10;&#10;    def get_all_jobs(self):&#10;        &quot;&quot;&quot;Return a list of all jobs.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job, assigned_node, status FROM jobs&quot;&#10;        )&#10;        rows = c.fetchall()&#10;        return [&#10;            {&#10;                &quot;job_id&quot;: row[0],&#10;                &quot;job&quot;: json.loads(row[1]),&#10;                &quot;assigned_node&quot;: row[2],&#10;                &quot;status&quot;: row[3],&#10;            }&#10;            for row in rows&#10;        ]&#10;&#10;    def assign_job_to_node(self, job_id, node_id):&#10;        &quot;&quot;&quot;Mark a job as assigned to a specific node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;UPDATE jobs SET assigned_node = ?, status = ? WHERE job_id = ?&quot;,&#10;            (node_id, &quot;assigned&quot;, job_id)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_all_nodes(self):&#10;        &quot;&quot;&quot;Return a list of all registered nodes.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&quot;SELECT node_id, profile FROM nodes&quot;)&#10;        rows = c.fetchall()&#10;        return [&#10;            {&#10;                &quot;node_id&quot;: row[0],&#10;                &quot;profile&quot;: json.loads(row[1]),&#10;            }&#10;            for row in rows&#10;        ]&#10;    &#10;    def close(self):&#10;        &quot;&quot;&quot;Close the database connection.&quot;&quot;&quot;&#10;        self.conn.close()&#10;        if os.path.exists(self.db_path):&#10;            os.remove(self.db_path)&#10;        &quot;&quot;&quot;Remove the database file if it exists.&quot;&quot;&quot;&#10;        if os.path.exists(self.db_path):&#10;            os.remove(self.db_path)&#10;        self.conn = None&#10;        &quot;&quot;&quot;Close the database connection.&quot;&quot;&quot;&#10;        self.conn.close()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Database layer for NEXAPod server coordinator.&#10;&quot;&quot;&quot;&#10;import sqlite3&#10;import os&#10;import json&#10;&#10;&#10;class DB:&#10;    &quot;&quot;&quot;Handles persistence of nodes, jobs, results, and votes.&quot;&quot;&quot;&#10;    def __init__(self, config):&#10;        &quot;&quot;&quot;Initialize database connection and ensure tables exist.&quot;&quot;&quot;&#10;        self.db_path = config.get(&quot;db_path&quot;, &quot;nexapod.db&quot;)&#10;        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)&#10;        self._init_tables()&#10;&#10;    def _init_tables(self):&#10;        &quot;&quot;&quot;Create tables for nodes, jobs, results, and votes if they do not exist.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS nodes (&#10;            node_id TEXT PRIMARY KEY,&#10;            profile TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS jobs (&#10;            job_id TEXT PRIMARY KEY,&#10;            job TEXT,&#10;            assigned_node TEXT,&#10;            status TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS results (&#10;            job_id TEXT,&#10;            node_id TEXT,&#10;            result TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        c.execute(&#10;            &quot;&quot;&quot;CREATE TABLE IF NOT EXISTS votes (&#10;            job_id TEXT,&#10;            sha256 TEXT,&#10;            node_id TEXT,&#10;            result TEXT&#10;        )&quot;&quot;&quot;&#10;        )&#10;        self.conn.commit()&#10;&#10;    def register_node(self, profile):&#10;        &quot;&quot;&quot;Register a new node profile and return its generated node_id.&quot;&quot;&quot;&#10;        node_id = os.urandom(8).hex()&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO nodes (node_id, profile) VALUES (?, ?)&quot;,&#10;            (node_id, json.dumps(profile))&#10;        )&#10;        self.conn.commit()&#10;        return node_id&#10;&#10;    def add_job(self, job):&#10;        &quot;&quot;&quot;Insert a new job with pending status.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO jobs (job_id, job, assigned_node, status) &quot;&#10;            &quot;VALUES (?, ?, ?, ?)&quot;,&#10;            (job[&quot;job_id&quot;], json.dumps(job), None, &quot;pending&quot;)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def assign_job(self, node_id):&#10;        &quot;&quot;&quot;Assign the first pending job to the given node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job FROM jobs WHERE status = ? LIMIT 1&quot;,&#10;            (&quot;pending&quot;,)&#10;        )&#10;        row = c.fetchone()&#10;        if row:&#10;            job_id, job_json = row&#10;            c.execute(&#10;                &quot;UPDATE jobs SET assigned_node = ?, status = ? WHERE job_id = ?&quot;,&#10;                (node_id, &quot;assigned&quot;, job_id)&#10;            )&#10;            self.conn.commit()&#10;            return json.loads(job_json)&#10;        return None&#10;&#10;    def store_result(self, result):&#10;        &quot;&quot;&quot;Store job result and mark the job as completed.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO results (job_id, node_id, result) VALUES (?, ?, ?)&quot;,&#10;            (result[&quot;job_id&quot;], result.get(&quot;node_id&quot;, &quot;&quot;), json.dumps(result))&#10;        )&#10;        c.execute(&#10;            &quot;UPDATE jobs SET status = ? WHERE job_id = ?&quot;,&#10;            (&quot;completed&quot;, result[&quot;job_id&quot;])&#10;        )&#10;        self.conn.commit()&#10;&#10;    def add_vote(self, result):&#10;        &quot;&quot;&quot;Record a vote for a job result from a node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;INSERT INTO votes (job_id, sha256, node_id, result) &quot;&#10;            &quot;VALUES (?, ?, ?, ?)&quot;,&#10;            (result[&quot;job_id&quot;], result[&quot;sha256&quot;], result.get(&quot;node_id&quot;, &quot;&quot;),&#10;             json.dumps(result))&#10;        )&#10;        self.conn.commit()&#10;&#10;    def count_votes(self, job_id, sha256):&#10;        &quot;&quot;&quot;Count distinct votes for a given job_id and hash.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT COUNT(DISTINCT node_id) FROM votes &quot;&#10;            &quot;WHERE job_id = ? AND sha256 = ?&quot;,&#10;            (job_id, sha256)&#10;        )&#10;        row = c.fetchone()&#10;        return row[0] if row else 0&#10;&#10;    def get_vote_result(self, job_id, sha256):&#10;        &quot;&quot;&quot;Retrieve the recorded result for a job and hash.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT result FROM votes &quot;&#10;            &quot;WHERE job_id = ? AND sha256 = ? LIMIT 1&quot;,&#10;            (job_id, sha256)&#10;        )&#10;        row = c.fetchone()&#10;        return json.loads(row[0]) if row else None&#10;&#10;    def finalize_job(self, result):&#10;        &quot;&quot;&quot;Finalize a job by storing its final result.&quot;&quot;&quot;&#10;        self.store_result(result)&#10;&#10;    def get_node_profile(self, node_id):&#10;        &quot;&quot;&quot;Retrieve stored node profile for a given node_id.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT profile FROM nodes WHERE node_id = ?&quot;,&#10;            (node_id,)&#10;        )&#10;        row = c.fetchone()&#10;        return json.loads(row[0]) if row else None&#10;&#10;    def get_pending_jobs(self):&#10;        &quot;&quot;&quot;Return a list of (job_id, job dict) for all pending jobs.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job FROM jobs WHERE status = ?&quot;,&#10;            (&quot;pending&quot;,)&#10;        )&#10;        rows = c.fetchall()&#10;        return [(job_id, json.loads(job_json)) for job_id, job_json in rows]&#10;&#10;    def get_all_jobs(self):&#10;        &quot;&quot;&quot;Return a list of all jobs.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;SELECT job_id, job, assigned_node, status FROM jobs&quot;&#10;        )&#10;        rows = c.fetchall()&#10;        return [&#10;            {&#10;                &quot;job_id&quot;: row[0],&#10;                &quot;job&quot;: json.loads(row[1]),&#10;                &quot;assigned_node&quot;: row[2],&#10;                &quot;status&quot;: row[3],&#10;            }&#10;            for row in rows&#10;        ]&#10;&#10;    def assign_job_to_node(self, job_id, node_id):&#10;        &quot;&quot;&quot;Mark a job as assigned to a specific node.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&#10;            &quot;UPDATE jobs SET assigned_node = ?, status = ? WHERE job_id = ?&quot;,&#10;            (node_id, &quot;assigned&quot;, job_id)&#10;        )&#10;        self.conn.commit()&#10;&#10;    def get_all_nodes(self):&#10;        &quot;&quot;&quot;Return a list of all registered nodes.&quot;&quot;&quot;&#10;        c = self.conn.cursor()&#10;        c.execute(&quot;SELECT node_id, profile FROM nodes&quot;)&#10;        rows = c.fetchall()&#10;        return [&#10;            {&#10;                &quot;node_id&quot;: row[0],&#10;                &quot;profile&quot;: json.loads(row[1]),&#10;            }&#10;            for row in rows&#10;        ]&#10;    &#10;    def close(self):&#10;        &quot;&quot;&quot;Safely closes the database connection.&quot;&quot;&quot;&#10;        if self.conn:&#10;            self.conn.close()&#10;            self.conn = None&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Server/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Server/requirements.txt" />
              <option name="updatedContent" value="fastapi&gt;=0.104.0&#10;uvicorn&gt;=0.24.0&#10;pydantic==2.11.7&#10;PyYAML==6.0.2&#10;prometheus_client==0.22.1&#10;cryptography&gt;=41.0.0&#10;requests&gt;=2.31.0&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/docker-compose.yaml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/docker-compose.yaml" />
              <option name="originalContent" value="version: '3.8'&#10;services:&#10;  nexapod-server:&#10;    restart: always&#10;    build:&#10;      context: Infrastructure&#10;      dockerfile: Dockerfile.server&#10;    ports:&#10;      - &quot;8000:8000&quot;&#10;    volumes:&#10;      - ./Infrastruture/k8s/nexapod-server-config.yaml:/app/config.yaml:ro&#10;&#10;  nexapod-client:&#10;    restart: always&#10;    build:&#10;      context: Infrastructure&#10;      dockerfile: Dockerfile.client&#10;    depends_on:&#10;      - nexapod-server&#10;    environment:&#10;      COORDINATOR_URL: &quot;http://nexapod-server:8000&quot;&#10;    volumes:&#10;      - ~/.nexapod:/root/.nexapod&#10;&#10;  prometheus:&#10;    image: prom/prometheus:latest&#10;    restart: always&#10;    ports:&#10;      - &quot;9090:9090&quot;&#10;    volumes:&#10;      - ./Infrastruture/k8s/prometheus-scrape-nexapod.yaml:/etc/prometheus/prometheus.yml:ro&#10;&#10;  grafana:&#10;    image: grafana/grafana:latest&#10;    restart: always&#10;    ports:&#10;      - &quot;3000:3000&quot;&#10;" />
              <option name="updatedContent" value="version: '3.8'&#10;services:&#10;  nexapod-server:&#10;    restart: always&#10;    build:&#10;      context: Infrastructure&#10;      dockerfile: Dockerfile.server&#10;    ports:&#10;      - &quot;8000:8000&quot;&#10;    volumes:&#10;      - ./Infrastructure/k8s/nexapod-server-config.yaml:/app/config.yaml:ro&#10;&#10;  nexapod-client:&#10;    restart: always&#10;    build:&#10;      context: Infrastructure&#10;      dockerfile: Dockerfile.client&#10;    depends_on:&#10;      - nexapod-server&#10;    environment:&#10;      COORDINATOR_URL: &quot;http://nexapod-server:8000&quot;&#10;    volumes:&#10;      - ~/.nexapod:/root/.nexapod&#10;&#10;  prometheus:&#10;    image: prom/prometheus:latest&#10;    restart: always&#10;    ports:&#10;      - &quot;9090:9090&quot;&#10;    volumes:&#10;      - ./Infrastructure/k8s/prometheus-scrape-nexapod.yaml:/etc/prometheus/prometheus.yml:ro&#10;&#10;  grafana:&#10;    image: grafana/grafana:latest&#10;    restart: always&#10;    ports:&#10;      - &quot;3000:3000&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>